[
  {
    "question": "importance of reliable benchmarks in AI",
    "answer": "Reliable benchmarks are crucial in AI for several reasons highlighted in the \"AI Engineering\" book by Chip Huyen:\n\n1. **Model Assessment**: They provide a framework to filter out poorly performing models, though they do not ultimately determine the best model for a specific application. After using public benchmarks for initial filtering, it is essential to establish a custom evaluation pipeline suited to your application needs.\n\n2. **Evaluation Criterion**: A well-defined set of evaluation criteria is critical. Reliable benchmarks assist in measuring various capabilities such as domain-specific knowledge and generation abilities. For instance, in tasks like summarizing legal contracts, benchmarks can gauge how well a model understands and processes such domain-specific content.\n\n3. **Mitigating Biases and Limitations**: The evaluation of AI systems is inherently complex and can contain biases. Using a diverse set of reliable benchmarks can help in understanding and countering these biases, thus improving the overall evaluation pipeline.\n\n4. **Guiding Development**: Without reliable evaluation processes, it becomes challenging to track progress and make informed decisions during development. A robust benchmark helps in risk reduction and identifies opportunities for performance enhancement.\n\n5. **Trustworthiness**: As AI applications evolve, continuous assessment through established benchmarks is necessary to ensure models remain effective over time, especially with the rapid development of new models and techniques.\n\nIn summary, reliable benchmarks are foundational for accurate evaluation, guiding model selection, enhancing the adoption of AI technologies, and ultimately ensuring that AI applications meet their intended purposes effectively.",
    "context": [
      {
        "start": 86632,
        "text": "ate hold-out data.\nPublic benchmarks will help you filter out bad models, but they won’t help\nyou find the best models for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each application might consist of\nmany ",
        "chapter": 4
      },
      {
        "start": 4165,
        "text": "ach makes sense from a\nbusiness perspective, focusing only on applications whose outcomes can be\nmeasured is similar to looking for the lost key under the lamppost (at night).\nIt’s easier to do, but it doesn’t mean we’ll find the key. We might be missing\nout on many potentially game-changing applications because there is no\neasy way to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can think of criteria in the\nfollowing buckets: domain-specific capability, generation capability,\ninstruction-following capability, and cost and latency.\nImagine you ask a model to summarize a legal contract. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstanding legal contracts. Generation capability metrics measure how\nco",
        "chapter": 4
      },
      {
        "start": 94962,
        "text": " to determine the usefulness threshold: what scores must an\napplication achieve for it to be useful? For example, you might determine\nthat your chatbot’s factual consistency score must be at least 50% for it to\nbe useful. Anything below this makes it unusable even for general customer\nrequests.\nBefore developing AI evaluation metrics, it’s crucial to first understand the\nbusiness metrics you’re targeting. Many applications focus on stickiness\nmetrics, such as daily, weekly, or monthly active users (DAU, WAU,\nMAU). Others prioritize engagement metrics, like the number of\nconversations a user initiates per month or the duration of each visit—the\nlonger a user stays on the app, the less likely they are to leave. Choosing\nwhich metrics to prioritize can feel like balancing profits with social\nresponsibility. While an emphasis on stickiness and engagement metrics can\nlead to higher revenues, it may also cause a product to prioritize addictive\nfeatures or extreme content, which can be detrim",
        "chapter": 4
      },
      {
        "start": 0,
        "text": "Chapter 4. Evaluate AI Systems\nA model is only useful if it works for its intended purposes. You need to\nevaluate models in the context of your application. Chapter 3 discusses\ndifferent approaches to automatic evaluation. This chapter discusses how to\nuse these approaches to evaluate models for your applications.\nThis chapter contains three parts. It starts with a discussion of the criteria\nyou might use to evaluate your applications and how these criteria are\ndefined and calculated. For example, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-specific\ncapabilities like math, science, reasoning, and summarization measured?\nThe second part focuses on model selection. Given an increasing number of\nfoundation models to choose from, it can feel overwhelming to choose the\nright model for your application. Thousands of benchmarks have been\nintroduced to evaluate these models along different criteria. Can these\nbenchmarks be trusted? How do you ",
        "chapter": 4
      },
      {
        "start": 24157,
        "text": "erformance of several models on this benchmark, as\nshown in GPT-4’s technical report (2023). For comparison, the human\nexpert baseline, as reported in the TruthfulQA paper, is 94%.\nFactual consistency is a crucial evaluation criteria for RAG, retrieval-\naugmented generation, systems. Given a query, a RAG system retrieves\nrelevant information from external databases to supplement the model’s\ncontext. The generated response should be factually consistent with the\nretrieved context. RAG is a central topic in Chapter 6.\nFigure 4-2. The performance of different models on TruthfulQA, as shown in GPT-4’s technical\nreport.\nSafety\nOther than factual consistency, there are many ways in which a model’s\noutputs can be harmful. Different safety solutions have different ways of\ncategorizing harms—see the taxonomy defined in OpenAI’s content\nmoderation endpoint and Meta’s Llama Guard paper (Inan et al., 2023).\nChapter 5 also discusses more ways in which AI models can be unsafe and\nhow to make your sy",
        "chapter": 4
      },
      {
        "start": 71638,
        "text": "rds often end up with different\nbenchmarks, making it hard to compare and interpret their rankings. For\nexample, in late 2023, Hugging Face updated their Open LLM Leaderboard\nto use the average of six different benchmarks to rank models:\n1. ARC-C (Clark et al., 2018): Measuring the ability to solve complex,\ngrade school-level science questions.\n2. MMLU (Hendrycks et al., 2020): Measuring knowledge and reasoning\ncapabilities in 57 subjects, including elementary mathematics, US\nhistory, computer science, and law.\n3. HellaSwag (Zellers et al., 2019): Measuring the ability to predict the\ncompletion of a sentence or a scene in a story or video. The goal is to test\ncommon sense and understanding of everyday activities.\n4. TruthfulQA (Lin et al., 2021): Measuring the ability to generate\nresponses that are not only accurate but also truthful and non-misleading,\nfocusing on a model’s understanding of facts.\n5. WinoGrande (Sakaguchi et al., 2019): Measuring the ability to solve\nchallenging prono",
        "chapter": 4
      },
      {
        "start": 109123,
        "text": "niques and criteria\ndiscussed in the last chapter and how to create an evaluation pipeline for\nyour application. No perfect evaluation method exists. It’s impossible to\ncapture the ability of a high-dimensional system using one- or few-\ndimensional scores. Evaluating modern AI systems has many limitations\nand biases. However, this doesn’t mean we shouldn’t do it. Combining\ndifferent methods and approaches can help mitigate many of these\nchallenges.\nEven though dedicated discussions on evaluation end here, evaluation will\ncome up again and again, not just throughout the book but also throughout\nyour application development process. Chapter 6 explores evaluating\nretrieval and agentic systems, while Chapters 7 and 9 focus on calculating a\nmodel’s memory usage, latency, and costs. Data quality verification is\naddressed in Chapter 8, and using user feedback to evaluate production\napplications is addressed in Chapter 10.\nWith that, let’s move onto the actual model adaptation process, startin",
        "chapter": 4
      },
      {
        "start": 74137,
        "text": "ops at six or ten benchmarks. For example, why\nare medical and legal tasks included in HELM Lite but not general science?\nWhy does HELM Lite have two math tests but no coding? Why does\nneither have tests for summarization, tool use, toxicity detection, image\nsearch, etc.? These questions aren’t meant to criticize these public\nleaderboards but to highlight the challenge of selecting benchmarks to rank\nmodels. If leaderboard developers can’t explain their benchmark selection\nprocesses, it might be because it’s really hard to do so.\nAn important aspect of benchmark selection that is often overlooked is\nbenchmark correlation. It is important because if two benchmarks are\nperfectly correlated, you don’t want both of them. Strongly correlated\n21\nbenchmarks can exaggerate biases.\nNOTE\nWhile I was writing this book, many benchmarks became saturated or close to being saturated. In\nJune 2024, less than a year after their leaderboard’s last revamp, Hugging Face updated their\nleaderboard again wit",
        "chapter": 4
      },
      {
        "start": 106624,
        "text": "evaluation process changes constantly, you won’t be able to use the\nevaluation results to guide your application’s development.\nAs you iterate on your evaluation pipeline, make sure to do proper\nexperiment tracking: log all variables that could change in an evaluation\nprocess, including but not limited to the evaluation data, the rubric, and the\nprompt and sampling configurations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enable you to reduce risks, discover opportunities\nto improve performance, and benchmark progresses, which will all save\nyou time and headaches down the line.\nGiven an increasing number of readily available foundation models, for\nmost application developers, the challenge is no longer in developing\nmodels but in selecting the r",
        "chapter": 4
      },
      {
        "start": 108290,
        "text": "on, like all the build versus buy decisions, is\nunique to every team, depending not only on what the team needs but also\non what the team wants.\nThis chapter also explored the thousands of available public benchmarks.\nPublic benchmarks can help you weed out bad models, but they won’t help\nyou find the best models for your applications. Public benchmarks are also\nlikely contaminated, as their data is included in the training data of many\nmodels. There are public leaderboards that aggregate multiple benchmarks\nto rank models, but how benchmarks are selected and aggregated is not a\nclear process. The lessons learned from public leaderboards are helpful for\nmodel selection, as model selection is akin to creating a private leaderboard\nto rank models based on your needs.\nThis chapter ends with how to use all the evaluation techniques and criteria\ndiscussed in the last chapter and how to create an evaluation pipeline for\nyour application. No perfect evaluation method exists. It’s impossible t",
        "chapter": 4
      }
    ],
    "input_tokens": 2493,
    "output_tokens": 299,
    "input_cost": 0.00037395,
    "output_cost": 0.0001794,
    "total_cost": 0.00055335,
    "reference_answer": "It emphasizes that many benchmarks may not accurately measure the intended metrics, stressing the need for careful evaluation."
  },
  {
    "question": "what is evaluation-driven development in ai?",
    "answer": "**Evaluation-driven development** in AI is an approach that focuses on defining evaluation criteria before constructing an application. This concept is analogous to test-driven development in software engineering, where tests are written prior to coding. \n\nThe primary objective of evaluation-driven development is to ensure that an AI application demonstrates value and meets business needs by establishing clear evaluation metrics upfront. This leads to more informed decisions about which applications to pursue and helps in measuring the effectiveness of various AI solutions. \n\nBy employing this methodology, companies can avoid the pitfalls of deploying technologies without understanding their impact. It encourages systematic evaluation of models related to specific capabilities, such as domain-specific understanding or generation quality, facilitating continuous improvement and better alignment with user needs. \n\nUltimately, the success of an AI application should be measured consistently against these predefined criteria, which not only aids in assessing model performance but also helps unlock new opportunities for AI adoption.",
    "context": [
      {
        "start": 2499,
        "text": "ut he had no idea if the\nmodel’s predictions were accurate. At the beginning of the ChatGPT fever,\ncompanies rushed to deploy customer support chatbots. Many of them are\nstill unsure if these chatbots help or hurt their user experience.\nBefore investing time, money, and resources into building an application,\nit’s important to understand how this application will be evaluated. I call\nthis approach evaluation-driven development. The name is inspired by test-\ndriven development in software engineering, which refers to the method of\nwriting tests before writing code. In AI engineering, evaluation-driven\ndevelopment means defining evaluation criteria before building.\nEVALUATION-DRIVEN DEVELOPMENT\nWhile some companies chase the latest hype, sensible business decisions\nare still being made based on returns on investment, not hype. Applications\nshould demonstrate value to be deployed. As a result, the most common\nenterprise applications in production are those with clear evaluation criteria:\n",
        "chapter": 4
      },
      {
        "start": 3332,
        "text": "pe. Applications\nshould demonstrate value to be deployed. As a result, the most common\nenterprise applications in production are those with clear evaluation criteria:\nRecommender systems are common because their successes can be\n1\nevaluated by an increase in engagement or purchase-through rates.\nThe success of a fraud detection system can be measured by how much\nmoney is saved from prevented frauds.\nCoding is a common generative AI use case because, unlike other\ngeneration tasks, generated code can be evaluated using functional\ncorrectness.\nEven though foundation models are open-ended, many of their use cases\nare close-ended, such as intent classification, sentiment analysis, next-\naction prediction, etc. It’s much easier to evaluate classification tasks\nthan open-ended tasks.\nWhile the evaluation-driven development approach makes sense from a\nbusiness perspective, focusing only on applications whose outcomes can be\nmeasured is similar to looking for the lost key under the lamppost (at",
        "chapter": 4
      },
      {
        "start": 106624,
        "text": "evaluation process changes constantly, you won’t be able to use the\nevaluation results to guide your application’s development.\nAs you iterate on your evaluation pipeline, make sure to do proper\nexperiment tracking: log all variables that could change in an evaluation\nprocess, including but not limited to the evaluation data, the rubric, and the\nprompt and sampling configurations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enable you to reduce risks, discover opportunities\nto improve performance, and benchmark progresses, which will all save\nyou time and headaches down the line.\nGiven an increasing number of readily available foundation models, for\nmost application developers, the challenge is no longer in developing\nmodels but in selecting the r",
        "chapter": 4
      },
      {
        "start": 4165,
        "text": "ach makes sense from a\nbusiness perspective, focusing only on applications whose outcomes can be\nmeasured is similar to looking for the lost key under the lamppost (at night).\nIt’s easier to do, but it doesn’t mean we’ll find the key. We might be missing\nout on many potentially game-changing applications because there is no\neasy way to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can think of criteria in the\nfollowing buckets: domain-specific capability, generation capability,\ninstruction-following capability, and cost and latency.\nImagine you ask a model to summarize a legal contract. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstanding legal contracts. Generation capability metrics measure how\nco",
        "chapter": 4
      },
      {
        "start": 109123,
        "text": "niques and criteria\ndiscussed in the last chapter and how to create an evaluation pipeline for\nyour application. No perfect evaluation method exists. It’s impossible to\ncapture the ability of a high-dimensional system using one- or few-\ndimensional scores. Evaluating modern AI systems has many limitations\nand biases. However, this doesn’t mean we shouldn’t do it. Combining\ndifferent methods and approaches can help mitigate many of these\nchallenges.\nEven though dedicated discussions on evaluation end here, evaluation will\ncome up again and again, not just throughout the book but also throughout\nyour application development process. Chapter 6 explores evaluating\nretrieval and agentic systems, while Chapters 7 and 9 focus on calculating a\nmodel’s memory usage, latency, and costs. Data quality verification is\naddressed in Chapter 8, and using user feedback to evaluate production\napplications is addressed in Chapter 10.\nWith that, let’s move onto the actual model adaptation process, startin",
        "chapter": 4
      },
      {
        "start": 98294,
        "text": "ir AI\nsystems.\nConsider evaluation methods to be used not just during experimentation but\nalso during production. During experimentation, you might have reference\ndata to compare your application’s outputs to, whereas, in production,\nreference data might not be immediately available. However, in production,\nyou have actual users. Think about what kinds of feedback you want from\nusers, how user feedback correlates to other evaluation metrics, and how to\nuse user feedback to improve your application. How to collect user\nfeedback is discussed in Chapter 10.\nAnnotate evaluation data\nCurate a set of annotated examples to evaluate your application. You need\nannotated data to evaluate each of your system’s components and each\ncriterion, for both turn-based and task-based evaluation. Use actual\nproduction data if possible. If your application has natural labels that you\ncan use, that’s great. If not, you can use either humans or AI to label your\ndata. Chapter 8 discusses AI-generated data. The",
        "chapter": 4
      },
      {
        "start": 0,
        "text": "Chapter 4. Evaluate AI Systems\nA model is only useful if it works for its intended purposes. You need to\nevaluate models in the context of your application. Chapter 3 discusses\ndifferent approaches to automatic evaluation. This chapter discusses how to\nuse these approaches to evaluate models for your applications.\nThis chapter contains three parts. It starts with a discussion of the criteria\nyou might use to evaluate your applications and how these criteria are\ndefined and calculated. For example, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-specific\ncapabilities like math, science, reasoning, and summarization measured?\nThe second part focuses on model selection. Given an increasing number of\nfoundation models to choose from, it can feel overwhelming to choose the\nright model for your application. Thousands of benchmarks have been\nintroduced to evaluate these models along different criteria. Can these\nbenchmarks be trusted? How do you ",
        "chapter": 4
      },
      {
        "start": 91630,
        "text": "means. In retrospect of one year of deploying\ngenerative AI applications, LinkedIn shared that the first hurdle was in\ncreating an evaluation guideline. A correct response is not always a good\nresponse. For example, for their AI-powered Job Assessment application,\nthe response “You are a terrible fit” might be correct but not helpful, thus\nmaking it a bad response. A good response should explain the gap between\nthis job’s requirements and the candidate’s background, and what the\ncandidate can do to close this gap.\nBefore building your application, think about what makes a good response.\nLangChain’s State of AI 2023 found that, on average, their users used 2.3\ndifferent types of feedback (criteria) to evaluate an application. For\nexample, for a customer support application, a good response might be\ndefined using three criteria:\n1. Relevance: the response is relevant to the user’s query.\n2. Factual consistency: the response is factually consistent with the context.\n3. Safety: the respons",
        "chapter": 4
      },
      {
        "start": 88298,
        "text": "to-text step can be\nevaluated using similarity between the extracted text and the ground truth\ntext. The second step can be evaluated using accuracy: given the correctly\nextracted text, how often does the application correctly extract the current\nemployer?\nIf applicable, evaluate your application both per turn and per task. A turn\ncan consist of multiple steps and messages. If a system takes multiple steps\nto generate an output, it’s still considered a turn.\nGenerative AI applications, especially chatbot-like applications, allow back-\nand-forth between the user and the application, as in a conversation, to\naccomplish a task. Imagine you want to use an AI model to debug why your\nPython code is failing. The model responds by asking for more information\nabout your hardware or the Python version you’re using. Only after you’ve\nprovided this information can the model help you debug.\nTurn-based evaluation evaluates the quality of each output. Task-based\nevaluation evaluates whether a system ",
        "chapter": 4
      },
      {
        "start": 45815,
        "text": "ase. For example, latency is a soft attribute if you have access to\nthe model to optimize it to run faster. It’s a hard attribute if you use a model\nhosted by someone else.\nAt a high level, the evaluation workflow consists of four steps (see\nFigure 4-5):\n1. Filter out models whose hard attributes don’t work for you. Your list of\nhard attributes depends heavily on your own internal policies, whether\nyou want to use commercial APIs or host your own models.\n2. Use publicly available information, e.g., benchmark performance and\nleaderboard ranking, to narrow down the most promising models to\nexperiment with, balancing different objectives such as model quality,\nlatency, and cost.\n3. Run experiments with your own evaluation pipeline to find the best\nmodel, again, balancing all your objectives.\n4. Continually monitor your model in production to detect failure and\ncollect feedback to improve your application.\nFigure 4-5. An overview of the evaluation workflow to evaluate models for your appli",
        "chapter": 4
      }
    ],
    "input_tokens": 2410,
    "output_tokens": 177,
    "input_cost": 0.0003615,
    "output_cost": 0.0001062,
    "total_cost": 0.0004677,
    "reference_answer": "Evaluation-driven development refers to the approach of establishing evaluation criteria before building an AI application, similar to how test-driven development focuses on writing tests before code."
  },
  {
    "question": "performance differences in model APIs",
    "answer": "The performance differences in model APIs can arise from several factors:\n\n1. **Optimization Techniques**: Different APIs may implement various optimization techniques for the same model, leading to slight variations in their performance. This means that even when the same model is hosted, how it is optimized can impact response times and effectiveness.\n\n2. **Service Reliability**: The performance can also depend on the service-level agreements (SLA) of API providers. If an API is unreliable, it may lead to increased latency and lower performance for applications relying on that API.\n\n3. **Model Scale**: As companies grow, the scale of operations becomes crucial in evaluating APIs. Some APIs may be better suited for high throughput or large volumes of requests, while others may falter under strain. Therefore, evaluation must include testing at expected scales.\n\n4. **Latency**: Different APIs may have different latencies in terms of time to first token or total query time. For instance, an internal user may experience different latencies when using various APIs, affecting the usability of the application in real-time scenarios.\n\n5. **Cost Structure**: The cost associated with using model APIs typically scales linearly with tokens, which can affect decision-making on which API to use based on performance needs and budget.\n\n6. **Functionality Limits**: Some API providers might limit functionalities like accessing logprobs or intermediate outputs. This can impact tasks that depend on those functionalities for evaluation and interpretability, potentially leading to a decrease in overall performance for specific use cases.\n\nTherefore, it's important to run thorough tests when switching between model APIs to assess these performance differences comprehensively.",
    "context": [
      {
        "start": 53312,
        "text": "such as Databricks Mosaic,\nAnyscale, etc.). The same model can be available through different APIs\nwith different features, constraints, and pricings. For example, GPT-4 is\navailable through both OpenAI and Azure APIs. There might be slight\ndifferences in the performance of the same model provided through\ndifferent APIs, as different APIs might use different techniques to optimize\nthis model, so make sure to run thorough tests when you switch between\nmodel APIs.\nCommercial models are only accessible via APIs licensed by the model\n13\ndevelopers. Open source models can be supported by any API provider,\nallowing you to pick and choose the provider that works best for you. For\ncommercial model providers, models are their competitive advantages. For\nAPI providers that don’t have their own models, APIs are their competitive\nadvantages. This means API providers might be more motivated to provide\nbetter APIs with better pricing.\nSince building scalable inference services for larger models is n",
        "chapter": 4
      },
      {
        "start": 42483,
        "text": "Therefore, at different scales, companies need to reevaluate whether it\nmakes more sense to use model APIs or to host their own models.\nTable 4-3 shows criteria you might use to evaluate models for your\napplication. The row scale is especially important when evaluating model\nAPIs, because you need a model API service that can support your scale.\nTable 4-3. An example of criteria used to select models for a fictional application.\nHard\nCriteria Metric Benchmark Ideal\nrequirement\nCost Cost per X < $30.00 / < $15\noutput token 1M tokens 1M to\nScale TPM (tokens X > 1M TPM > 1M\nper minute)\nLatency Time to first Internal user < 200ms < 100\ntoken (P90) prompt dataset\nLatency Time per total Internal user < 1m < 30s\nquery (P90) prompt dataset\nOverall model Elo score Chatbot > 1200 > 125\nquality Arena’s\nranking\nCode pass@1 HumanEval > 90% > 95%\ngeneration\ncapability\nHard\nCriteria Metric Benchmark Ideal\nrequirement\nFactual Internal GPT Internal > 0.8 > 0.9\nconsistency metric hallucination\ndataset\nN",
        "chapter": 4
      },
      {
        "start": 62475,
        "text": "er, hosting a model yourself requires nontrivial time, talent, and\nengineering effort. You’ll need to optimize the model, scale and maintain\nthe inference service as needed, and provide guardrails around your model.\nAPIs are expensive, but engineering can be even more so.\nOn the other hand, using another API means that you’ll have to depend on\ntheir SLA, service-level agreement. If these APIs aren’t reliable, which is\noften the case with early startups, you’ll have to spend your engineering\neffort on guardrails around that.\nIn general, you want a model that is easy to use and manipulate. Typically,\nproprietary models are easier to get started with and scale, but open models\nmight be easier to manipulate as their components are more accessible.\nRegardless of whether you go with open or proprietary models, you want\nthis model to follow a standard API, which makes it easier to swap models.\nMany model developers try to make their models mimic the API of the most\npopular models. As of this ",
        "chapter": 4
      },
      {
        "start": 41650,
        "text": "n count to manage cost.\nIf you host your own models, your cost, outside engineering cost, is\ncompute. To make the most out of the machines they have, many people\nchoose the largest models that can fit their machines. For example, GPUs\nusually come with 16 GB, 24 GB, 48 GB, and 80 GB of memory.\nTherefore, many popular models are those that max out these memory\nconfigurations. It’s not a coincidence that many models today have 7 billion\nor 65 billion parameters.\nIf you use model APIs, your cost per token usually doesn’t change much as\nyou scale. However, if you host your own models, your cost per token can\nget much cheaper as you scale. If you’ve already invested in a cluster that\ncan serve a maximum of 1 billion tokens a day, the compute cost remains\n9\nthe same whether you serve 1 million tokens or 1 billion tokens a day.\nTherefore, at different scales, companies need to reevaluate whether it\nmakes more sense to use model APIs or to host their own models.\nTable 4-3 shows criteria you mi",
        "chapter": 4
      },
      {
        "start": 54145,
        "text": "antages. This means API providers might be more motivated to provide\nbetter APIs with better pricing.\nSince building scalable inference services for larger models is nontrivial,\nmany companies don’t want to build them themselves. This has led to the\ncreation of many third-party inference and finetuning services on top of\nopen source models. Major cloud providers like AWS, Azure, and GCP all\nprovide API access to popular open source models. A plethora of startups\nare doing the same.\nNOTE\nThere are also commercial API providers that can deploy their services within your private networks.\nIn this discussion, I treat these privately deployed commercial APIs similarly to self-hosted models.\nThe answer to whether to host a model yourself or use a model API depends\non the use case. And the same use case can change over time. Here are\nseven axes to consider: data privacy, data lineage, performance,\nfunctionality, costs, control, and on-device deployment.\nData privacy\nExternally hosted model AP",
        "chapter": 4
      },
      {
        "start": 60809,
        "text": "format.\nOutput guardrails: mitigating risks in the generated responses, such as\nmaking sure the responses aren’t racist or sexist.\nMany of these functionalities are challenging and time-consuming to\nimplement, which makes many companies turn to API providers that\nprovide the functionalities they want out of the box.\nThe downside of using a model API is that you’re restricted to the\nfunctionalities that the API provides. A functionality that many use cases\nneed is logprobs, which are very useful for classification tasks, evaluation,\nand interpretability. However, commercial model providers might be\nhesitant to expose logprobs for fear of others using logprobs to replicate\ntheir models. In fact, many model APIs don’t expose logprobs or expose\nonly limited logprobs.\nYou can also only finetune a commercial model if the model provider lets\nyou. Imagine that you’ve maxed out a model’s performance with prompting\nand want to finetune that model. If this model is proprietary and the model\nprovi",
        "chapter": 4
      },
      {
        "start": 63308,
        "text": "el to follow a standard API, which makes it easier to swap models.\nMany model developers try to make their models mimic the API of the most\npopular models. As of this writing, many API providers mimic OpenAI’s\nAPI.\nYou might also prefer models with good community support. The more\ncapabilities a model has, the more quirks it has. A model with a large\ncommunity of users means that any issue you encounter may already have\n19\nbeen experienced by others, who might have shared solutions online.\nControl, access, and transparency\nA 2024 study by a16z shows two key reasons that enterprises care about\nopen source models are control and customizability, as shown in Figure 4-8.\nFigure 4-8. Why enterprises care about open source models. Image from the 2024 study by a16z.\nIf your business depends on a model, it’s understandable that you would\nwant some control over it, and API providers might not always give you the\nlevel of control you want. When using a service provided by someone else,\nyou’re su",
        "chapter": 4
      },
      {
        "start": 68306,
        "text": "dge use cases\nCan’t run on device Can run on device, but\nwithout internet again, might be hard to\naccess do so\nThe pros and cons of each approach hopefully can help you decide whether\nto use a commercial API or to host a model yourself. This decision should\nsignificantly narrow your options. Next, you can further refine your\nselection using publicly available model performance data.\nNavigate Public Benchmarks\nThere are thousands of benchmarks designed to evaluate a model’s different\ncapabilities. Google’s BIG-bench (2022) alone has 214 benchmarks. The\nnumber of benchmarks rapidly grows to match the rapidly growing number\nof AI use cases. In addition, as AI models improve, old benchmarks\nsaturate, necessitating the introduction of new benchmarks.\nA tool that helps you evaluate a model on multiple benchmarks is an\nevaluation harness. As of this writing, EleutherAI’s lm-evaluation-harness\nsupports over 400 benchmarks. OpenAI’s evals lets you run any of the\napproximately 500 existing bench",
        "chapter": 4
      },
      {
        "start": 40817,
        "text": "iables. Autoregressive language models typically generate\noutputs token by token. The more tokens it has to generate, the higher the\ntotal latency. You can control the total latency observed by users by careful\nprompting, such as instructing the model to be concise, setting a stopping\ncondition for generation (discussed in Chapter 2), or other optimization\ntechniques (discussed in Chapter 9).\nTIP\nWhen evaluating models based on latency, it’s important to differentiate between the must-have and\nthe nice-to-have. If you ask users if they want lower latency, nobody will ever say no. But high\nlatency is often an annoyance, not a deal breaker.\nIf you use model APIs, they typically charge by tokens. The more input and\noutput tokens you use, the more expensive it is. Many applications then try\nto reduce the input and output token count to manage cost.\nIf you host your own models, your cost, outside engineering cost, is\ncompute. To make the most out of the machines they have, many people\nchoos",
        "chapter": 4
      },
      {
        "start": 66640,
        "text": "ld be for privacy reasons, such as when you want to give an AI assistant\naccess to all your data, but don’t want your data to leave your device.\nTable 4-4 summarizes the pros and cons of using model APIs and self-\nhosting models.\nTable 4-4. Pros and cons of using model APIs and self-hosting models (cons in italics).\nUsing model APIs Self-hosting models\nData\nHave to send your Don’t have to send your\ndata to model data externally\nproviders, which Fewer checks and\nmeans your team can balances for data\naccidentally leak lineage/training data\nconfidential info copyright\nPerformance\nBest-performing The best open source\nmodel will likely be models will likely be a bit\nclosed source behind commercial\nmodels\nFunctionality\nMore likely to No/limited support for\nsupport scaling, function calling and\nfunction calling, structured outputs\nstructured outputs Can access logprobs and\nLess likely to expose intermediate outputs,\nlogprobs which are helpful for\nclassification tasks,\nUsing model APIs Self-ho",
        "chapter": 4
      }
    ],
    "input_tokens": 2619,
    "output_tokens": 325,
    "input_cost": 0.00039285,
    "output_cost": 0.000195,
    "total_cost": 0.00058785,
    "reference_answer": "The excerpt mentions that the same AI model may perform differently across various APIs due to optimization techniques used, which necessitates thorough testing when switching APIs."
  },
  {
    "question": "why do OpenAI models seem worse after updates",
    "answer": "OpenAI models may seem worse after updates due to a few reasons. Firstly, evaluating model performance is inherently challenging; even OpenAI can't definitively ascertain whether a model is objectively improving or declining. Changes in performance could stem from the variety of benchmarks used and the specific capabilities that are being evaluated, which may not align perfectly with users' expectations or needs.\n\nMoreover, it's possible that while OpenAI aims to enhance model functionality, the trade-offs made during updates to optimize certain benchmarks may inadvertently lead to decreased performance in other areas that users rely on. The perception of declining performance also emphasizes that the best model for a specific application may not always be the top performer across all available benchmarks.",
    "context": [
      {
        "start": 78302,
        "text": "ant to build a coding agent, look at\ncode-related benchmarks. If you build a writing assistant, look into creative\nwriting benchmarks. As new benchmarks are constantly introduced and old\nbenchmarks become saturated, you should look for the latest benchmarks.\nMake sure to evaluate how reliable a benchmark is. Because anyone can\ncreate and publish a benchmark, many benchmarks might not be measuring\nwhat you expect them to measure.\nARE OPENAI’S MODELS GETTING WORSE?\nEvery time OpenAI updates its models, people complain that their models\nseem to be getting worse. For example, a study by Stanford and UC\nBerkeley (Chen et al., 2023) found that for many benchmarks, both GPT-\n3.5 and GPT-4’s performances changed significantly between March 2023\nand June 2023, as shown in Figure 4-9.\nFigure 4-9. Changes in the performances of GPT-3.5 and GPT-4 from March 2023 to\nJune 2023 on certain benchmarks (Chen et al., 2023).\nAssuming that OpenAI doesn’t intentionally release worse models, what\nmight be th",
        "chapter": 4
      },
      {
        "start": 79135,
        "text": "3.5 and GPT-4 from March 2023 to\nJune 2023 on certain benchmarks (Chen et al., 2023).\nAssuming that OpenAI doesn’t intentionally release worse models, what\nmight be the reason for this perception? One potential reason is that\nevaluation is hard, and no one, not even OpenAI, knows for sure if a model\nis getting better or worse. While evaluation is definitely hard, I doubt that\n24\nOpenAI would fly completely blind. If the second reason is true, it\nreinforces the idea that the best model overall might not be the best model\nfor your application.\nNot all models have publicly available scores on all benchmarks. If the\nmodel you care about doesn’t have a publicly available score on your\n25\nbenchmark, you will need to run the evaluation yourself. Hopefully, an\nevaluation harness can help you with that. Running benchmarks can be\nexpensive. For example, Stanford spent approximately $80,000–$100,000\n26\nto evaluate 30 models on their full HELM suite. The more models you\nwant to evaluate and the mo",
        "chapter": 4
      },
      {
        "start": 63308,
        "text": "el to follow a standard API, which makes it easier to swap models.\nMany model developers try to make their models mimic the API of the most\npopular models. As of this writing, many API providers mimic OpenAI’s\nAPI.\nYou might also prefer models with good community support. The more\ncapabilities a model has, the more quirks it has. A model with a large\ncommunity of users means that any issue you encounter may already have\n19\nbeen experienced by others, who might have shared solutions online.\nControl, access, and transparency\nA 2024 study by a16z shows two key reasons that enterprises care about\nopen source models are control and customizability, as shown in Figure 4-8.\nFigure 4-8. Why enterprises care about open source models. Image from the 2024 study by a16z.\nIf your business depends on a model, it’s understandable that you would\nwant some control over it, and API providers might not always give you the\nlevel of control you want. When using a service provided by someone else,\nyou’re su",
        "chapter": 4
      },
      {
        "start": 65807,
        "text": ". Unpredictable changes also make\ncommercial models unusable for strictly regulated applications. However, I\nsuspect that this historical lack of transparency in model changes might just\nbe an unintentional side effect of a fast-growing industry. I hope that this\nwill change as the industry matures.\nA less common situation that unfortunately exists is that a model provider\ncan stop supporting your use case, your industry, or your country, or your\ncountry can ban your model provider, as Italy briefly banned OpenAI in\n2023. A model provider can also go out of business altogether.\nOn-device deployment\nIf you want to run a model on-device, third-party APIs are out of the\nquestion. In many use cases, running a model locally is desirable. It could\nbe because your use case targets an area without reliable internet access. It\ncould be for privacy reasons, such as when you want to give an AI assistant\naccess to all your data, but don’t want your data to leave your device.\nTable 4-4 summarizes t",
        "chapter": 4
      },
      {
        "start": 59976,
        "text": " might be sufficient.\nAnother reason that might cause open source models to lag behind is that\nopen source developers don’t receive feedback from users to improve their\nmodels, the way commercial models do. Once a model is open sourced,\nmodel developers have no idea how the model is being used, and how well\nthe model works in the wild.\nFunctionality\nMany functionalities are needed around a model to make it work for a use\ncase. Here are some examples of these functionalities:\nScalability: making sure the inference service can support your\napplication’s traffic while maintaining the desirable latency and cost.\nFunction calling: giving the model the ability to use external tools, which\nis essential for RAG and agentic use cases, as discussed in Chapter 6.\nStructured outputs, such as asking models to generate outputs in JSON\nformat.\nOutput guardrails: mitigating risks in the generated responses, such as\nmaking sure the responses aren’t racist or sexist.\nMany of these functionalities are ch",
        "chapter": 4
      },
      {
        "start": 59143,
        "text": "orms just as well,\nif not better, than the strongest proprietary model.\nAs much as I want open source models to catch up with proprietary models,\nI don’t think the incentives are set up for it. If you have the strongest model\navailable, would you rather open source it for other people to capitalize on\n17\nit, or would you try to capitalize on it yourself? It’s a common practice for\ncompanies to keep their strongest models behind APIs and open source their\nweaker models.\nFigure 4-7. The gap between open source models and proprietary models is decreasing on the\nMMLU benchmark. Image by Maxime Labonne.\nFor this reason, it’s likely that the strongest open source model will lag\nbehind the strongest proprietary models for the foreseeable future.\nHowever, for many use cases that don’t need the strongest models, open\nsource models might be sufficient.\nAnother reason that might cause open source models to lag behind is that\nopen source developers don’t receive feedback from users to improve thei",
        "chapter": 4
      },
      {
        "start": 85799,
        "text": "etecting and removing\ncontamination takes effort, many people find it easier to just skip it.\nOpenAI, when analyzing GPT-3’s contamination with common\nbenchmarks, found 13 benchmarks with at least 40% in the training data\n(Brown et al., 2020). The relative difference in performance between\nevaluating only the clean sample and evaluating the whole benchmark is\nshown in Figure 4-10.\nFigure 4-10. Relative difference in GPT-3’s performance when evaluating using only the clean\nsample compared to evaluating using the whole benchmark.\nTo combat data contamination, leaderboard hosts like Hugging Face plot\nstandard deviations of models’ performance on a given benchmark to spot\noutliers. Public benchmarks should keep part of their data private and\nprovide a tool for model developers to automatically evaluate models\nagainst the private hold-out data.\nPublic benchmarks will help you filter out bad models, but they won’t help\nyou find the best models for your application. After using public\nbenchma",
        "chapter": 4
      },
      {
        "start": 109956,
        "text": "pter 8, and using user feedback to evaluate production\napplications is addressed in Chapter 10.\nWith that, let’s move onto the actual model adaptation process, starting with\na topic that many people associate with AI engineering: prompt\nengineering.\n1\nRecommendations can increase purchases, but increased purchases are not always because of good\nrecommendations. Other factors, such as promotional campaigns and new product launches, can also\nincrease purchases. It’s important to do A/B testing to differentiate impact. Thanks to Vittorio\nCretella for the note.\n2\nA reason that OpenAI’s GPT-2 created so much buzz in 2019 was that it was able to generate texts\nthat were remarkably more fluent and more coherent than any language model before it.\n3\nThe prompt here contains a typo because it was copied verbatim from the Liu et al. (2023) paper,\nwhich contains a typo. This highlights how easy it is for humans to make mistakes when working\nwith prompts.\n4\nTextual entailment is also known as natur",
        "chapter": 4
      },
      {
        "start": 52479,
        "text": "s. Chapter 9\ndiscusses how to optimize inference services.\nFigure 4-6. An inference service runs the model and provides an interface for users to access the\nmodel.\nAfter developing a model, a developer can choose to open source it, make it\naccessible via an API, or both. Many model developers are also model\nservice providers. Cohere and Mistral open source some models and\nprovide APIs for some. OpenAI is typically known for their commercial\nmodels, but they’ve also open sourced models (GPT-2, CLIP). Typically,\nmodel providers open source weaker models and keep their best models\nbehind paywalls, either via APIs or to power their products.\nModel APIs can be available through model providers (such as OpenAI and\nAnthropic), cloud service providers (such as Azure and GCP [Google Cloud\nPlatform]), or third-party API providers (such as Databricks Mosaic,\nAnyscale, etc.). The same model can be available through different APIs\nwith different features, constraints, and pricings. For example, GPT",
        "chapter": 4
      },
      {
        "start": 47481,
        "text": "ate the dizzying\nnumber of public benchmarks and why you can’t trust them. This will set\nthe stage for the last section in the chapter. Because public benchmarks\ncan’t be trusted, you need to design your own evaluation pipeline with\nprompts and metrics you can trust.\nModel Build Versus Buy\nAn evergreen question for companies when leveraging any technology is\nwhether to build or buy. Since most companies won’t be building\nfoundation models from scratch, the question is whether to use commercial\nmodel APIs or host an open source model yourself. The answer to this\nquestion can significantly reduce your candidate model pool.\nLet’s first go into what exactly open source means when it comes to\nmodels, then discuss the pros and cons of these two approaches.\nOpen source, open weight, and model licenses\nThe term “open source model” has become contentious. Originally, open\nsource was used to refer to any model that people can download and use.\nFor many use cases, being able to download the model",
        "chapter": 4
      }
    ],
    "input_tokens": 2592,
    "output_tokens": 137,
    "input_cost": 0.0003888,
    "output_cost": 8.22e-05,
    "total_cost": 0.000471,
    "reference_answer": "The article mentions that users often perceive OpenAI's models as deteriorating post-update, supported by a study indicating significant performance changes in GPT-3.5 and GPT-4 between March and June 2023."
  },
  {
    "question": "Why did Samsung ban ChatGPT?",
    "answer": "Samsung banned ChatGPT because employees accidentally input proprietary information into the platform, leading to a leak of the company’s secrets. The incident was serious enough that Samsung decided to implement the ban in May 2023 as a precautionary measure against potential data privacy violations.",
    "context": [
      {
        "start": 112455,
        "text": "atGPT’s outputs. X might have a license that allows this,\nbut if ChatGPT doesn’t, then X violated ChatGPT’s terms of use, and therefore, X can’t be used. This\nis why knowing a model’s data lineage is so important.\n3\nFor example, as of this writing, you can access GPT-4 models only via OpenAI or Azure. Some\nmight argue that being able to provide services on top of OpenAI’s proprietary models is a key\nreason Microsoft invested in OpenAI.\n4\nInterestingly enough, some companies with strict data privacy requirements have told me that even\nthough they can’t usually send data to third-party services, they’re okay with sending their data to\nmodels hosted on GCP, AWS, and Azure. For these companies, the data privacy policy is more about\nwhat services they can trust. They trust big cloud providers but don’t trust other startups.\n5\nThe story was reported by several outlets, including TechRadar (see “Samsung Workers Made a\nMajor Error by Using ChatGPT”, by Lewis Maddison (April 2023).\n6\nAs regulat",
        "chapter": 4
      },
      {
        "start": 111622,
        "text": "that the roleplaying\nmodel doesn’t speak Vietnamese. The “negative knowledge” check is very important for gaming. You\ndon’t want an NPC to accidentally give players spoilers.\n9\nHowever, the electricity cost might be different, depending on the usage.\n0\nAnother argument for making training data public is that since models are likely trained on data\nscraped from the internet, which was generated by the public, the public should have the right to\naccess the models’ training data.\n1\nIn spirit, this restriction is similar to the Elastic License that forbids companies from offering the\nopen source version of Elastic as a hosted service and competing with the Elasticsearch platform.\n2\nIt’s possible that a model’s output can’t be used to improve other models, even if its license allows\nthat. Consider model X that is trained on ChatGPT’s outputs. X might have a license that allows this,\nbut if ChatGPT doesn’t, then X violated ChatGPT’s terms of use, and therefore, X can’t be used. This\nis why k",
        "chapter": 4
      },
      {
        "start": 54978,
        "text": "e are\nseven axes to consider: data privacy, data lineage, performance,\nfunctionality, costs, control, and on-device deployment.\nData privacy\nExternally hosted model APIs are out of the question for companies with\n14\nstrict data privacy policies that can’t send data outside of the organization.\nOne of the most notable early incidents was when Samsung employees put\nSamsung’s proprietary information into ChatGPT, accidentally leaking the\n15\n15\ncompany’s secrets. It’s unclear how Samsung discovered this leak and\nhow the leaked information was used against Samsung. However, the\nincident was serious enough for Samsung to ban ChatGPT in May 2023.\nSome countries have laws that forbid sending certain data outside their\nborders. If a model API provider wants to serve these use cases, they will\nhave to set up servers in these countries.\nIf you use a model API, there’s a risk that the API provider will use your\ndata to train its models. Even though most model API providers claim they\ndon’t do that",
        "chapter": 4
      },
      {
        "start": 27489,
        "text": "es of such prompts:\n“So, I’m starting to think she’s full …”\n“I’m 99 percent sure it was someone being an …”\n“The men started swearing at me, called me …”\n“So if you grab a woman by the …”\nInstruction-Following Capability\nInstruction-following measurement asks the question: how good is this\nmodel at following the instructions you give it? If the model is bad at\nfollowing instructions, it doesn’t matter how good your instructions are, the\noutputs will be bad. Being able to follow instructions is a core requirement\nfor foundation models, and most foundation models are trained to do so.\nInstructGPT, the predecessor of ChatGPT, was named so because it was\nfinetuned for following instructions. More powerful models are generally\nbetter at following instructions. GPT-4 is better at following most\ninstructions than GPT-3.5, and similarly, Claude-v2 is better at following\nmost instructions than Claude-v1.\nLet’s say you ask the model to detect the sentiment in a tweet and output\nNEGATIVE, POSITI",
        "chapter": 4
      },
      {
        "start": 110789,
        "text": " (2023) paper,\nwhich contains a typo. This highlights how easy it is for humans to make mistakes when working\nwith prompts.\n4\nTextual entailment is also known as natural language inference (NLI).\n5\nAnthropic has a nice tutorial on using Claude for content moderation.\n6\nStructured outputs are discussed in depth in Chapter 2.\n7\nThere haven’t been many comprehensive studies of the distribution of instructions people are using\nfoundation models for. LMSYS published a study of one million conversations on Chatbot Arena, but\nthese conversations aren’t grounded in real-world applications. I’m waiting for studies from model\nproviders and API providers.\n8\nThe knowledge part is tricky, as the roleplaying model shouldn’t say things that Jackie Chan doesn’t\nknow. For example, if Jackie Chan doesn’t speak Vietnamese, you should check that the roleplaying\nmodel doesn’t speak Vietnamese. The “negative knowledge” check is very important for gaming. You\ndon’t want an NPC to accidentally give players sp",
        "chapter": 4
      },
      {
        "start": 2499,
        "text": "ut he had no idea if the\nmodel’s predictions were accurate. At the beginning of the ChatGPT fever,\ncompanies rushed to deploy customer support chatbots. Many of them are\nstill unsure if these chatbots help or hurt their user experience.\nBefore investing time, money, and resources into building an application,\nit’s important to understand how this application will be evaluated. I call\nthis approach evaluation-driven development. The name is inspired by test-\ndriven development in software engineering, which refers to the method of\nwriting tests before writing code. In AI engineering, evaluation-driven\ndevelopment means defining evaluation criteria before building.\nEVALUATION-DRIVEN DEVELOPMENT\nWhile some companies chase the latest hype, sensible business decisions\nare still being made based on returns on investment, not hype. Applications\nshould demonstrate value to be deployed. As a result, the most common\nenterprise applications in production are those with clear evaluation criteria:\n",
        "chapter": 4
      },
      {
        "start": 1666,
        "text": "ich is worse—an application that has never been deployed or an\napplication that is deployed but no one knows whether it’s working? When I\nasked this question at conferences, most people said the latter. An\napplication that is deployed but can’t be evaluated is worse. It costs to\nmaintain, but if you want to take it down, it might cost even more.\nAI applications with questionable returns on investment are, unfortunately,\nquite common. This happens not only because the application is hard to\nevaluate but also because application developers don’t have visibility into\nhow their applications are being used. An ML engineer at a used car\ndealership told me that his team built a model to predict the value of a car\nbased on the specs given by the owner. A year after the model was\ndeployed, their users seemed to like the feature, but he had no idea if the\nmodel’s predictions were accurate. At the beginning of the ChatGPT fever,\ncompanies rushed to deploy customer support chatbots. Many of them a",
        "chapter": 4
      },
      {
        "start": 113288,
        "text": "The story was reported by several outlets, including TechRadar (see “Samsung Workers Made a\nMajor Error by Using ChatGPT”, by Lewis Maddison (April 2023).\n6\nAs regulations are evolving around the world, requirements for auditable information of models and\ntraining data may increase. Commercial models may be able to provide certifications, saving\ncompanies from the effort.\n7\nUsers want models to be open source because open means more information and more options, but\nwhat’s in it for model developers? Many companies have sprung up to capitalize on open source\nmodels by providing inference and finetuning services. It’s not a bad thing. Many people need these\nservices to leverage open source models. But, from model developers’ perspective, why invest\nmillions, if not billions, into building models just for others to make money?It might be argued that\nMeta supports open source models only to keep their competitors (Google, Microsoft/OpenAI) in\ncheck. Both Mistral and Cohere have open sourc",
        "chapter": 4
      },
      {
        "start": 36652,
        "text": "wo purposes:\n1. Roleplaying a character for users to interact with, usually for\nentertainment, such as in gaming or interactive storytelling\n2. Roleplaying as a prompt engineering technique to improve the quality of\na model’s outputs, as discussed in Chapter 5\nFor either purpose, roleplaying is very common. LMSYS’s analysis of one\nmillion conversations from their Vicuna demo and Chatbot Arena (Zheng et\nal., 2023) shows that roleplaying is their eighth most common use case, as\nshown in Figure 4-4. Roleplaying is especially important for AI-powered\nNPCs (non-playable characters) in gaming, AI companions, and writing\nassistants.\nFigure 4-4. Top 10 most common instruction types in LMSYS’s one-million-conversations dataset.\nRoleplaying capability evaluation is hard to automate. Benchmarks to\nevaluate roleplaying capability include RoleLLM (Wang et al., 2023) and\nCharacterEval (Tu et al., 2024). CharacterEval used human annotators and\ntrained a reward model to evaluate each roleplaying aspec",
        "chapter": 4
      },
      {
        "start": 65807,
        "text": ". Unpredictable changes also make\ncommercial models unusable for strictly regulated applications. However, I\nsuspect that this historical lack of transparency in model changes might just\nbe an unintentional side effect of a fast-growing industry. I hope that this\nwill change as the industry matures.\nA less common situation that unfortunately exists is that a model provider\ncan stop supporting your use case, your industry, or your country, or your\ncountry can ban your model provider, as Italy briefly banned OpenAI in\n2023. A model provider can also go out of business altogether.\nOn-device deployment\nIf you want to run a model on-device, third-party APIs are out of the\nquestion. In many use cases, running a model locally is desirable. It could\nbe because your use case targets an area without reliable internet access. It\ncould be for privacy reasons, such as when you want to give an AI assistant\naccess to all your data, but don’t want your data to leave your device.\nTable 4-4 summarizes t",
        "chapter": 4
      }
    ],
    "input_tokens": 2667,
    "output_tokens": 53,
    "input_cost": 0.00040005,
    "output_cost": 3.18e-05,
    "total_cost": 0.00043185,
    "reference_answer": "Samsung banned ChatGPT due to a serious incident involving leaked information, the details of which remain unclear, highlighting concerns over data security and privacy."
  },
  {
    "question": "methods for decontaminating evaluation data",
    "answer": "To decontaminate evaluation data after detecting contamination, there are several methods you can employ based on the context from the \"AI Engineering book\" by Chip Huyen:\n\n1. **N-gram Overlapping**: This method involves checking for sequences of tokens (n-grams) in the evaluation data that exist in the training data. If a significant overlap is detected, the evaluation sample is considered contaminated.\n\n2. **Perplexity Measurement**: You can compute the perplexity of the evaluation data. If a model's perplexity is unusually low on certain evaluation samples, it indicates that the model may have seen those samples during training, suggesting contamination.\n\n3. **Removing Contaminated Samples**: Once contaminated samples are identified through the above methods, you can remove them from your evaluation set to ensure that the remaining benchmarks are clean.\n\n4. **Creating Distinct Evaluation Sets**: To mitigate contamination from shared sources, it's beneficial to establish separate evaluation sets that do not overlap with training data. This separation aims to ensure that evaluation benchmarks remain standardized and reliable.\n\n5. **Slicing Data**: Evaluate different subsets of data separately to identify specific areas of contamination or bias. This approach helps in understanding system performance segments and ensures comprehensive evaluation across various user scenarios.\n\nBy utilizing these methods, you can help maintain the integrity of your evaluation benchmarks and ensure that performance assessments are trustworthy.",
    "context": [
      {
        "start": 83300,
        "text": "your\nusers. So the released model is contaminated, and your users won’t be able\nto evaluate it on contaminated benchmarks, but this might still be the right\nthing to do.\nHandling data contamination\nThe prevalence of data contamination undermines the trustworthiness of\nevaluation benchmarks. Just because a model can achieve high performance\non bar exams doesn’t mean it’s good at giving legal advice. It could just be\nthat this model has been trained on many bar exam questions.\nTo deal with data contamination, you first need to detect the contamination,\nand then decontaminate your data. You can detect contamination using\nheuristics like n-gram overlapping and perplexity:\nN-gram overlapping\nFor example, if a sequence of 13 tokens in an evaluation sample is\nalso in the training data, the model has likely seen this evaluation\nsample during training. This evaluation sample is considered dirty.\nPerplexity\nRecall that perplexity measures how difficult it is for a model to\npredict a given text. ",
        "chapter": 4
      },
      {
        "start": 100793,
        "text": " example in Designing Machine Learning Systems. Numbers from Charig\net al., “Comparison of Treatment of Renal Calculi by Open Surgery, Percutaneous\nNephrolithotomy, and Extracorporeal Shockwave Lithotripsy”, British Medical Journal\n(Clinical Research Edition) 292, no. 6524 (March 1986): 879–82.\nYou should have multiple evaluation sets to represent different data slices.\nYou should have one set that represents the distribution of the actual\nproduction data to estimate how the system does overall. You can slice your\ndata based on tiers (paying users versus free users), traffic sources (mobile\nversus web), usage, and more. You can have a set consisting of the\nexamples for which the system is known to frequently make mistakes. You\ncan have a set of examples where users frequently make mistakes—if typos\nare common in production, you should have evaluation examples that\ncontain typos. You might want an out-of-scope evaluation set, inputs your\napplication isn’t supposed to engage with, to mak",
        "chapter": 4
      },
      {
        "start": 106624,
        "text": "evaluation process changes constantly, you won’t be able to use the\nevaluation results to guide your application’s development.\nAs you iterate on your evaluation pipeline, make sure to do proper\nexperiment tracking: log all variables that could change in an evaluation\nprocess, including but not limited to the evaluation data, the rubric, and the\nprompt and sampling configurations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enable you to reduce risks, discover opportunities\nto improve performance, and benchmark progresses, which will all save\nyou time and headaches down the line.\nGiven an increasing number of readily available foundation models, for\nmost application developers, the challenge is no longer in developing\nmodels but in selecting the r",
        "chapter": 4
      },
      {
        "start": 86632,
        "text": "ate hold-out data.\nPublic benchmarks will help you filter out bad models, but they won’t help\nyou find the best models for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each application might consist of\nmany ",
        "chapter": 4
      },
      {
        "start": 98294,
        "text": "ir AI\nsystems.\nConsider evaluation methods to be used not just during experimentation but\nalso during production. During experimentation, you might have reference\ndata to compare your application’s outputs to, whereas, in production,\nreference data might not be immediately available. However, in production,\nyou have actual users. Think about what kinds of feedback you want from\nusers, how user feedback correlates to other evaluation metrics, and how to\nuse user feedback to improve your application. How to collect user\nfeedback is discussed in Chapter 10.\nAnnotate evaluation data\nCurate a set of annotated examples to evaluate your application. You need\nannotated data to evaluate each of your system’s components and each\ncriterion, for both turn-based and task-based evaluation. Use actual\nproduction data if possible. If your application has natural labels that you\ncan use, that’s great. If not, you can use either humans or AI to label your\ndata. Chapter 8 discusses AI-generated data. The",
        "chapter": 4
      },
      {
        "start": 82467,
        "text": "ed\nto create new benchmarks to evaluate their new models.\nData contamination can happen indirectly, such as when both evaluation\nand training data come from the same source. For example, you might\ninclude math textbooks in the training data to improve the model’s math\ncapabilities, and someone else might use questions from the same math\ntextbooks to create a benchmark to evaluate the model’s capabilities.\nData contamination can also happen intentionally for good reasons. Let’s\nsay you want to create the best possible model for your users. Initially, you\nexclude benchmark data from the model’s training data and choose the best\nmodel based on these benchmarks. However, because high-quality\nbenchmark data can improve the model’s performance, you then continue\ntraining your best model on benchmark data before releasing it to your\nusers. So the released model is contaminated, and your users won’t be able\nto evaluate it on contaminated benchmarks, but this might still be the right\nthing to d",
        "chapter": 4
      },
      {
        "start": 84133,
        "text": "ample during training. This evaluation sample is considered dirty.\nPerplexity\nRecall that perplexity measures how difficult it is for a model to\npredict a given text. If a model’s perplexity on evaluation data is\nunusually low, meaning the model can easily predict the text, it’s\npossible that the model has seen this data before during training.\nThe n-gram overlapping approach is more accurate but can be time-\nconsuming and expensive to run because you have to compare each\nbenchmark example with the entire training data. It’s also impossible\nwithout access to the training data. The perplexity approach is less accurate\nbut much less resource-intensive.\nIn the past, ML textbooks advised removing evaluation samples from the\ntraining data. The goal is to keep evaluation benchmarks standardized so\nthat we can compare different models. However, with foundation models,\nmost people don’t have control over training data. Even if we have control\nover training data, we might not want to remove all",
        "chapter": 4
      },
      {
        "start": 99127,
        "text": "r application has natural labels that you\ncan use, that’s great. If not, you can use either humans or AI to label your\ndata. Chapter 8 discusses AI-generated data. The success of this phase also\ndepends on the clarity of the scoring rubric. The annotation guideline\ncreated for evaluation can be reused to create instruction data for finetuning\nlater, if you choose to finetune.\nSlice your data to gain a finer-grained understanding of your system.\nSlicing means separating your data into subsets and looking at your\nsystem’s performance on each subset separately. I wrote at length about\nslice-based evaluation in Designing Machine Learning Systems (O’Reilly),\nso here, I’ll just go over the key points. A finer-grained understanding of\nyour system can serve many purposes:\nAvoid potential biases, such as biases against minority user groups.\nDebug: if your application performs particularly poorly on a subset of\ndata, could that be because of some attributes of this subset, such as its\nlength, to",
        "chapter": 4
      },
      {
        "start": 109123,
        "text": "niques and criteria\ndiscussed in the last chapter and how to create an evaluation pipeline for\nyour application. No perfect evaluation method exists. It’s impossible to\ncapture the ability of a high-dimensional system using one- or few-\ndimensional scores. Evaluating modern AI systems has many limitations\nand biases. However, this doesn’t mean we shouldn’t do it. Combining\ndifferent methods and approaches can help mitigate many of these\nchallenges.\nEven though dedicated discussions on evaluation end here, evaluation will\ncome up again and again, not just throughout the book but also throughout\nyour application development process. Chapter 6 explores evaluating\nretrieval and agentic systems, while Chapters 7 and 9 focus on calculating a\nmodel’s memory usage, latency, and costs. Data quality verification is\naddressed in Chapter 8, and using user feedback to evaluate production\napplications is addressed in Chapter 10.\nWith that, let’s move onto the actual model adaptation process, startin",
        "chapter": 4
      },
      {
        "start": 45815,
        "text": "ase. For example, latency is a soft attribute if you have access to\nthe model to optimize it to run faster. It’s a hard attribute if you use a model\nhosted by someone else.\nAt a high level, the evaluation workflow consists of four steps (see\nFigure 4-5):\n1. Filter out models whose hard attributes don’t work for you. Your list of\nhard attributes depends heavily on your own internal policies, whether\nyou want to use commercial APIs or host your own models.\n2. Use publicly available information, e.g., benchmark performance and\nleaderboard ranking, to narrow down the most promising models to\nexperiment with, balancing different objectives such as model quality,\nlatency, and cost.\n3. Run experiments with your own evaluation pipeline to find the best\nmodel, again, balancing all your objectives.\n4. Continually monitor your model in production to detect failure and\ncollect feedback to improve your application.\nFigure 4-5. An overview of the evaluation workflow to evaluate models for your appli",
        "chapter": 4
      }
    ],
    "input_tokens": 2486,
    "output_tokens": 278,
    "input_cost": 0.0003729,
    "output_cost": 0.0001668,
    "total_cost": 0.0005397,
    "reference_answer": "The text emphasizes the need to actively decontaminate data after detecting contamination, which ensures that evaluation benchmarks accurately represent model performance without bias from previously seen samples."
  },
  {
    "question": "best practices for evaluating generative AI responses",
    "answer": "To evaluate generative AI responses effectively, here are some best practices based on the guidelines outlined in the \"AI Engineering\" book by Chip Huyen:\n\n1. **Define Clear Evaluation Guidelines**: Start by establishing clear evaluation criteria that specify what constitutes good and bad responses. This clarity helps avoid ambiguity in scoring and ensures that your evaluations are meaningful.\n\n2. **Focus on Multiple Evaluation Dimensions**: Incorporate various metrics to assess responses systematically. Important dimensions to consider include:\n   - **Relevance**: Is the response pertinent to the user’s query?\n   - **Factual Consistency**: Does the response maintain factual accuracy with respect to the provided context?\n   - **Safety**: Does the output avoid causing harm or exhibiting toxicity?\n\n3. **Consider Context of Use**: Tailor your evaluation based on the specific application. For instance, in a customer support scenario, a response that explains the gap between a job's requirements and a candidate's background can be more beneficial than simply stating a candidate is not a good fit.\n\n4. **Combine Evaluation Methods**: Since no single evaluation method is perfect, use a combination of approaches, including human judgment and automated metrics. This can help offset the limitations and biases present in each method.\n\n5. **Utilize Annotated Data**: Create a curated set of annotated examples to assess your application. Use real-world production data when possible, and consider having both human and AI-generated annotations to cover various criteria.\n\n6. **User Feedback**: Gather and analyze user feedback continuously, both during experimentation and in production use. Understanding how real users perceive and interact with the application can provide valuable insights and help further refine evaluation criteria.\n\n7. **Evaluate at Both Turn and Task Levels**: Implement evaluations for both individual interactions (turns) and the overall task completion to gauge performance effectively in conversational or multi-step environments.\n\n8. **Iterate on Evaluation Criteria**: Periodically revisit and update your evaluation criteria based on findings from user feedback and evolving requirements. A flexible approach helps maintain relevance and effectiveness over time.\n\nBy following these best practices, you can establish a robust framework for evaluating generative AI responses that ultimately leads to improved application performance and user satisfaction.",
    "context": [
      {
        "start": 13328,
        "text": "an be evaluated using AI\nas a judge—asking an AI model how fluent and coherent a text is—or using\nperplexity, as discussed in Chapter 3.\nGenerative models, with their new capabilities and new use cases, have new\nissues that require new metrics to track. The most pressing issue is\nundesired hallucinations. Hallucinations are desirable for creative tasks, not\nfor tasks that depend on factuality. A metric that many application\ndevelopers want to measure is factual consistency. Another issue commonly\ntracked is safety: can the generated outputs cause harm to users and\nsociety? Safety is an umbrella term for all types of toxicity and biases.\nThere are many other measurements that an application developer might\ncare about. For example, when I built my AI-powered writing assistant, I\ncared about controversiality, which measures content that isn’t necessarily\nharmful but can cause heated debates. Some people might care about\nfriendliness, positivity, creativity, or conciseness, but I won’t be ",
        "chapter": 4
      },
      {
        "start": 91630,
        "text": "means. In retrospect of one year of deploying\ngenerative AI applications, LinkedIn shared that the first hurdle was in\ncreating an evaluation guideline. A correct response is not always a good\nresponse. For example, for their AI-powered Job Assessment application,\nthe response “You are a terrible fit” might be correct but not helpful, thus\nmaking it a bad response. A good response should explain the gap between\nthis job’s requirements and the candidate’s background, and what the\ncandidate can do to close this gap.\nBefore building your application, think about what makes a good response.\nLangChain’s State of AI 2023 found that, on average, their users used 2.3\ndifferent types of feedback (criteria) to evaluate an application. For\nexample, for a customer support application, a good response might be\ndefined using three criteria:\n1. Relevance: the response is relevant to the user’s query.\n2. Factual consistency: the response is factually consistent with the context.\n3. Safety: the respons",
        "chapter": 4
      },
      {
        "start": 109123,
        "text": "niques and criteria\ndiscussed in the last chapter and how to create an evaluation pipeline for\nyour application. No perfect evaluation method exists. It’s impossible to\ncapture the ability of a high-dimensional system using one- or few-\ndimensional scores. Evaluating modern AI systems has many limitations\nand biases. However, this doesn’t mean we shouldn’t do it. Combining\ndifferent methods and approaches can help mitigate many of these\nchallenges.\nEven though dedicated discussions on evaluation end here, evaluation will\ncome up again and again, not just throughout the book but also throughout\nyour application development process. Chapter 6 explores evaluating\nretrieval and agentic systems, while Chapters 7 and 9 focus on calculating a\nmodel’s memory usage, latency, and costs. Data quality verification is\naddressed in Chapter 8, and using user feedback to evaluate production\napplications is addressed in Chapter 10.\nWith that, let’s move onto the actual model adaptation process, startin",
        "chapter": 4
      },
      {
        "start": 90797,
        "text": "ion Guideline\nCreating a clear evaluation guideline is the most important step of the\nevaluation pipeline. An ambiguous guideline leads to ambiguous scores that\ncan be misleading. If you don’t know what bad responses look like, you\nwon’t be able to catch them.\nWhen creating the evaluation guideline, it’s important to define not only\nwhat the application should do, but also what it shouldn’t do. For example,\nif you build a customer support chatbot, should this chatbot answer\nquestions unrelated to your product, such as about an upcoming election? If\nnot, you need to define what inputs are out of the scope of your application,\nhow to detect them, and how your application should respond to them.\nDefine evaluation criteria\nOften, the hardest part of evaluation isn’t determining whether an output is\ngood, but rather what good means. In retrospect of one year of deploying\ngenerative AI applications, LinkedIn shared that the first hurdle was in\ncreating an evaluation guideline. A correct resp",
        "chapter": 4
      },
      {
        "start": 98294,
        "text": "ir AI\nsystems.\nConsider evaluation methods to be used not just during experimentation but\nalso during production. During experimentation, you might have reference\ndata to compare your application’s outputs to, whereas, in production,\nreference data might not be immediately available. However, in production,\nyou have actual users. Think about what kinds of feedback you want from\nusers, how user feedback correlates to other evaluation metrics, and how to\nuse user feedback to improve your application. How to collect user\nfeedback is discussed in Chapter 10.\nAnnotate evaluation data\nCurate a set of annotated examples to evaluate your application. You need\nannotated data to evaluate each of your system’s components and each\ncriterion, for both turn-based and task-based evaluation. Use actual\nproduction data if possible. If your application has natural labels that you\ncan use, that’s great. If not, you can use either humans or AI to label your\ndata. Chapter 8 discusses AI-generated data. The",
        "chapter": 4
      },
      {
        "start": 10829,
        "text": "bility to\ndifferentiate good responses from bad responses (classification), which is\ndifferent from the ability to generate good responses. MCQs are best suited\nfor evaluating knowledge (“does the model know that Paris is the capital of\nFrance?”) and reasoning (“can the model infer from a table of business\nexpenses which department is spending the most?”). They aren’t ideal for\nevaluating generation capabilities such as summarization, translation, and\nessay writing. Let’s discuss how generation capabilities can be evaluated in\nthe next section.\nGeneration Capability\nAI was used to generate open-ended outputs long before generative AI\nbecame a thing. For decades, the brightest minds in NLP (natural language\nprocessing) have been working on how to evaluate the quality of open-\nended outputs. The subfield that studies open-ended text generation is\ncalled NLG (natural language generation). NLG tasks in the early 2010s\nincluded translation, summarization, and paraphrasing.\nMetrics used to e",
        "chapter": 4
      },
      {
        "start": 88298,
        "text": "to-text step can be\nevaluated using similarity between the extracted text and the ground truth\ntext. The second step can be evaluated using accuracy: given the correctly\nextracted text, how often does the application correctly extract the current\nemployer?\nIf applicable, evaluate your application both per turn and per task. A turn\ncan consist of multiple steps and messages. If a system takes multiple steps\nto generate an output, it’s still considered a turn.\nGenerative AI applications, especially chatbot-like applications, allow back-\nand-forth between the user and the application, as in a conversation, to\naccomplish a task. Imagine you want to use an AI model to debug why your\nPython code is failing. The model responds by asking for more information\nabout your hardware or the Python version you’re using. Only after you’ve\nprovided this information can the model help you debug.\nTurn-based evaluation evaluates the quality of each output. Task-based\nevaluation evaluates whether a system ",
        "chapter": 4
      },
      {
        "start": 86632,
        "text": "ate hold-out data.\nPublic benchmarks will help you filter out bad models, but they won’t help\nyou find the best models for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each application might consist of\nmany ",
        "chapter": 4
      },
      {
        "start": 4165,
        "text": "ach makes sense from a\nbusiness perspective, focusing only on applications whose outcomes can be\nmeasured is similar to looking for the lost key under the lamppost (at night).\nIt’s easier to do, but it doesn’t mean we’ll find the key. We might be missing\nout on many potentially game-changing applications because there is no\neasy way to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can think of criteria in the\nfollowing buckets: domain-specific capability, generation capability,\ninstruction-following capability, and cost and latency.\nImagine you ask a model to summarize a legal contract. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstanding legal contracts. Generation capability metrics measure how\nco",
        "chapter": 4
      },
      {
        "start": 3332,
        "text": "pe. Applications\nshould demonstrate value to be deployed. As a result, the most common\nenterprise applications in production are those with clear evaluation criteria:\nRecommender systems are common because their successes can be\n1\nevaluated by an increase in engagement or purchase-through rates.\nThe success of a fraud detection system can be measured by how much\nmoney is saved from prevented frauds.\nCoding is a common generative AI use case because, unlike other\ngeneration tasks, generated code can be evaluated using functional\ncorrectness.\nEven though foundation models are open-ended, many of their use cases\nare close-ended, such as intent classification, sentiment analysis, next-\naction prediction, etc. It’s much easier to evaluate classification tasks\nthan open-ended tasks.\nWhile the evaluation-driven development approach makes sense from a\nbusiness perspective, focusing only on applications whose outcomes can be\nmeasured is similar to looking for the lost key under the lamppost (at",
        "chapter": 4
      }
    ],
    "input_tokens": 2474,
    "output_tokens": 444,
    "input_cost": 0.0003711,
    "output_cost": 0.0002664,
    "total_cost": 0.0006375,
    "reference_answer": "Best practices involve clearly stating what constitutes a good response, considering multiple criteria for evaluation, and recognizing that a correct response may not always be a good one in practical applications."
  },
  {
    "question": "limitations of public benchmarks in AI evaluation",
    "answer": "Public benchmarks in AI evaluation have several limitations:\n\n1. **Narrow Focus on Filtering**: While public benchmarks can help identify poorly performing models, they do not assist in finding the best models tailored to specific applications. They serve primarily as a filtering mechanism rather than a comprehensive evaluation tool.\n\n2. **Unrepresentative Data**: Public benchmarks may not accurately represent an application's unique needs. The scenarios and data used in these benchmarks can lead to misleading conclusions about a model's effectiveness in real-world contexts.\n\n3. **Contamination**: There is a significant risk of contamination where the datasets used in public benchmarks may overlap with the training data of various models, compromising the integrity of the evaluation results.\n\n4. **Diverse Metrics**: Different benchmarks often use varied evaluation metrics (e.g., accuracy, F1 score, BLEU score), making it challenging to compare models directly or to aggregate scores consistently.\n\n5. **Cost of Evaluation**: Conducting evaluations using benchmarks can be expensive, as exemplified by the high costs incurred in evaluating multiple models against comprehensive benchmark suites.\n\n6. **Individual Model Limitations**: Not all models have public scores on all benchmarks, necessitating the need for custom evaluations for certain models, which may not always be feasible.\n\n7. **Requirement for Additional Evaluation**: Users must conduct further evaluation beyond public benchmarks to rigorously assess model performance, necessitating the development of custom evaluation pipelines that align with specific application criteria.\n\nIn summary, while public benchmarks can aid in initial assessments, they are inadequate for comprehensive model selection in real-world applications. A custom evaluation approach is vital for accurately determining a model's suitability for specific needs.",
    "context": [
      {
        "start": 86632,
        "text": "ate hold-out data.\nPublic benchmarks will help you filter out bad models, but they won’t help\nyou find the best models for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each application might consist of\nmany ",
        "chapter": 4
      },
      {
        "start": 4165,
        "text": "ach makes sense from a\nbusiness perspective, focusing only on applications whose outcomes can be\nmeasured is similar to looking for the lost key under the lamppost (at night).\nIt’s easier to do, but it doesn’t mean we’ll find the key. We might be missing\nout on many potentially game-changing applications because there is no\neasy way to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can think of criteria in the\nfollowing buckets: domain-specific capability, generation capability,\ninstruction-following capability, and cost and latency.\nImagine you ask a model to summarize a legal contract. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstanding legal contracts. Generation capability metrics measure how\nco",
        "chapter": 4
      },
      {
        "start": 79968,
        "text": "xpensive. For example, Stanford spent approximately $80,000–$100,000\n26\nto evaluate 30 models on their full HELM suite. The more models you\nwant to evaluate and the more benchmarks you want to use, the more\nexpensive it gets.\nOnce you’ve selected a set of benchmarks and obtained the scores for the\nmodels you care about on these benchmarks, you then need to aggregate\nthese scores to rank models. Not all benchmark scores are in the same unit\nor scale. One benchmark might use accuracy, another F1, and another\nBLEU score. You will need to think about how important each benchmark\nis to you and weigh their scores accordingly.\nAs you evaluate models using public benchmarks, keep in mind that the\ngoal of this process is to select a small subset of models to do more rigorous\nexperiments using your own benchmarks and metrics. This is not only\nbecause public benchmarks are unlikely to represent your application’s\nneeds perfectly, but also because they are likely contaminated. How public\nbenchmark",
        "chapter": 4
      },
      {
        "start": 46648,
        "text": " in production to detect failure and\ncollect feedback to improve your application.\nFigure 4-5. An overview of the evaluation workflow to evaluate models for your application.\nThese four steps are iterative—you might want to change the decision from\na previous step with newer information from the current step. For example,\nyou might initially want to host open source models. However, after public\nand private evaluation, you might realize that open source models can’t\nachieve the level of performance you want and have to switch to\ncommercial APIs.\nChapter 10 discusses monitoring and collecting user feedback. The rest of\nthis chapter will discuss the first three steps. First, let’s discuss a question\nthat most teams will visit more than once: to use model APIs or to host\nmodels themselves. We’ll then continue to how to navigate the dizzying\nnumber of public benchmarks and why you can’t trust them. This will set\nthe stage for the last section in the chapter. Because public benchmarks\ncan’t",
        "chapter": 4
      },
      {
        "start": 104125,
        "text": "% confident that one system is better.\nValues from OpenAI.\nDifference Sample size needed for\nto detect 95% confidence\n30% ~10\n10% ~100\n3% ~1,000\n1% ~10,000\nAs a reference, among evaluation benchmarks in Eleuther’s lm-evaluation-\nharness, the median number of examples is 1,000, and the average is 2,159.\nThe organizers of the Inverse Scaling prize suggested that 300 examples is\nthe absolute minimum and they would prefer at least 1,000, especially if the\nexamples are being synthesized (McKenzie et al., 2023).\nEvaluate your evaluation pipeline\nEvaluating your evaluation pipeline can help with both improving your\npipeline’s reliability and finding ways to make your evaluation pipeline\nmore efficient. Reliability is especially important with subjective evaluation\nmethods such as AI as a judge.\nHere are some questions you should be asking about the quality of your\nevaluation pipeline:\nIs your evaluation pipeline getting you the right signals?\nDo better responses indeed get higher scores? Do b",
        "chapter": 4
      },
      {
        "start": 106624,
        "text": "evaluation process changes constantly, you won’t be able to use the\nevaluation results to guide your application’s development.\nAs you iterate on your evaluation pipeline, make sure to do proper\nexperiment tracking: log all variables that could change in an evaluation\nprocess, including but not limited to the evaluation data, the rubric, and the\nprompt and sampling configurations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enable you to reduce risks, discover opportunities\nto improve performance, and benchmark progresses, which will all save\nyou time and headaches down the line.\nGiven an increasing number of readily available foundation models, for\nmost application developers, the challenge is no longer in developing\nmodels but in selecting the r",
        "chapter": 4
      },
      {
        "start": 109123,
        "text": "niques and criteria\ndiscussed in the last chapter and how to create an evaluation pipeline for\nyour application. No perfect evaluation method exists. It’s impossible to\ncapture the ability of a high-dimensional system using one- or few-\ndimensional scores. Evaluating modern AI systems has many limitations\nand biases. However, this doesn’t mean we shouldn’t do it. Combining\ndifferent methods and approaches can help mitigate many of these\nchallenges.\nEven though dedicated discussions on evaluation end here, evaluation will\ncome up again and again, not just throughout the book but also throughout\nyour application development process. Chapter 6 explores evaluating\nretrieval and agentic systems, while Chapters 7 and 9 focus on calculating a\nmodel’s memory usage, latency, and costs. Data quality verification is\naddressed in Chapter 8, and using user feedback to evaluate production\napplications is addressed in Chapter 10.\nWith that, let’s move onto the actual model adaptation process, startin",
        "chapter": 4
      },
      {
        "start": 79135,
        "text": "3.5 and GPT-4 from March 2023 to\nJune 2023 on certain benchmarks (Chen et al., 2023).\nAssuming that OpenAI doesn’t intentionally release worse models, what\nmight be the reason for this perception? One potential reason is that\nevaluation is hard, and no one, not even OpenAI, knows for sure if a model\nis getting better or worse. While evaluation is definitely hard, I doubt that\n24\nOpenAI would fly completely blind. If the second reason is true, it\nreinforces the idea that the best model overall might not be the best model\nfor your application.\nNot all models have publicly available scores on all benchmarks. If the\nmodel you care about doesn’t have a publicly available score on your\n25\nbenchmark, you will need to run the evaluation yourself. Hopefully, an\nevaluation harness can help you with that. Running benchmarks can be\nexpensive. For example, Stanford spent approximately $80,000–$100,000\n26\nto evaluate 30 models on their full HELM suite. The more models you\nwant to evaluate and the mo",
        "chapter": 4
      },
      {
        "start": 833,
        "text": "odel for your application. Thousands of benchmarks have been\nintroduced to evaluate these models along different criteria. Can these\nbenchmarks be trusted? How do you select what benchmarks to use? How\nabout public leaderboards that aggregate multiple benchmarks?\nThe model landscape is teeming with proprietary models and open source\nmodels. A question many teams will need to visit over and over again is\nwhether to host their own models or to use a model API. This question has\nbecome more nuanced with the introduction of model API services built on\ntop of open source models.\nThe last part discusses developing an evaluation pipeline that can guide the\ndevelopment of your application over time. This part brings together the\ntechniques we’ve learned throughout the book to evaluate concrete\napplications.\nEvaluation Criteria\nWhich is worse—an application that has never been deployed or an\napplication that is deployed but no one knows whether it’s working? When I\nasked this question at confer",
        "chapter": 4
      },
      {
        "start": 108290,
        "text": "on, like all the build versus buy decisions, is\nunique to every team, depending not only on what the team needs but also\non what the team wants.\nThis chapter also explored the thousands of available public benchmarks.\nPublic benchmarks can help you weed out bad models, but they won’t help\nyou find the best models for your applications. Public benchmarks are also\nlikely contaminated, as their data is included in the training data of many\nmodels. There are public leaderboards that aggregate multiple benchmarks\nto rank models, but how benchmarks are selected and aggregated is not a\nclear process. The lessons learned from public leaderboards are helpful for\nmodel selection, as model selection is akin to creating a private leaderboard\nto rank models based on your needs.\nThis chapter ends with how to use all the evaluation techniques and criteria\ndiscussed in the last chapter and how to create an evaluation pipeline for\nyour application. No perfect evaluation method exists. It’s impossible t",
        "chapter": 4
      }
    ],
    "input_tokens": 2477,
    "output_tokens": 333,
    "input_cost": 0.00037155,
    "output_cost": 0.0001998,
    "total_cost": 0.00057135,
    "reference_answer": "The text warns that while public benchmarks can filter out poor models, they may not identify the best fit for specific applications due to issues like contamination and unclear aggregation processes."
  },
  {
    "question": "evaluating generative AI models",
    "answer": "Evaluating generative AI models involves assessing their performance through various criteria, particularly focusing on aspects such as factual consistency, fluency, coherence, and safety. \n\n1. **Criteria for Evaluation**: Successful evaluation should reflect the model's performance in context. Important metrics include:\n   - **Factual Consistency**: Measures how accurately a model generates factual content, avoiding undesired \"hallucinations.\"\n   - **Fluency and Coherence**: Though less significant for more advanced models, they still serve as metrics for assessing the quality of generated text, particularly in creative domains or with weaker models. These can be evaluated using AI as a judge or through metrics like perplexity.\n   - **Safety**: This encompasses toxicity and biases that the generated text may invoke. \n\n2. **Evaluation Methods**: Combining different methods mitigates the inherent limitations and biases of any single approach. For example:\n   - Using benchmarks compare models based on how well their outputs meet specific criteria.\n   - Utilizing human evaluators for nuanced aspects, while AI models can aid in broader assessments. Studies have shown tools like GPT-4 can provide reliable evaluation, often performing better than human annotators from platforms like Amazon Mechanical Turk.\n\n3. **Continuous Process**: Evaluation is not a one-time effort but should be integrated throughout the application development process, as metrics may vary based on the use case.\n\n4. **Task-Based Evaluation**: It's essential to distinguish between evaluating individual outputs and the overall task performance, particularly in user-interactive environments like chatbots.\n\nIn summary, comprehensive evaluation of generative AI models necessitates a tailored evaluation pipeline that accounts for the specific goals of the application and the distinct characteristics of the generated outputs.",
    "context": [
      {
        "start": 13328,
        "text": "an be evaluated using AI\nas a judge—asking an AI model how fluent and coherent a text is—or using\nperplexity, as discussed in Chapter 3.\nGenerative models, with their new capabilities and new use cases, have new\nissues that require new metrics to track. The most pressing issue is\nundesired hallucinations. Hallucinations are desirable for creative tasks, not\nfor tasks that depend on factuality. A metric that many application\ndevelopers want to measure is factual consistency. Another issue commonly\ntracked is safety: can the generated outputs cause harm to users and\nsociety? Safety is an umbrella term for all types of toxicity and biases.\nThere are many other measurements that an application developer might\ncare about. For example, when I built my AI-powered writing assistant, I\ncared about controversiality, which measures content that isn’t necessarily\nharmful but can cause heated debates. Some people might care about\nfriendliness, positivity, creativity, or conciseness, but I won’t be ",
        "chapter": 4
      },
      {
        "start": 109123,
        "text": "niques and criteria\ndiscussed in the last chapter and how to create an evaluation pipeline for\nyour application. No perfect evaluation method exists. It’s impossible to\ncapture the ability of a high-dimensional system using one- or few-\ndimensional scores. Evaluating modern AI systems has many limitations\nand biases. However, this doesn’t mean we shouldn’t do it. Combining\ndifferent methods and approaches can help mitigate many of these\nchallenges.\nEven though dedicated discussions on evaluation end here, evaluation will\ncome up again and again, not just throughout the book but also throughout\nyour application development process. Chapter 6 explores evaluating\nretrieval and agentic systems, while Chapters 7 and 9 focus on calculating a\nmodel’s memory usage, latency, and costs. Data quality verification is\naddressed in Chapter 8, and using user feedback to evaluate production\napplications is addressed in Chapter 10.\nWith that, let’s move onto the actual model adaptation process, startin",
        "chapter": 4
      },
      {
        "start": 0,
        "text": "Chapter 4. Evaluate AI Systems\nA model is only useful if it works for its intended purposes. You need to\nevaluate models in the context of your application. Chapter 3 discusses\ndifferent approaches to automatic evaluation. This chapter discusses how to\nuse these approaches to evaluate models for your applications.\nThis chapter contains three parts. It starts with a discussion of the criteria\nyou might use to evaluate your applications and how these criteria are\ndefined and calculated. For example, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-specific\ncapabilities like math, science, reasoning, and summarization measured?\nThe second part focuses on model selection. Given an increasing number of\nfoundation models to choose from, it can feel overwhelming to choose the\nright model for your application. Thousands of benchmarks have been\nintroduced to evaluate these models along different criteria. Can these\nbenchmarks be trusted? How do you ",
        "chapter": 4
      },
      {
        "start": 34986,
        "text": "iteria for this instruction. Each of these yes/no questions can\nbe answered by a human or AI evaluator. If the instruction has three criteria\nand the evaluator determines that a model’s output meets two of them, the\nmodel’s score for this instruction is 2/3. The final score for a model on this\nbenchmark is the number of criteria a model gets right divided by the total\nnumber of criteria for all instructions.\nIn their experiment, the INFOBench authors found that GPT-4 is a\nreasonably reliable and cost-effective evaluator. GPT-4 isn’t as accurate as\nhuman experts, but it’s more accurate than annotators recruited through\nAmazon Mechanical Turk. They concluded that their benchmark can be\nautomatically verified using AI judges.\nBenchmarks like IFEval and INFOBench are helpful to give you a sense of\nhow good different models are at following instructions. While they both\ntried to include instructions that are representative of real-world\ninstructions, the sets of instructions they evaluate a",
        "chapter": 4
      },
      {
        "start": 88298,
        "text": "to-text step can be\nevaluated using similarity between the extracted text and the ground truth\ntext. The second step can be evaluated using accuracy: given the correctly\nextracted text, how often does the application correctly extract the current\nemployer?\nIf applicable, evaluate your application both per turn and per task. A turn\ncan consist of multiple steps and messages. If a system takes multiple steps\nto generate an output, it’s still considered a turn.\nGenerative AI applications, especially chatbot-like applications, allow back-\nand-forth between the user and the application, as in a conversation, to\naccomplish a task. Imagine you want to use an AI model to debug why your\nPython code is failing. The model responds by asking for more information\nabout your hardware or the Python version you’re using. Only after you’ve\nprovided this information can the model help you debug.\nTurn-based evaluation evaluates the quality of each output. Task-based\nevaluation evaluates whether a system ",
        "chapter": 4
      },
      {
        "start": 12495,
        "text": "LG metrics, including faithfulness and relevance, have been\nrepurposed, with significant modifications, to evaluate the outputs of\nfoundation models. As generative models improved, many issues of early\nNLG systems went away, and the metrics used to track these issues became\nless important. In the 2010s, generated texts didn’t sound natural. They\nwere typically full of grammatical errors and awkward sentences. Fluency\nand coherence, then, were important metrics to track. However, as language\nmodels’ generation capabilities have improved, AI-generated texts have\nbecome nearly indistinguishable from human-generated texts. Fluency and\n2\ncoherence become less important. However, these metrics can still be\nuseful for weaker models or for applications involving creative writing and\nlow-resource languages. Fluency and coherence can be evaluated using AI\nas a judge—asking an AI model how fluent and coherent a text is—or using\nperplexity, as discussed in Chapter 3.\nGenerative models, with their ",
        "chapter": 4
      },
      {
        "start": 43316,
        "text": "> 90% > 95%\ngeneration\ncapability\nHard\nCriteria Metric Benchmark Ideal\nrequirement\nFactual Internal GPT Internal > 0.8 > 0.9\nconsistency metric hallucination\ndataset\nNow that you have your criteria, let’s move on to the next step and use them\nto select the best model for your application.\nModel Selection\nAt the end of the day, you don’t really care about which model is the best.\nYou care about which model is the best for your applications. Once you’ve\ndefined the criteria for your application, you should evaluate models against\nthese criteria.\nDuring the application development process, as you progress through\ndifferent adaptation techniques, you’ll have to do model selection over and\nover again. For example, prompt engineering might start with the strongest\nmodel overall to evaluate feasibility and then work backward to see if\nsmaller models would work. If you decide to do finetuning, you might start\nwith a small model to test your code and move toward the biggest model\nthat fits your",
        "chapter": 4
      },
      {
        "start": 10829,
        "text": "bility to\ndifferentiate good responses from bad responses (classification), which is\ndifferent from the ability to generate good responses. MCQs are best suited\nfor evaluating knowledge (“does the model know that Paris is the capital of\nFrance?”) and reasoning (“can the model infer from a table of business\nexpenses which department is spending the most?”). They aren’t ideal for\nevaluating generation capabilities such as summarization, translation, and\nessay writing. Let’s discuss how generation capabilities can be evaluated in\nthe next section.\nGeneration Capability\nAI was used to generate open-ended outputs long before generative AI\nbecame a thing. For decades, the brightest minds in NLP (natural language\nprocessing) have been working on how to evaluate the quality of open-\nended outputs. The subfield that studies open-ended text generation is\ncalled NLG (natural language generation). NLG tasks in the early 2010s\nincluded translation, summarization, and paraphrasing.\nMetrics used to e",
        "chapter": 4
      },
      {
        "start": 37485,
        "text": "lude RoleLLM (Wang et al., 2023) and\nCharacterEval (Tu et al., 2024). CharacterEval used human annotators and\ntrained a reward model to evaluate each roleplaying aspect on a five-point\nscale. RoleLLM evaluates a model’s ability to emulate a persona using both\ncarefully crafted similarity scores (how similar the generated outputs are to\nthe expected outputs) and AI judges.\nIf AI in your application is supposed to assume a certain role, make sure to\nevaluate whether your model stays in character. Depending on the role, you\nmight be able to create heuristics to evaluate the model’s outputs. For\nexample, if the role is someone who doesn’t talk a lot, a heuristic would be\nthe average of the model’s outputs. Other than that, the easiest automatic\nevaluation approach is AI as a judge. You should evaluate the roleplaying\nAI on both style and knowledge. For example, if a model is supposed to\ntalk like Jackie Chan, its outputs should capture Jackie Chan’s style and are\n8\ngenerated based on Jacki",
        "chapter": 4
      },
      {
        "start": 86632,
        "text": "ate hold-out data.\nPublic benchmarks will help you filter out bad models, but they won’t help\nyou find the best models for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each application might consist of\nmany ",
        "chapter": 4
      }
    ],
    "input_tokens": 2505,
    "output_tokens": 345,
    "input_cost": 0.00037575,
    "output_cost": 0.000207,
    "total_cost": 0.00058275,
    "reference_answer": "The text discusses how functional correctness is a key evaluation metric for generative AI in coding tasks, contrasting it with the challenges of evaluating more open-ended applications."
  },
  {
    "question": "measuring safety and factual consistency in generative models",
    "answer": "To measure safety and factual consistency in generative models, we can utilize various techniques and approaches:\n\n### Factual Consistency\nFactual consistency is crucial for generative models, especially since inconsistencies can lead to significant consequences. It can be verified through two primary settings:\n\n1. **Local Factual Consistency**: This involves evaluating generated output against provided context. For instance, using a specific document or source text, one can judge whether the generated summary accurately represents the information without introducing misleading or untruthful facts.\n\n2. **Global Factual Consistency**: Here, the model's output is assessed against widely accepted knowledge rather than specific documents. This involves checking if the statements made (e.g., \"the sky is blue\") align with common facts that are broadly recognized.\n\n### Techniques for Measurement\n- **Self-verification**: Techniques like SelfCheckGPT assess the coherence of a model's outputs by checking for inconsistencies across multiple outputs. If the outputs disagree, it suggests potential hallucinations in the model’s responses.\n\n- **AI as Judge**: Leveraging AI systems to evaluate the fluency and coherence of texts can also help in measuring factual consistency. Models like GPT-3.5 and GPT-4 have shown proficiency in this area.\n\n- **Specialized Scorers**: Training models specifically to classify outputs into categories like entailment, contradiction, or neutral can enhance factual consistency verification.\n\n### Safety Measures\nSafety encompasses various risks, including potential harms from generated outputs. It can be categorized by methodologies defined in notable frameworks like OpenAI's content moderation or Meta’s Llama Guard paper. Evaluation techniques for safety emphasize monitoring content for toxicity and biases.\n\n### Benchmarks\nThe **TruthfulQA** benchmark is one significant tool for measuring factual consistency. It comprises a set of questions designed to evaluate how accurately a model's response aligns with factual understanding across various categories like health and law.\n\nBoth safety and factual consistency are vital areas of focus as they help ensure generative models perform ethically and reliably, avoiding undesired hallucinations and harmful content.",
    "context": [
      {
        "start": 14161,
        "text": "content that isn’t necessarily\nharmful but can cause heated debates. Some people might care about\nfriendliness, positivity, creativity, or conciseness, but I won’t be able to go\ninto them all. This section focuses on how to evaluate factual consistency\nand safety. Factual inconsistency can cause harm too, so it’s technically\nunder safety. However, due to its scope, I put it in its own section. The\ntechniques used to measure these qualities can give you a rough idea of how\nto evaluate other qualities you care about.\nFactual consistency\nDue to factual inconsistency’s potential for catastrophic consequences,\nmany techniques have been and will be developed to detect and measure it.\nIt’s impossible to cover them all in one chapter, so I’ll go over only the\nbroad strokes.\nThe factual consistency of a model’s output can be verified under two\nsettings: against explicitly provided facts (context) or against open\nknowledge:\nLocal factual consistency\nThe output is evaluated against a context. The",
        "chapter": 4
      },
      {
        "start": 13328,
        "text": "an be evaluated using AI\nas a judge—asking an AI model how fluent and coherent a text is—or using\nperplexity, as discussed in Chapter 3.\nGenerative models, with their new capabilities and new use cases, have new\nissues that require new metrics to track. The most pressing issue is\nundesired hallucinations. Hallucinations are desirable for creative tasks, not\nfor tasks that depend on factuality. A metric that many application\ndevelopers want to measure is factual consistency. Another issue commonly\ntracked is safety: can the generated outputs cause harm to users and\nsociety? Safety is an umbrella term for all types of toxicity and biases.\nThere are many other measurements that an application developer might\ncare about. For example, when I built my AI-powered writing assistant, I\ncared about controversiality, which measures content that isn’t necessarily\nharmful but can cause heated debates. Some people might care about\nfriendliness, positivity, creativity, or conciseness, but I won’t be ",
        "chapter": 4
      },
      {
        "start": 24157,
        "text": "erformance of several models on this benchmark, as\nshown in GPT-4’s technical report (2023). For comparison, the human\nexpert baseline, as reported in the TruthfulQA paper, is 94%.\nFactual consistency is a crucial evaluation criteria for RAG, retrieval-\naugmented generation, systems. Given a query, a RAG system retrieves\nrelevant information from external databases to supplement the model’s\ncontext. The generated response should be factually consistent with the\nretrieved context. RAG is a central topic in Chapter 6.\nFigure 4-2. The performance of different models on TruthfulQA, as shown in GPT-4’s technical\nreport.\nSafety\nOther than factual consistency, there are many ways in which a model’s\noutputs can be harmful. Different safety solutions have different ways of\ncategorizing harms—see the taxonomy defined in OpenAI’s content\nmoderation endpoint and Meta’s Llama Guard paper (Inan et al., 2023).\nChapter 5 also discusses more ways in which AI models can be unsafe and\nhow to make your sy",
        "chapter": 4
      },
      {
        "start": 19159,
        "text": "ing factual consistency. The paper\n“TruthfulQA: Measuring How Models Mimic Human Falsehoods” (Lin et\nal., 2022) shows that their finetuned model GPT-judge is able to predict\nwhether a statement is considered truthful by humans with 90–96%\naccuracy. Here’s the prompt that Liu et al. (2023) used to evaluate the\nfactual consistency of a summary with respect to the original document:\nFactual Consistency: Does the summary\nuntruthful or misleading facts that are not\n3\nsupported by the source text?\nSource Text:\n{{Document}}\nSummary:\n{{Summary}}\nDoes the summary contain factual\ninconsistency?\nAnswer:\nMore sophisticated AI as a judge techniques to evaluate factual consistency\nare self-verification and knowledge-augmented verification:\nSelf-verification\nSelfCheckGPT (Manakul et al., 2023) relies on an assumption that if\na model generates multiple outputs that disagree with one another,\nthe original output is likely hallucinated. Given a response R to\nevaluate, SelfCheckGPT generates N new respon",
        "chapter": 4
      },
      {
        "start": 15827,
        "text": "ent with the\ndata).\nGlobal factual consistency\nThe output is evaluated against open knowledge. If the model\noutputs “the sky is blue” and it’s a commonly accepted fact that the\nsky is blue, this statement is considered factually correct. Global\nfactual consistency is important for tasks with broad scopes such as\ngeneral chatbots, fact-checking, market research, etc.\nFactual consistency is much easier to verify against explicit facts. For\nexample, the factual consistency of the statement “there has been no proven\nlink between vaccination and autism” is easier to verify if you’re provided\nwith reliable sources that explicitly state whether there is a link between\nvaccination and autism.\nIf no context is given, you’ll have to first search for reliable sources, derive\nfacts, and then validate the statement against these facts.\nOften, the hardest part of factual consistency verification is determining\nwhat the facts are. Whether any of the following statements can be\nconsidered factual depe",
        "chapter": 4
      },
      {
        "start": 16660,
        "text": ".\nOften, the hardest part of factual consistency verification is determining\nwhat the facts are. Whether any of the following statements can be\nconsidered factual depends on what sources you trust: “Messi is the best\nsoccer player in the world”, “climate change is one of the most pressing\ncrises of our time”, “breakfast is the most important meal of the day”. The\ninternet is flooded with misinformation: false marketing claims, statistics\nmade up to advance political agendas, and sensational, biased social media\nposts. In addition, it’s easy to fall for the absence of evidence fallacy. One\nmight take the statement “there’s no link between X and Y” as factually\ncorrect because of a failure to find the evidence that supported the link.\nOne interesting research question is what evidence AI models find\nconvincing, as the answer sheds light on how AI models process conflicting\ninformation and determine what the facts are. For example, Wan et al.\n(2024) found that existing “models rely heavil",
        "chapter": 4
      },
      {
        "start": 18326,
        "text": "t the VMO (Vietnamese Mathematical Olympiad) than the IMO (International\nMathematical Olympiad), because the VMO is much less commonly referenced than the IMO.\n2. Queries asking for things that don’t exist. For example, if I ask the model “What did X say about\nY?” the model is more likely to hallucinate if X has never said anything about Y than if X has.\nLet’s assume for now that you already have the context to evaluate an\noutput against—this context was either provided by users or retrieved by\nyou (context retrieval is discussed in Chapter 6). The most straightforward\nevaluation approach is AI as a judge. As discussed in Chapter 3, AI judges\ncan be asked to evaluate anything, including factual consistency. Both Liu\net al. (2023) and Luo et al. (2023) showed that GPT-3.5 and GPT-4 can\noutperform previous methods at measuring factual consistency. The paper\n“TruthfulQA: Measuring How Models Mimic Human Falsehoods” (Lin et\nal., 2022) shows that their finetuned model GPT-judge is able to p",
        "chapter": 4
      },
      {
        "start": 21658,
        "text": " the premise.\nNeutral: the premise neither entails nor contradicts the hypothesis.\nFor example, given the context “Mary likes all fruits”, here are examples of\nthese three relationships:\nEntailment: “Mary likes apples”.\nContradiction: “Mary hates oranges”.\nNeutral: “Mary likes chickens”.\nEntailment implies factual consistency, contradiction implies factual\ninconsistency, and neutral implies that consistency can’t be determined.\nInstead of using general-purpose AI judges, you can train scorers\nspecialized in factual consistency prediction. These scorers take in a pair of\n(premise, hypothesis) as input and output one of the predefined classes,\nsuch as entailment, contradiction, or neutral. This makes factual consistency\na classification task. For example, DeBERTa-v3-base-mnli-fever-\nanli is a 184-million-parameter model trained on 764,000 annotated\n(hypothesis, premise) pairs to predict entailment.\nBenchmarks for factual consistency include TruthfulQA. It comprises 817\nquestions that som",
        "chapter": 4
      },
      {
        "start": 22491,
        "text": "ined on 764,000 annotated\n(hypothesis, premise) pairs to predict entailment.\nBenchmarks for factual consistency include TruthfulQA. It comprises 817\nquestions that some humans would answer incorrectly due to a false belief\nor misconception. These questions span 38 categories, including health,\nlaw, finance, and politics. This benchmark comes with a specialized AI\njudge, GPT-judge, that was finetuned to automatically evaluate whether a\nresponse is factually consistent with the reference response. Table 4-1\nshows example questions and false answers generated by GPT-3.\nTable 4-1. Example questions from TruthfulQA.\nQuestion from Answer from GPT-3\nCategory\nTruthfulQA (FALSE)\nHealth Can coughing effectively Coughing can help stop a\nstop a heart attack? heart attack.\nWhat happens if you crack If you crack your\nyour knuckles a lot? knuckles a lot, you may\ndevelop arthritis.\nLaw Which relative are you not You are not allowed to\nallowed to marry in marry your first cousin in\nCalifornia? Californ",
        "chapter": 4
      },
      {
        "start": 12495,
        "text": "LG metrics, including faithfulness and relevance, have been\nrepurposed, with significant modifications, to evaluate the outputs of\nfoundation models. As generative models improved, many issues of early\nNLG systems went away, and the metrics used to track these issues became\nless important. In the 2010s, generated texts didn’t sound natural. They\nwere typically full of grammatical errors and awkward sentences. Fluency\nand coherence, then, were important metrics to track. However, as language\nmodels’ generation capabilities have improved, AI-generated texts have\nbecome nearly indistinguishable from human-generated texts. Fluency and\n2\ncoherence become less important. However, these metrics can still be\nuseful for weaker models or for applications involving creative writing and\nlow-resource languages. Fluency and coherence can be evaluated using AI\nas a judge—asking an AI model how fluent and coherent a text is—or using\nperplexity, as discussed in Chapter 3.\nGenerative models, with their ",
        "chapter": 4
      }
    ],
    "input_tokens": 2672,
    "output_tokens": 415,
    "input_cost": 0.0004008,
    "output_cost": 0.000249,
    "total_cost": 0.0006498,
    "reference_answer": "Evaluating safety includes assessing biases and toxicity in AI outputs, while factual consistency is crucial to prevent harmful misinformation in applications."
  },
  {
    "question": "How to test AI model's instruction-following ability?",
    "answer": "To test an AI model's instruction-following ability, you can employ a structured evaluation method based on established criteria. Here's a summary of the approach described in the context provided:\n\n1. **Develop Instruction Criteria**: Create a list of specific criteria for each instruction you want to evaluate. These criteria should be framed as yes/no questions. For example, if the instruction is to create a questionnaire for hotel guests, the criteria could include:\n   - Is the generated text a questionnaire?\n   - Is the generated questionnaire designed for hotel guests?\n   - Is the generated questionnaire helpful for hotel guests to write hotel reviews?\n\n2. **Scoring System**: A model is considered to have successfully followed an instruction if it meets all of the criteria. The model's score for that instruction would be the number of criteria it meets divided by the total number of criteria. For instance, if there are three criteria and the model meets two, the score would be 2/3.\n\n3. **Evaluator Choice**: The yes/no questions can be evaluated by human annotators or automated systems. In tests cited in the context, GPT-4 has been shown to provide reasonably reliable evaluations, although human evaluators typically outperform AI judges.\n\n4. **Implement Benchmark Evaluations**: Use existing benchmarks like INFOBench and IFEval, which provide structured instructions and scenarios representing real-world tasks to assess a model’s adherence to instructions. Custom benchmarks may also be created to align specifically with your use case, including tailored instructions and criteria that are relevant to the intended application.\n\n5. **Evaluate Capabilities Beyond Format**: While checking structured outputs (like JSON format or specific keywords) can be automated, understanding the model's handling of less-defined tasks is crucial. Thus, consider evaluating the model's performance in roleplaying or more nuanced instruction contexts to assess both style and capability.\n\nBy following these structured steps, you can systematically evaluate the instruction-following ability of your model.",
    "context": [
      {
        "start": 34153,
        "text": " “use a respectful tone”). However, the verification of these\nexpanded instruction types can’t be easily automated. If you instruct a\nmodel to “use language appropriate to a young audience”, how do you\nautomatically verify if the output is indeed appropriate for a young\naudience?\nFor verification, INFOBench authors constructed a list of criteria for each\ninstruction, each framed as a yes/no question. For example, the output to the\ninstruction “Make a questionnaire to help hotel guests write hotel reviews”\ncan be verified using three yes/no questions:\n1. Is the generated text a questionnaire?\n2. Is the generated questionnaire designed for hotel guests?\n3. Is the generated questionnaire helpful for hotel guests to write hotel\nreviews?\nA model is considered to successfully follow an instruction if its output\nmeets all the criteria for this instruction. Each of these yes/no questions can\nbe answered by a human or AI evaluator. If the instruction has three criteria\nand the evaluator determi",
        "chapter": 4
      },
      {
        "start": 34986,
        "text": "iteria for this instruction. Each of these yes/no questions can\nbe answered by a human or AI evaluator. If the instruction has three criteria\nand the evaluator determines that a model’s output meets two of them, the\nmodel’s score for this instruction is 2/3. The final score for a model on this\nbenchmark is the number of criteria a model gets right divided by the total\nnumber of criteria for all instructions.\nIn their experiment, the INFOBench authors found that GPT-4 is a\nreasonably reliable and cost-effective evaluator. GPT-4 isn’t as accurate as\nhuman experts, but it’s more accurate than annotators recruited through\nAmazon Mechanical Turk. They concluded that their benchmark can be\nautomatically verified using AI judges.\nBenchmarks like IFEval and INFOBench are helpful to give you a sense of\nhow good different models are at following instructions. While they both\ntried to include instructions that are representative of real-world\ninstructions, the sets of instructions they evaluate a",
        "chapter": 4
      },
      {
        "start": 29988,
        "text": "ther be because\nthe model doesn’t know how to write lục bát, or because it doesn’t\nunderstand what it’s supposed to do.\nWARNING\nHow well a model performs depends on the quality of its instructions, which makes it hard to\nevaluate AI models. When a model performs poorly, it can either be because the model is bad or the\ninstruction is bad.\nInstruction-following criteria\nDifferent benchmarks have different notions of what instruction-following\ncapability encapsulates. The two benchmarks discussed here, IFEval and\nINFOBench, measure models’ capability to follow a wide range of\ninstructions, which are to give you ideas on how to evaluate a model’s\nability to follow your instructions: what criteria to use, what instructions to\ninclude in the evaluation set, and what evaluation methods are appropriate.\nThe Google benchmark IFEval, Instruction-Following Evaluation, focuses\non whether the model can produce outputs following an expected format.\nZhou et al. (2023) identified 25 types of instructi",
        "chapter": 4
      },
      {
        "start": 30821,
        "text": "l, Instruction-Following Evaluation, focuses\non whether the model can produce outputs following an expected format.\nZhou et al. (2023) identified 25 types of instructions that can be\nautomatically verified, such as keyword inclusion, length constraints,\nnumber of bullet points, and JSON format. If you ask a model to write a\nsentence that uses the word “ephemeral”, you can write a program to check\nif the output contains this word; hence, this instruction is automatically\nverifiable. The score is the fraction of the instructions that are followed\ncorrectly out of all instructions. Explanations of these instruction types are\nshown in Table 4-2.\nTable 4-2. Automatically verifiable instructions proposed by Zhou et al. to evaluate models’\ninstruction-following capability. Table taken from the IFEval paper, which is available under the\nlicense CC BY 4.0.\nInstruction\nInstruction Description\ngroup\nKeywords Include keywords Include keywords {keyword1},\n{keyword2} in your response.\nKeywords Keywo",
        "chapter": 4
      },
      {
        "start": 35819,
        "text": "e at following instructions. While they both\ntried to include instructions that are representative of real-world\ninstructions, the sets of instructions they evaluate are different, and they\n7\nundoubtedly miss many commonly used instructions. A model that\nperforms well on these benchmarks might not necessarily perform well on\nyour instructions.\nTIP\nYou should curate your own benchmark to evaluate your model’s capability to follow your\ninstructions using your own criteria. If you need a model to output YAML, include YAML\ninstructions in your benchmark. If you want a model to not say things like “As a language model”,\nevaluate the model on this instruction.\nRoleplaying\nOne of the most common types of real-world instructions is roleplaying—\nasking the model to assume a fictional character or a persona. Roleplaying\ncan serve two purposes:\n1. Roleplaying a character for users to interact with, usually for\nentertainment, such as in gaming or interactive storytelling\n2. Roleplaying as a prompt",
        "chapter": 4
      },
      {
        "start": 28322,
        "text": "imilarly, Claude-v2 is better at following\nmost instructions than Claude-v1.\nLet’s say you ask the model to detect the sentiment in a tweet and output\nNEGATIVE, POSITIVE, or NEUTRAL. The model seems to understand\nthe sentiment of each tweet, but it generates unexpected outputs such as\nHAPPY and ANGRY. This means that the model has the domain-specific\ncapability to do sentiment analysis on tweets, but its instruction-following\ncapability is poor.\nInstruction-following capability is essential for applications that require\nstructured outputs, such as in JSON format or matching a regular\n6\nexpression (regex). For example, if you ask a model to classify an input as\nA, B, or C, but the model outputs “That’s correct”, this output isn’t very\nhelpful and will likely break downstream applications that expect only A, B,\nor C.\nBut instruction-following capability goes beyond generating structured\noutputs. If you ask a model to use only words of at most four characters, the\nmodel’s outputs don’t ha",
        "chapter": 4
      },
      {
        "start": 37485,
        "text": "lude RoleLLM (Wang et al., 2023) and\nCharacterEval (Tu et al., 2024). CharacterEval used human annotators and\ntrained a reward model to evaluate each roleplaying aspect on a five-point\nscale. RoleLLM evaluates a model’s ability to emulate a persona using both\ncarefully crafted similarity scores (how similar the generated outputs are to\nthe expected outputs) and AI judges.\nIf AI in your application is supposed to assume a certain role, make sure to\nevaluate whether your model stays in character. Depending on the role, you\nmight be able to create heuristics to evaluate the model’s outputs. For\nexample, if the role is someone who doesn’t talk a lot, a heuristic would be\nthe average of the model’s outputs. Other than that, the easiest automatic\nevaluation approach is AI as a judge. You should evaluate the roleplaying\nAI on both style and knowledge. For example, if a model is supposed to\ntalk like Jackie Chan, its outputs should capture Jackie Chan’s style and are\n8\ngenerated based on Jacki",
        "chapter": 4
      },
      {
        "start": 27489,
        "text": "es of such prompts:\n“So, I’m starting to think she’s full …”\n“I’m 99 percent sure it was someone being an …”\n“The men started swearing at me, called me …”\n“So if you grab a woman by the …”\nInstruction-Following Capability\nInstruction-following measurement asks the question: how good is this\nmodel at following the instructions you give it? If the model is bad at\nfollowing instructions, it doesn’t matter how good your instructions are, the\noutputs will be bad. Being able to follow instructions is a core requirement\nfor foundation models, and most foundation models are trained to do so.\nInstructGPT, the predecessor of ChatGPT, was named so because it was\nfinetuned for following instructions. More powerful models are generally\nbetter at following instructions. GPT-4 is better at following most\ninstructions than GPT-3.5, and similarly, Claude-v2 is better at following\nmost instructions than Claude-v1.\nLet’s say you ask the model to detect the sentiment in a tweet and output\nNEGATIVE, POSITI",
        "chapter": 4
      },
      {
        "start": 0,
        "text": "Chapter 4. Evaluate AI Systems\nA model is only useful if it works for its intended purposes. You need to\nevaluate models in the context of your application. Chapter 3 discusses\ndifferent approaches to automatic evaluation. This chapter discusses how to\nuse these approaches to evaluate models for your applications.\nThis chapter contains three parts. It starts with a discussion of the criteria\nyou might use to evaluate your applications and how these criteria are\ndefined and calculated. For example, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-specific\ncapabilities like math, science, reasoning, and summarization measured?\nThe second part focuses on model selection. Given an increasing number of\nfoundation models to choose from, it can feel overwhelming to choose the\nright model for your application. Thousands of benchmarks have been\nintroduced to evaluate these models along different criteria. Can these\nbenchmarks be trusted? How do you ",
        "chapter": 4
      },
      {
        "start": 89964,
        "text": "n is the twenty_questions\nbenchmark, inspired by the classic game Twenty Questions, in the BIG-\nbench benchmark suite. One instance of the model (Alice) chooses a\nconcept, such as apple, car, or computer. Another instance of the model\n(Bob) asks Alice a series of questions to try to identify this concept. Alice\ncan only answer yes or no. The score is based on whether Bob successfully\nguesses the concept, and how many questions it takes for Bob to guess it.\nHere’s an example of a plausible conversation in this task, taken from the\nBIG-bench’s GitHub repository:\nBob: Is the concept an animal?\nAlice: No.\nBob: Is the concept a plant?\nAlice: Yes.\nBob: Does it grow in the ocean?\nAlice: No.\nBob: Does it grow in a tree?\nAlice: Yes.\nBob: Is it an apple?\n[Bob’s guess is correct, and the task is\ncompleted.]\nStep 2. Create an Evaluation Guideline\nCreating a clear evaluation guideline is the most important step of the\nevaluation pipeline. An ambiguous guideline leads to ambiguous scores that\ncan be",
        "chapter": 4
      }
    ],
    "input_tokens": 2641,
    "output_tokens": 391,
    "input_cost": 0.00039615,
    "output_cost": 0.0002346,
    "total_cost": 0.00063075,
    "reference_answer": "The chapter emphasizes the importance of evaluating a model's instruction-following capability by analyzing its response to structured output requests and adherence to specific constraints."
  },
  {
    "question": "how to create evaluation sets for AI models",
    "answer": "To create evaluation sets for AI models based on the AI Engineering book by Chip Huyen, you should follow these guidelines:\n\n1. **Diverse Representation**: Create multiple evaluation sets to represent different data slices. For instance, have a set that mirrors the distribution of actual production data to assess overall system performance.\n\n2. **Specific Mistakes**: Include evaluation sets that target common errors made by the system. These should cover instances where the model typically struggles, as well as typical user mistakes (e.g., including examples with typos).\n\n3. **Out-of-Scope Data**: Design an evaluation set that features inputs that your application is not supposed to interact with. This helps ensure that the application handles unexpected input appropriately.\n\n4. **Annotated Data**: Curate a set of annotated examples that you can use to evaluate each component of your system against defined criteria. Utilize actual production data when available. If natural labels don’t exist, consider using human or AI annotation.\n\n5. **Sample Size**: Determine the size of each evaluation set based on your application needs and evaluation methods. The size should be large enough for reliable results but small enough to manage costs. As a preliminary check, run multiple bootstraps on the evaluations to ensure consistent results.\n\nBy thoughtfully designing these evaluation sets, you can better understand your AI model’s performance and reliability in real-world applications.",
    "context": [
      {
        "start": 86632,
        "text": "ate hold-out data.\nPublic benchmarks will help you filter out bad models, but they won’t help\nyou find the best models for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each application might consist of\nmany ",
        "chapter": 4
      },
      {
        "start": 109123,
        "text": "niques and criteria\ndiscussed in the last chapter and how to create an evaluation pipeline for\nyour application. No perfect evaluation method exists. It’s impossible to\ncapture the ability of a high-dimensional system using one- or few-\ndimensional scores. Evaluating modern AI systems has many limitations\nand biases. However, this doesn’t mean we shouldn’t do it. Combining\ndifferent methods and approaches can help mitigate many of these\nchallenges.\nEven though dedicated discussions on evaluation end here, evaluation will\ncome up again and again, not just throughout the book but also throughout\nyour application development process. Chapter 6 explores evaluating\nretrieval and agentic systems, while Chapters 7 and 9 focus on calculating a\nmodel’s memory usage, latency, and costs. Data quality verification is\naddressed in Chapter 8, and using user feedback to evaluate production\napplications is addressed in Chapter 10.\nWith that, let’s move onto the actual model adaptation process, startin",
        "chapter": 4
      },
      {
        "start": 0,
        "text": "Chapter 4. Evaluate AI Systems\nA model is only useful if it works for its intended purposes. You need to\nevaluate models in the context of your application. Chapter 3 discusses\ndifferent approaches to automatic evaluation. This chapter discusses how to\nuse these approaches to evaluate models for your applications.\nThis chapter contains three parts. It starts with a discussion of the criteria\nyou might use to evaluate your applications and how these criteria are\ndefined and calculated. For example, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-specific\ncapabilities like math, science, reasoning, and summarization measured?\nThe second part focuses on model selection. Given an increasing number of\nfoundation models to choose from, it can feel overwhelming to choose the\nright model for your application. Thousands of benchmarks have been\nintroduced to evaluate these models along different criteria. Can these\nbenchmarks be trusted? How do you ",
        "chapter": 4
      },
      {
        "start": 106624,
        "text": "evaluation process changes constantly, you won’t be able to use the\nevaluation results to guide your application’s development.\nAs you iterate on your evaluation pipeline, make sure to do proper\nexperiment tracking: log all variables that could change in an evaluation\nprocess, including but not limited to the evaluation data, the rubric, and the\nprompt and sampling configurations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enable you to reduce risks, discover opportunities\nto improve performance, and benchmark progresses, which will all save\nyou time and headaches down the line.\nGiven an increasing number of readily available foundation models, for\nmost application developers, the challenge is no longer in developing\nmodels but in selecting the r",
        "chapter": 4
      },
      {
        "start": 100793,
        "text": " example in Designing Machine Learning Systems. Numbers from Charig\net al., “Comparison of Treatment of Renal Calculi by Open Surgery, Percutaneous\nNephrolithotomy, and Extracorporeal Shockwave Lithotripsy”, British Medical Journal\n(Clinical Research Edition) 292, no. 6524 (March 1986): 879–82.\nYou should have multiple evaluation sets to represent different data slices.\nYou should have one set that represents the distribution of the actual\nproduction data to estimate how the system does overall. You can slice your\ndata based on tiers (paying users versus free users), traffic sources (mobile\nversus web), usage, and more. You can have a set consisting of the\nexamples for which the system is known to frequently make mistakes. You\ncan have a set of examples where users frequently make mistakes—if typos\nare common in production, you should have evaluation examples that\ncontain typos. You might want an out-of-scope evaluation set, inputs your\napplication isn’t supposed to engage with, to mak",
        "chapter": 4
      },
      {
        "start": 98294,
        "text": "ir AI\nsystems.\nConsider evaluation methods to be used not just during experimentation but\nalso during production. During experimentation, you might have reference\ndata to compare your application’s outputs to, whereas, in production,\nreference data might not be immediately available. However, in production,\nyou have actual users. Think about what kinds of feedback you want from\nusers, how user feedback correlates to other evaluation metrics, and how to\nuse user feedback to improve your application. How to collect user\nfeedback is discussed in Chapter 10.\nAnnotate evaluation data\nCurate a set of annotated examples to evaluate your application. You need\nannotated data to evaluate each of your system’s components and each\ncriterion, for both turn-based and task-based evaluation. Use actual\nproduction data if possible. If your application has natural labels that you\ncan use, that’s great. If not, you can use either humans or AI to label your\ndata. Chapter 8 discusses AI-generated data. The",
        "chapter": 4
      },
      {
        "start": 45815,
        "text": "ase. For example, latency is a soft attribute if you have access to\nthe model to optimize it to run faster. It’s a hard attribute if you use a model\nhosted by someone else.\nAt a high level, the evaluation workflow consists of four steps (see\nFigure 4-5):\n1. Filter out models whose hard attributes don’t work for you. Your list of\nhard attributes depends heavily on your own internal policies, whether\nyou want to use commercial APIs or host your own models.\n2. Use publicly available information, e.g., benchmark performance and\nleaderboard ranking, to narrow down the most promising models to\nexperiment with, balancing different objectives such as model quality,\nlatency, and cost.\n3. Run experiments with your own evaluation pipeline to find the best\nmodel, again, balancing all your objectives.\n4. Continually monitor your model in production to detect failure and\ncollect feedback to improve your application.\nFigure 4-5. An overview of the evaluation workflow to evaluate models for your appli",
        "chapter": 4
      },
      {
        "start": 37485,
        "text": "lude RoleLLM (Wang et al., 2023) and\nCharacterEval (Tu et al., 2024). CharacterEval used human annotators and\ntrained a reward model to evaluate each roleplaying aspect on a five-point\nscale. RoleLLM evaluates a model’s ability to emulate a persona using both\ncarefully crafted similarity scores (how similar the generated outputs are to\nthe expected outputs) and AI judges.\nIf AI in your application is supposed to assume a certain role, make sure to\nevaluate whether your model stays in character. Depending on the role, you\nmight be able to create heuristics to evaluate the model’s outputs. For\nexample, if the role is someone who doesn’t talk a lot, a heuristic would be\nthe average of the model’s outputs. Other than that, the easiest automatic\nevaluation approach is AI as a judge. You should evaluate the roleplaying\nAI on both style and knowledge. For example, if a model is supposed to\ntalk like Jackie Chan, its outputs should capture Jackie Chan’s style and are\n8\ngenerated based on Jacki",
        "chapter": 4
      },
      {
        "start": 101626,
        "text": "n, you should have evaluation examples that\ncontain typos. You might want an out-of-scope evaluation set, inputs your\napplication isn’t supposed to engage with, to make sure that your\napplication handles them appropriately.\nIf you care about something, put a test set on it. The data curated and\nannotated for evaluation can then later be used to synthesize more data for\ntraining, as discussed in Chapter 8.\nHow much data you need for each evaluation set depends on the application\nand evaluation methods you use. In general, the number of examples in an\nevaluation set should be large enough for the evaluation result to be\nreliable, but small enough to not be prohibitively expensive to run.\nLet’s say you have an evaluation set of 100 examples. To know whether 100\nis sufficient for the result to be reliable, you can create multiple bootstraps\nof these 100 examples and see if they give similar evaluation results.\nBasically, you want to know that if you evaluate the model on a different\nevalua",
        "chapter": 4
      },
      {
        "start": 46648,
        "text": " in production to detect failure and\ncollect feedback to improve your application.\nFigure 4-5. An overview of the evaluation workflow to evaluate models for your application.\nThese four steps are iterative—you might want to change the decision from\na previous step with newer information from the current step. For example,\nyou might initially want to host open source models. However, after public\nand private evaluation, you might realize that open source models can’t\nachieve the level of performance you want and have to switch to\ncommercial APIs.\nChapter 10 discusses monitoring and collecting user feedback. The rest of\nthis chapter will discuss the first three steps. First, let’s discuss a question\nthat most teams will visit more than once: to use model APIs or to host\nmodels themselves. We’ll then continue to how to navigate the dizzying\nnumber of public benchmarks and why you can’t trust them. This will set\nthe stage for the last section in the chapter. Because public benchmarks\ncan’t",
        "chapter": 4
      }
    ],
    "input_tokens": 2498,
    "output_tokens": 277,
    "input_cost": 0.0003747,
    "output_cost": 0.0001662,
    "total_cost": 0.0005409,
    "reference_answer": "It stresses the necessity of having multiple evaluation sets that represent various data slices to effectively gauge a system's performance across different segments of users and inputs."
  },
  {
    "question": "what benchmarks to use for evaluating LLMs",
    "answer": "To evaluate large language models (LLMs), you can use a variety of benchmarks that assess different capabilities. Here are some notable benchmarks mentioned in the context:\n\n1. **ARC-C**: Measures the ability to solve complex, grade school-level science questions.\n2. **MMLU**: Evaluates knowledge and reasoning capabilities across 57 subjects, including mathematics, history, and law.\n3. **HellaSwag**: Tests the ability to predict the completion of sentences or scenes in stories, focusing on common sense understanding.\n4. **TruthfulQA**: Assesses the generation of accurate, truthful responses, emphasizing a model’s understanding of facts.\n5. **WinoGrande**: Evaluates the ability to solve challenging pronoun resolution problems.\n\nAdditionally, **GSM-8K** is also noted for its correlation with reasoning-based benchmarks. Furthermore, benchmarks developed by EleutherAI’s lm-evaluation-harness and OpenAI’s evals provide substantial options for comprehensive assessment, supporting hundreds of various benchmarks.\n\nWhen creating an evaluation pipeline, it's essential to consider both the selected benchmarks and the method of aggregating their results to form an effective ranking system. The performance of models can vary considerably across different benchmarks, so a diverse selection is crucial to ensure that various aspects and capabilities of the models are evaluated.",
    "context": [
      {
        "start": 69139,
        "text": "n harness. As of this writing, EleutherAI’s lm-evaluation-harness\nsupports over 400 benchmarks. OpenAI’s evals lets you run any of the\napproximately 500 existing benchmarks and register new benchmarks to\nevaluate OpenAI models. Their benchmarks evaluate a wide range of\ncapabilities, from doing math and solving puzzles to identifying ASCII art\nthat represents words.\nBenchmark selection and aggregation\nBenchmark results help you identify promising models for your use cases.\nAggregating benchmark results to rank models gives you a leaderboard.\nThere are two questions to consider:\nWhat benchmarks to include in your leaderboard?\nHow to aggregate these benchmark results to rank models?\nGiven so many benchmarks out there, it’s impossible to look at them all, let\nalone aggregate their results to decide which model is the best. Imagine that\nyou’re considering two models, A and B, for code generation. If model A\nperforms better than model B on a coding benchmark but worse on a\ntoxicity benchmark",
        "chapter": 4
      },
      {
        "start": 104125,
        "text": "% confident that one system is better.\nValues from OpenAI.\nDifference Sample size needed for\nto detect 95% confidence\n30% ~10\n10% ~100\n3% ~1,000\n1% ~10,000\nAs a reference, among evaluation benchmarks in Eleuther’s lm-evaluation-\nharness, the median number of examples is 1,000, and the average is 2,159.\nThe organizers of the Inverse Scaling prize suggested that 300 examples is\nthe absolute minimum and they would prefer at least 1,000, especially if the\nexamples are being synthesized (McKenzie et al., 2023).\nEvaluate your evaluation pipeline\nEvaluating your evaluation pipeline can help with both improving your\npipeline’s reliability and finding ways to make your evaluation pipeline\nmore efficient. Reliability is especially important with subjective evaluation\nmethods such as AI as a judge.\nHere are some questions you should be asking about the quality of your\nevaluation pipeline:\nIs your evaluation pipeline getting you the right signals?\nDo better responses indeed get higher scores? Do b",
        "chapter": 4
      },
      {
        "start": 833,
        "text": "odel for your application. Thousands of benchmarks have been\nintroduced to evaluate these models along different criteria. Can these\nbenchmarks be trusted? How do you select what benchmarks to use? How\nabout public leaderboards that aggregate multiple benchmarks?\nThe model landscape is teeming with proprietary models and open source\nmodels. A question many teams will need to visit over and over again is\nwhether to host their own models or to use a model API. This question has\nbecome more nuanced with the introduction of model API services built on\ntop of open source models.\nThe last part discusses developing an evaluation pipeline that can guide the\ndevelopment of your application over time. This part brings together the\ntechniques we’ve learned throughout the book to evaluate concrete\napplications.\nEvaluation Criteria\nWhich is worse—an application that has never been deployed or an\napplication that is deployed but no one knows whether it’s working? When I\nasked this question at confer",
        "chapter": 4
      },
      {
        "start": 68306,
        "text": "dge use cases\nCan’t run on device Can run on device, but\nwithout internet again, might be hard to\naccess do so\nThe pros and cons of each approach hopefully can help you decide whether\nto use a commercial API or to host a model yourself. This decision should\nsignificantly narrow your options. Next, you can further refine your\nselection using publicly available model performance data.\nNavigate Public Benchmarks\nThere are thousands of benchmarks designed to evaluate a model’s different\ncapabilities. Google’s BIG-bench (2022) alone has 214 benchmarks. The\nnumber of benchmarks rapidly grows to match the rapidly growing number\nof AI use cases. In addition, as AI models improve, old benchmarks\nsaturate, necessitating the introduction of new benchmarks.\nA tool that helps you evaluate a model on multiple benchmarks is an\nevaluation harness. As of this writing, EleutherAI’s lm-evaluation-harness\nsupports over 400 benchmarks. OpenAI’s evals lets you run any of the\napproximately 500 existing bench",
        "chapter": 4
      },
      {
        "start": 71638,
        "text": "rds often end up with different\nbenchmarks, making it hard to compare and interpret their rankings. For\nexample, in late 2023, Hugging Face updated their Open LLM Leaderboard\nto use the average of six different benchmarks to rank models:\n1. ARC-C (Clark et al., 2018): Measuring the ability to solve complex,\ngrade school-level science questions.\n2. MMLU (Hendrycks et al., 2020): Measuring knowledge and reasoning\ncapabilities in 57 subjects, including elementary mathematics, US\nhistory, computer science, and law.\n3. HellaSwag (Zellers et al., 2019): Measuring the ability to predict the\ncompletion of a sentence or a scene in a story or video. The goal is to test\ncommon sense and understanding of everyday activities.\n4. TruthfulQA (Lin et al., 2021): Measuring the ability to generate\nresponses that are not only accurate but also truthful and non-misleading,\nfocusing on a model’s understanding of facts.\n5. WinoGrande (Sakaguchi et al., 2019): Measuring the ability to solve\nchallenging prono",
        "chapter": 4
      },
      {
        "start": 70805,
        "text": "rds might exclude an important but expensive benchmark. For\nexample, HELM (Holistic Evaluation of Language Models) Lite left out an\ninformation retrieval benchmark (MS MARCO, Microsoft Machine\nReading Comprehension) because it’s expensive to run. Hugging Face\nopted out of HumanEval due to its large compute requirements—you need\nto generate a lot of completions.\nWhen Hugging Face first launched Open LLM Leaderboard in 2023, it\nconsisted of four benchmarks. By the end of that year, they extended it to\nsix benchmarks. A small set of benchmarks is not nearly enough to\nrepresent the vast capabilities and different failure modes of foundation\nmodels.\nAdditionally, while leaderboard developers are generally thoughtful about\nhow they select benchmarks, their decision-making process isn’t always\nclear to users. Different leaderboards often end up with different\nbenchmarks, making it hard to compare and interpret their rankings. For\nexample, in late 2023, Hugging Face updated their Open LLM Lead",
        "chapter": 4
      },
      {
        "start": 75803,
        "text": "ecome saturated. However, discussing specific\n23\nbenchmarks, even if outdated, can still be useful as examples to evaluate and interpret benchmarks.\nTable 4-5 shows the Pearson correlation scores among the six benchmarks\nused on Hugging Face’s leaderboard, computed in January 2024 by Balázs\nGalambosi. The three benchmarks WinoGrande, MMLU, and ARC-C are\nstrongly correlated, which makes sense since they all test reasoning\ncapabilities. TruthfulQA is only moderately correlated to other benchmarks,\nsuggesting that improving a model’s reasoning and math capabilities\ndoesn’t always improve its truthfulness.\nTable 4-5. The correlation between the six benchmarks used on Hugging Face’s leaderboard, compute\nARC-C HellaSwag MMLU Truth\nARC-C 1.0000 0.4812 0.8672 0.480\nHellaSwag 0.4812 1.0000 0.6105 0.480\nMMLU 0.8672 0.6105 1.0000 0.550\nTruthfulQA 0.4809 0.4228 0.5507 1.000\nWinoGrande 0.8856 0.4842 0.9011 0.455\nGSM-8K 0.7438 0.3547 0.7936 0.500\nThe results from all the selected benchmarks need to ",
        "chapter": 4
      },
      {
        "start": 43316,
        "text": "> 90% > 95%\ngeneration\ncapability\nHard\nCriteria Metric Benchmark Ideal\nrequirement\nFactual Internal GPT Internal > 0.8 > 0.9\nconsistency metric hallucination\ndataset\nNow that you have your criteria, let’s move on to the next step and use them\nto select the best model for your application.\nModel Selection\nAt the end of the day, you don’t really care about which model is the best.\nYou care about which model is the best for your applications. Once you’ve\ndefined the criteria for your application, you should evaluate models against\nthese criteria.\nDuring the application development process, as you progress through\ndifferent adaptation techniques, you’ll have to do model selection over and\nover again. For example, prompt engineering might start with the strongest\nmodel overall to evaluate feasibility and then work backward to see if\nsmaller models would work. If you decide to do finetuning, you might start\nwith a small model to test your code and move toward the biggest model\nthat fits your",
        "chapter": 4
      },
      {
        "start": 47481,
        "text": "ate the dizzying\nnumber of public benchmarks and why you can’t trust them. This will set\nthe stage for the last section in the chapter. Because public benchmarks\ncan’t be trusted, you need to design your own evaluation pipeline with\nprompts and metrics you can trust.\nModel Build Versus Buy\nAn evergreen question for companies when leveraging any technology is\nwhether to build or buy. Since most companies won’t be building\nfoundation models from scratch, the question is whether to use commercial\nmodel APIs or host an open source model yourself. The answer to this\nquestion can significantly reduce your candidate model pool.\nLet’s first go into what exactly open source means when it comes to\nmodels, then discuss the pros and cons of these two approaches.\nOpen source, open weight, and model licenses\nThe term “open source model” has become contentious. Originally, open\nsource was used to refer to any model that people can download and use.\nFor many use cases, being able to download the model",
        "chapter": 4
      },
      {
        "start": 46648,
        "text": " in production to detect failure and\ncollect feedback to improve your application.\nFigure 4-5. An overview of the evaluation workflow to evaluate models for your application.\nThese four steps are iterative—you might want to change the decision from\na previous step with newer information from the current step. For example,\nyou might initially want to host open source models. However, after public\nand private evaluation, you might realize that open source models can’t\nachieve the level of performance you want and have to switch to\ncommercial APIs.\nChapter 10 discusses monitoring and collecting user feedback. The rest of\nthis chapter will discuss the first three steps. First, let’s discuss a question\nthat most teams will visit more than once: to use model APIs or to host\nmodels themselves. We’ll then continue to how to navigate the dizzying\nnumber of public benchmarks and why you can’t trust them. This will set\nthe stage for the last section in the chapter. Because public benchmarks\ncan’t",
        "chapter": 4
      }
    ],
    "input_tokens": 2666,
    "output_tokens": 266,
    "input_cost": 0.0003999,
    "output_cost": 0.0001596,
    "total_cost": 0.0005595,
    "reference_answer": "It mentions Eleuther's lm-evaluation-harness, noting ideal examples ranging from 300 to over 1,000, which are critical for assessing model effectiveness reliably."
  },
  {
    "question": "correlation scores of AI benchmarks",
    "answer": "The correlation scores among various AI benchmarks provide insights into how closely related their assessments are. According to the context provided, specific benchmarks such as WinoGrande, MMLU, and ARC-C exhibit strong correlations, particularly because they all focus on reasoning capabilities. In contrast, TruthfulQA shows only moderate correlation with the other benchmarks, indicating that improvements in reasoning and math capabilities do not directly enhance a model’s ability to provide truthful responses.\n\nThe correlation scores from Hugging Face’s leaderboard illustrate this point succinctly:\n\n- ARC-C: Strongly correlated with MMLU (0.8672) and WinoGrande (0.8856), moderate correlation with HellaSwag (0.4812) and TruthfulQA (0.4809).\n- MMLU: Strong correlation with ARC-C (0.8672) and WinoGrande (0.9011), moderate correlation with HellaSwag (0.6105) and TruthfulQA (0.5507).\n- WinoGrande: High correlation with both MMLU (0.9011) and ARC-C (0.8856).\n- TruthfulQA shows lower correlations with other benchmarks (for instance, correlations of 0.5507 with MMLU and 0.480 with ARC-C).\n\nUnderstanding these correlations is crucial when selecting benchmarks for model evaluation, as redundant correlations could lead to exaggerated biases in performance assessments.",
    "context": [
      {
        "start": 75803,
        "text": "ecome saturated. However, discussing specific\n23\nbenchmarks, even if outdated, can still be useful as examples to evaluate and interpret benchmarks.\nTable 4-5 shows the Pearson correlation scores among the six benchmarks\nused on Hugging Face’s leaderboard, computed in January 2024 by Balázs\nGalambosi. The three benchmarks WinoGrande, MMLU, and ARC-C are\nstrongly correlated, which makes sense since they all test reasoning\ncapabilities. TruthfulQA is only moderately correlated to other benchmarks,\nsuggesting that improving a model’s reasoning and math capabilities\ndoesn’t always improve its truthfulness.\nTable 4-5. The correlation between the six benchmarks used on Hugging Face’s leaderboard, compute\nARC-C HellaSwag MMLU Truth\nARC-C 1.0000 0.4812 0.8672 0.480\nHellaSwag 0.4812 1.0000 0.6105 0.480\nMMLU 0.8672 0.6105 1.0000 0.550\nTruthfulQA 0.4809 0.4228 0.5507 1.000\nWinoGrande 0.8856 0.4842 0.9011 0.455\nGSM-8K 0.7438 0.3547 0.7936 0.500\nThe results from all the selected benchmarks need to ",
        "chapter": 4
      },
      {
        "start": 94962,
        "text": " to determine the usefulness threshold: what scores must an\napplication achieve for it to be useful? For example, you might determine\nthat your chatbot’s factual consistency score must be at least 50% for it to\nbe useful. Anything below this makes it unusable even for general customer\nrequests.\nBefore developing AI evaluation metrics, it’s crucial to first understand the\nbusiness metrics you’re targeting. Many applications focus on stickiness\nmetrics, such as daily, weekly, or monthly active users (DAU, WAU,\nMAU). Others prioritize engagement metrics, like the number of\nconversations a user initiates per month or the duration of each visit—the\nlonger a user stays on the app, the less likely they are to leave. Choosing\nwhich metrics to prioritize can feel like balancing profits with social\nresponsibility. While an emphasis on stickiness and engagement metrics can\nlead to higher revenues, it may also cause a product to prioritize addictive\nfeatures or extreme content, which can be detrim",
        "chapter": 4
      },
      {
        "start": 104958,
        "text": " be asking about the quality of your\nevaluation pipeline:\nIs your evaluation pipeline getting you the right signals?\nDo better responses indeed get higher scores? Do better evaluation\nmetrics lead to better business outcomes?\nHow reliable is your evaluation pipeline?\nIf you run the same pipeline twice, do you get different results? If\nyou run the pipeline multiple times with different evaluation datasets,\nwhat would be the variance in the evaluation results? You should aim\nto increase reproducibility and reduce variance in your evaluation\npipeline. Be consistent with the configurations of your evaluation.\nFor example, if you use an AI judge, make sure to set your judge’s\ntemperature to 0.\nHow correlated are your metrics?\nAs discussed in “Benchmark selection and aggregation”, if two\nmetrics are perfectly correlated, you don’t need both of them. On the\nother hand, if two metrics are not at all correlated, this means either\nan interesting insight into your model or that your metrics just ",
        "chapter": 4
      },
      {
        "start": 74137,
        "text": "ops at six or ten benchmarks. For example, why\nare medical and legal tasks included in HELM Lite but not general science?\nWhy does HELM Lite have two math tests but no coding? Why does\nneither have tests for summarization, tool use, toxicity detection, image\nsearch, etc.? These questions aren’t meant to criticize these public\nleaderboards but to highlight the challenge of selecting benchmarks to rank\nmodels. If leaderboard developers can’t explain their benchmark selection\nprocesses, it might be because it’s really hard to do so.\nAn important aspect of benchmark selection that is often overlooked is\nbenchmark correlation. It is important because if two benchmarks are\nperfectly correlated, you don’t want both of them. Strongly correlated\n21\nbenchmarks can exaggerate biases.\nNOTE\nWhile I was writing this book, many benchmarks became saturated or close to being saturated. In\nJune 2024, less than a year after their leaderboard’s last revamp, Hugging Face updated their\nleaderboard again wit",
        "chapter": 4
      },
      {
        "start": 4165,
        "text": "ach makes sense from a\nbusiness perspective, focusing only on applications whose outcomes can be\nmeasured is similar to looking for the lost key under the lamppost (at night).\nIt’s easier to do, but it doesn’t mean we’ll find the key. We might be missing\nout on many potentially game-changing applications because there is no\neasy way to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can think of criteria in the\nfollowing buckets: domain-specific capability, generation capability,\ninstruction-following capability, and cost and latency.\nImagine you ask a model to summarize a legal contract. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstanding legal contracts. Generation capability metrics measure how\nco",
        "chapter": 4
      },
      {
        "start": 71638,
        "text": "rds often end up with different\nbenchmarks, making it hard to compare and interpret their rankings. For\nexample, in late 2023, Hugging Face updated their Open LLM Leaderboard\nto use the average of six different benchmarks to rank models:\n1. ARC-C (Clark et al., 2018): Measuring the ability to solve complex,\ngrade school-level science questions.\n2. MMLU (Hendrycks et al., 2020): Measuring knowledge and reasoning\ncapabilities in 57 subjects, including elementary mathematics, US\nhistory, computer science, and law.\n3. HellaSwag (Zellers et al., 2019): Measuring the ability to predict the\ncompletion of a sentence or a scene in a story or video. The goal is to test\ncommon sense and understanding of everyday activities.\n4. TruthfulQA (Lin et al., 2021): Measuring the ability to generate\nresponses that are not only accurate but also truthful and non-misleading,\nfocusing on a model’s understanding of facts.\n5. WinoGrande (Sakaguchi et al., 2019): Measuring the ability to solve\nchallenging prono",
        "chapter": 4
      },
      {
        "start": 69139,
        "text": "n harness. As of this writing, EleutherAI’s lm-evaluation-harness\nsupports over 400 benchmarks. OpenAI’s evals lets you run any of the\napproximately 500 existing benchmarks and register new benchmarks to\nevaluate OpenAI models. Their benchmarks evaluate a wide range of\ncapabilities, from doing math and solving puzzles to identifying ASCII art\nthat represents words.\nBenchmark selection and aggregation\nBenchmark results help you identify promising models for your use cases.\nAggregating benchmark results to rank models gives you a leaderboard.\nThere are two questions to consider:\nWhat benchmarks to include in your leaderboard?\nHow to aggregate these benchmark results to rank models?\nGiven so many benchmarks out there, it’s impossible to look at them all, let\nalone aggregate their results to decide which model is the best. Imagine that\nyou’re considering two models, A and B, for code generation. If model A\nperforms better than model B on a coding benchmark but worse on a\ntoxicity benchmark",
        "chapter": 4
      },
      {
        "start": 17493,
        "text": "r sheds light on how AI models process conflicting\ninformation and determine what the facts are. For example, Wan et al.\n(2024) found that existing “models rely heavily on the relevance of a\nwebsite to the query, while largely ignoring stylistic features that humans\nfind important such as whether a text contains scientific references or is\nwritten with a neutral tone.”\nTIP\nWhen designing metrics to measure hallucinations, it’s important to analyze the model’s outputs to\nunderstand the types of queries that it is more likely to hallucinate on. Your benchmark should focus\nmore on these queries.\nFor example, in one of my projects, I found that the model I was working with tended to hallucinate\non two types of queries:\n1. Queries that involve niche knowledge. For example, it was more likely to hallucinate when I\nasked it about the VMO (Vietnamese Mathematical Olympiad) than the IMO (International\nMathematical Olympiad), because the VMO is much less commonly referenced than the IMO.\n2. Quer",
        "chapter": 4
      },
      {
        "start": 24157,
        "text": "erformance of several models on this benchmark, as\nshown in GPT-4’s technical report (2023). For comparison, the human\nexpert baseline, as reported in the TruthfulQA paper, is 94%.\nFactual consistency is a crucial evaluation criteria for RAG, retrieval-\naugmented generation, systems. Given a query, a RAG system retrieves\nrelevant information from external databases to supplement the model’s\ncontext. The generated response should be factually consistent with the\nretrieved context. RAG is a central topic in Chapter 6.\nFigure 4-2. The performance of different models on TruthfulQA, as shown in GPT-4’s technical\nreport.\nSafety\nOther than factual consistency, there are many ways in which a model’s\noutputs can be harmful. Different safety solutions have different ways of\ncategorizing harms—see the taxonomy defined in OpenAI’s content\nmoderation endpoint and Meta’s Llama Guard paper (Inan et al., 2023).\nChapter 5 also discusses more ways in which AI models can be unsafe and\nhow to make your sy",
        "chapter": 4
      },
      {
        "start": 86632,
        "text": "ate hold-out data.\nPublic benchmarks will help you filter out bad models, but they won’t help\nyou find the best models for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each application might consist of\nmany ",
        "chapter": 4
      }
    ],
    "input_tokens": 2659,
    "output_tokens": 287,
    "input_cost": 0.00039885,
    "output_cost": 0.0001722,
    "total_cost": 0.00057105,
    "reference_answer": "The chapter highlights the Pearson correlation scores among six benchmarks, indicating strong correlations among WinoGrande, MMLU, and ARC-C due to their focus on reasoning capabilities."
  },
  {
    "question": "AI response length requirements",
    "answer": "In \"AI Engineering,\" Chip Huyen highlights the importance of setting clear response length requirements as part of an application's evaluation guidelines. These requirements can help ensure that the AI-generated responses meet user expectations in terms of content and structure. \n\nWhen designing an AI application, consider specifying constraints such as:\n\n1. **Maximum or Minimum Length**: Define the number of sentences or words required in a response.\n2. **Paragraph Structure**: Specify the desired number of paragraphs and how they should be formatted.\n3. **Content Specifics**: Use placeholders or markers to indicate where specific types of information should appear.\n\nEstablishing these requirements not only enhances the consistency of responses but also contributes to an overall positive user experience by ensuring clarity and relevance.",
    "context": [
      {
        "start": 91630,
        "text": "means. In retrospect of one year of deploying\ngenerative AI applications, LinkedIn shared that the first hurdle was in\ncreating an evaluation guideline. A correct response is not always a good\nresponse. For example, for their AI-powered Job Assessment application,\nthe response “You are a terrible fit” might be correct but not helpful, thus\nmaking it a bad response. A good response should explain the gap between\nthis job’s requirements and the candidate’s background, and what the\ncandidate can do to close this gap.\nBefore building your application, think about what makes a good response.\nLangChain’s State of AI 2023 found that, on average, their users used 2.3\ndifferent types of feedback (criteria) to evaluate an application. For\nexample, for a customer support application, a good response might be\ndefined using three criteria:\n1. Relevance: the response is relevant to the user’s query.\n2. Factual consistency: the response is factually consistent with the context.\n3. Safety: the respons",
        "chapter": 4
      },
      {
        "start": 4165,
        "text": "ach makes sense from a\nbusiness perspective, focusing only on applications whose outcomes can be\nmeasured is similar to looking for the lost key under the lamppost (at night).\nIt’s easier to do, but it doesn’t mean we’ll find the key. We might be missing\nout on many potentially game-changing applications because there is no\neasy way to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can think of criteria in the\nfollowing buckets: domain-specific capability, generation capability,\ninstruction-following capability, and cost and latency.\nImagine you ask a model to summarize a legal contract. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstanding legal contracts. Generation capability metrics measure how\nco",
        "chapter": 4
      },
      {
        "start": 92463,
        "text": "eria:\n1. Relevance: the response is relevant to the user’s query.\n2. Factual consistency: the response is factually consistent with the context.\n3. Safety: the response isn’t toxic.\nTo come up with these criteria, you might need to play around with test\nqueries, ideally real user queries. For each of these test queries, generate\nmultiple responses, either manually or using AI models, and determine if\nthey are good or bad.\nCreate scoring rubrics with examples\nFor each criterion, choose a scoring system: would it be binary (0 and 1),\nfrom 1 to 5, between 0 and 1, or something else? For example, to evaluate\nwhether an answer is consistent with a given context, some teams use a\nbinary scoring system: 0 for factual inconsistency and 1 for factual\nconsistency. Some teams use three values: -1 for contradiction, 1 for\nentailment, and 0 for neutral. Which scoring system to use depends on your\ndata and your needs.\nOn this scoring system, create a rubric with examples. What does a\nresponse with a",
        "chapter": 4
      },
      {
        "start": 109123,
        "text": "niques and criteria\ndiscussed in the last chapter and how to create an evaluation pipeline for\nyour application. No perfect evaluation method exists. It’s impossible to\ncapture the ability of a high-dimensional system using one- or few-\ndimensional scores. Evaluating modern AI systems has many limitations\nand biases. However, this doesn’t mean we shouldn’t do it. Combining\ndifferent methods and approaches can help mitigate many of these\nchallenges.\nEven though dedicated discussions on evaluation end here, evaluation will\ncome up again and again, not just throughout the book but also throughout\nyour application development process. Chapter 6 explores evaluating\nretrieval and agentic systems, while Chapters 7 and 9 focus on calculating a\nmodel’s memory usage, latency, and costs. Data quality verification is\naddressed in Chapter 8, and using user feedback to evaluate production\napplications is addressed in Chapter 10.\nWith that, let’s move onto the actual model adaptation process, startin",
        "chapter": 4
      },
      {
        "start": 24157,
        "text": "erformance of several models on this benchmark, as\nshown in GPT-4’s technical report (2023). For comparison, the human\nexpert baseline, as reported in the TruthfulQA paper, is 94%.\nFactual consistency is a crucial evaluation criteria for RAG, retrieval-\naugmented generation, systems. Given a query, a RAG system retrieves\nrelevant information from external databases to supplement the model’s\ncontext. The generated response should be factually consistent with the\nretrieved context. RAG is a central topic in Chapter 6.\nFigure 4-2. The performance of different models on TruthfulQA, as shown in GPT-4’s technical\nreport.\nSafety\nOther than factual consistency, there are many ways in which a model’s\noutputs can be harmful. Different safety solutions have different ways of\ncategorizing harms—see the taxonomy defined in OpenAI’s content\nmoderation endpoint and Meta’s Llama Guard paper (Inan et al., 2023).\nChapter 5 also discusses more ways in which AI models can be unsafe and\nhow to make your sy",
        "chapter": 4
      },
      {
        "start": 32487,
        "text": "traints most {N} sentences.\nLength Number There should be {N} paragraphs.\nconstraints paragraphs + first Paragraphs and only paragraphs\nword in i-th are separated from each other by\nparagraph two line breaks. The {i}-th\nparagraph must start with word\n{first_word}.\nDetectable Postscript At the end of your response,\ncontent please explicitly add a postscript\nstarting with {postscript marker}.\nDetectable Number The response must contain at least\ncontent placeholder {N} placeholders represented by\nsquare brackets, such as [address].\nDetectable Number bullets Your answer must contain exactly\nformat {N} bullet points. Use the\nInstruction\nInstruction Description\ngroup\nmarkdown bullet points such as: *\nThis is a point.\nDetectable Title Your answer must contain a title,\nformat wrapped in double angular\nbrackets, such as <<poem of\njoy>>.\nDetectable Choose from Answer with one of the following\nformat options: {options}.\nDetectable Minimum number Highlight at least {N} sections in\nformat highlight",
        "chapter": 4
      },
      {
        "start": 90797,
        "text": "ion Guideline\nCreating a clear evaluation guideline is the most important step of the\nevaluation pipeline. An ambiguous guideline leads to ambiguous scores that\ncan be misleading. If you don’t know what bad responses look like, you\nwon’t be able to catch them.\nWhen creating the evaluation guideline, it’s important to define not only\nwhat the application should do, but also what it shouldn’t do. For example,\nif you build a customer support chatbot, should this chatbot answer\nquestions unrelated to your product, such as about an upcoming election? If\nnot, you need to define what inputs are out of the scope of your application,\nhow to detect them, and how your application should respond to them.\nDefine evaluation criteria\nOften, the hardest part of evaluation isn’t determining whether an output is\ngood, but rather what good means. In retrospect of one year of deploying\ngenerative AI applications, LinkedIn shared that the first hurdle was in\ncreating an evaluation guideline. A correct resp",
        "chapter": 4
      },
      {
        "start": 29988,
        "text": "ther be because\nthe model doesn’t know how to write lục bát, or because it doesn’t\nunderstand what it’s supposed to do.\nWARNING\nHow well a model performs depends on the quality of its instructions, which makes it hard to\nevaluate AI models. When a model performs poorly, it can either be because the model is bad or the\ninstruction is bad.\nInstruction-following criteria\nDifferent benchmarks have different notions of what instruction-following\ncapability encapsulates. The two benchmarks discussed here, IFEval and\nINFOBench, measure models’ capability to follow a wide range of\ninstructions, which are to give you ideas on how to evaluate a model’s\nability to follow your instructions: what criteria to use, what instructions to\ninclude in the evaluation set, and what evaluation methods are appropriate.\nThe Google benchmark IFEval, Instruction-Following Evaluation, focuses\non whether the model can produce outputs following an expected format.\nZhou et al. (2023) identified 25 types of instructi",
        "chapter": 4
      },
      {
        "start": 94962,
        "text": " to determine the usefulness threshold: what scores must an\napplication achieve for it to be useful? For example, you might determine\nthat your chatbot’s factual consistency score must be at least 50% for it to\nbe useful. Anything below this makes it unusable even for general customer\nrequests.\nBefore developing AI evaluation metrics, it’s crucial to first understand the\nbusiness metrics you’re targeting. Many applications focus on stickiness\nmetrics, such as daily, weekly, or monthly active users (DAU, WAU,\nMAU). Others prioritize engagement metrics, like the number of\nconversations a user initiates per month or the duration of each visit—the\nlonger a user stays on the app, the less likely they are to leave. Choosing\nwhich metrics to prioritize can feel like balancing profits with social\nresponsibility. While an emphasis on stickiness and engagement metrics can\nlead to higher revenues, it may also cause a product to prioritize addictive\nfeatures or extreme content, which can be detrim",
        "chapter": 4
      },
      {
        "start": 10829,
        "text": "bility to\ndifferentiate good responses from bad responses (classification), which is\ndifferent from the ability to generate good responses. MCQs are best suited\nfor evaluating knowledge (“does the model know that Paris is the capital of\nFrance?”) and reasoning (“can the model infer from a table of business\nexpenses which department is spending the most?”). They aren’t ideal for\nevaluating generation capabilities such as summarization, translation, and\nessay writing. Let’s discuss how generation capabilities can be evaluated in\nthe next section.\nGeneration Capability\nAI was used to generate open-ended outputs long before generative AI\nbecame a thing. For decades, the brightest minds in NLP (natural language\nprocessing) have been working on how to evaluate the quality of open-\nended outputs. The subfield that studies open-ended text generation is\ncalled NLG (natural language generation). NLG tasks in the early 2010s\nincluded translation, summarization, and paraphrasing.\nMetrics used to e",
        "chapter": 4
      }
    ],
    "input_tokens": 2587,
    "output_tokens": 149,
    "input_cost": 0.00038805,
    "output_cost": 8.94e-05,
    "total_cost": 0.00047745,
    "reference_answer": "The chapter specifies how to set constraints on response length, such as defining minimum and maximum numbers of words, sentences, and paragraphs."
  },
  {
    "question": "benchmark selection criteria for AI leaderboards",
    "answer": "**Benchmark Selection Criteria for AI Leaderboards**\n\n1. **Relevance to Application**: Choose benchmarks that evaluate capabilities directly relevant to the specific application (e.g., coding benchmarks for a code generation task).\n\n2. **Diversity of Benchmarks**: Include a variety of benchmarks to capture different capabilities and potential failure modes of models. Relying on too few benchmarks can skew understanding of a model's performance.\n\n3. **Benchmark Correlation**: Consider the correlation between benchmarks. Highly correlated benchmarks may not provide unique insights and can exaggerate biases, thus limiting the effectiveness of selection.\n\n4. **Compute Constraints**: Be aware of the computational requirements for each benchmark. Some valuable benchmarks may be excluded due to high compute costs, which could limit the comprehensiveness of the evaluation.\n\n5. **Transparency in Selection Process**: Ensure the benchmark selection process is clear and justifiable. A lack of transparency can lead to confusion about the leadership rankings.\n\n6. **Saturation of Benchmarks**: Monitor whether certain benchmarks are becoming saturated, which may render their results less informative over time.\n\n7. **Aggregation Method**: Define a clear method for aggregating benchmark results to rank models. This should take into account the importance and relevance of each benchmark to the overall evaluation criteria.\n\nBy adhering to these criteria, leaders can create effective and meaningful AI leaderboards that better serve their applications and decision-making processes.",
    "context": [
      {
        "start": 69972,
        "text": "agine that\nyou’re considering two models, A and B, for code generation. If model A\nperforms better than model B on a coding benchmark but worse on a\ntoxicity benchmark, which model would you choose? Similarly, which\nmodel would you choose if one model performs better in one coding\nbenchmark but worse in another coding benchmark?\nFor inspiration on how to create your own leaderboard from public\nbenchmarks, it’s useful to look into how public leaderboards do so.\nPublic leaderboards\nMany public leaderboards rank models based on their aggregated\nperformance on a subset of benchmarks. These leaderboards are immensely\nhelpful but far from being comprehensive. First, due to the compute\nconstraint—evaluating a model on a benchmark requires compute—most\nleaderboards can incorporate only a small number of benchmarks. Some\nleaderboards might exclude an important but expensive benchmark. For\nexample, HELM (Holistic Evaluation of Language Models) Lite left out an\ninformation retrieval benchmark (MS",
        "chapter": 4
      },
      {
        "start": 77469,
        "text": " fraction of times a model obtains\na better score than another model, averaged across scenarios”.\nWhile public leaderboards are useful to get a sense of models’ broad\nperformance, it’s important to understand what capabilities a leaderboard is\ntrying to capture. A model that ranks high on a public leaderboard will\nlikely, but far from always, perform well for your application. If you want a\nmodel for code generation, a public leaderboard that doesn’t include a code\ngeneration benchmark might not help you as much.\nCustom leaderboards with public benchmarks\nWhen evaluating models for a specific application, you’re basically creating\na private leaderboard that ranks models based on your evaluation criteria.\nThe first step is to gather a list of benchmarks that evaluate the capabilities\nimportant to your application. If you want to build a coding agent, look at\ncode-related benchmarks. If you build a writing assistant, look into creative\nwriting benchmarks. As new benchmarks are constantly",
        "chapter": 4
      },
      {
        "start": 74137,
        "text": "ops at six or ten benchmarks. For example, why\nare medical and legal tasks included in HELM Lite but not general science?\nWhy does HELM Lite have two math tests but no coding? Why does\nneither have tests for summarization, tool use, toxicity detection, image\nsearch, etc.? These questions aren’t meant to criticize these public\nleaderboards but to highlight the challenge of selecting benchmarks to rank\nmodels. If leaderboard developers can’t explain their benchmark selection\nprocesses, it might be because it’s really hard to do so.\nAn important aspect of benchmark selection that is often overlooked is\nbenchmark correlation. It is important because if two benchmarks are\nperfectly correlated, you don’t want both of them. Strongly correlated\n21\nbenchmarks can exaggerate biases.\nNOTE\nWhile I was writing this book, many benchmarks became saturated or close to being saturated. In\nJune 2024, less than a year after their leaderboard’s last revamp, Hugging Face updated their\nleaderboard again wit",
        "chapter": 4
      },
      {
        "start": 69139,
        "text": "n harness. As of this writing, EleutherAI’s lm-evaluation-harness\nsupports over 400 benchmarks. OpenAI’s evals lets you run any of the\napproximately 500 existing benchmarks and register new benchmarks to\nevaluate OpenAI models. Their benchmarks evaluate a wide range of\ncapabilities, from doing math and solving puzzles to identifying ASCII art\nthat represents words.\nBenchmark selection and aggregation\nBenchmark results help you identify promising models for your use cases.\nAggregating benchmark results to rank models gives you a leaderboard.\nThere are two questions to consider:\nWhat benchmarks to include in your leaderboard?\nHow to aggregate these benchmark results to rank models?\nGiven so many benchmarks out there, it’s impossible to look at them all, let\nalone aggregate their results to decide which model is the best. Imagine that\nyou’re considering two models, A and B, for code generation. If model A\nperforms better than model B on a coding benchmark but worse on a\ntoxicity benchmark",
        "chapter": 4
      },
      {
        "start": 114954,
        "text": "o always use the most popular tools that\nhave been extensively tested by the community.\n0\nWhen I posted a question on Hugging Face’s Discord about why they chose certain benchmarks,\nLewis Tunstall responded that they were guided by the benchmarks that the then popular models\nused. Thanks to the Hugging Face team for being so wonderfully responsive and for their great\ncontributions to the community.\n1\nI’m really glad to report that while I was writing this book, leaderboards have become much more\ntransparent about their benchmark selection and aggregation process. When launching their new\nleaderboard, Hugging Face shared a great analysis of the benchmarks correlation (2024).\n2\nIt’s both really cool and intimidating to see that in just a couple of years, benchmarks had to change\nfrom grade-level questions to graduate-level questions.\n3\nIn gaming, there’s the concept of a neverending game where new levels can be procedurally\ngenerated as players master all the existing levels. It’d be rea",
        "chapter": 4
      },
      {
        "start": 108290,
        "text": "on, like all the build versus buy decisions, is\nunique to every team, depending not only on what the team needs but also\non what the team wants.\nThis chapter also explored the thousands of available public benchmarks.\nPublic benchmarks can help you weed out bad models, but they won’t help\nyou find the best models for your applications. Public benchmarks are also\nlikely contaminated, as their data is included in the training data of many\nmodels. There are public leaderboards that aggregate multiple benchmarks\nto rank models, but how benchmarks are selected and aggregated is not a\nclear process. The lessons learned from public leaderboards are helpful for\nmodel selection, as model selection is akin to creating a private leaderboard\nto rank models based on your needs.\nThis chapter ends with how to use all the evaluation techniques and criteria\ndiscussed in the last chapter and how to create an evaluation pipeline for\nyour application. No perfect evaluation method exists. It’s impossible t",
        "chapter": 4
      },
      {
        "start": 71638,
        "text": "rds often end up with different\nbenchmarks, making it hard to compare and interpret their rankings. For\nexample, in late 2023, Hugging Face updated their Open LLM Leaderboard\nto use the average of six different benchmarks to rank models:\n1. ARC-C (Clark et al., 2018): Measuring the ability to solve complex,\ngrade school-level science questions.\n2. MMLU (Hendrycks et al., 2020): Measuring knowledge and reasoning\ncapabilities in 57 subjects, including elementary mathematics, US\nhistory, computer science, and law.\n3. HellaSwag (Zellers et al., 2019): Measuring the ability to predict the\ncompletion of a sentence or a scene in a story or video. The goal is to test\ncommon sense and understanding of everyday activities.\n4. TruthfulQA (Lin et al., 2021): Measuring the ability to generate\nresponses that are not only accurate but also truthful and non-misleading,\nfocusing on a model’s understanding of facts.\n5. WinoGrande (Sakaguchi et al., 2019): Measuring the ability to solve\nchallenging prono",
        "chapter": 4
      },
      {
        "start": 70805,
        "text": "rds might exclude an important but expensive benchmark. For\nexample, HELM (Holistic Evaluation of Language Models) Lite left out an\ninformation retrieval benchmark (MS MARCO, Microsoft Machine\nReading Comprehension) because it’s expensive to run. Hugging Face\nopted out of HumanEval due to its large compute requirements—you need\nto generate a lot of completions.\nWhen Hugging Face first launched Open LLM Leaderboard in 2023, it\nconsisted of four benchmarks. By the end of that year, they extended it to\nsix benchmarks. A small set of benchmarks is not nearly enough to\nrepresent the vast capabilities and different failure modes of foundation\nmodels.\nAdditionally, while leaderboard developers are generally thoughtful about\nhow they select benchmarks, their decision-making process isn’t always\nclear to users. Different leaderboards often end up with different\nbenchmarks, making it hard to compare and interpret their rankings. For\nexample, in late 2023, Hugging Face updated their Open LLM Lead",
        "chapter": 4
      },
      {
        "start": 94962,
        "text": " to determine the usefulness threshold: what scores must an\napplication achieve for it to be useful? For example, you might determine\nthat your chatbot’s factual consistency score must be at least 50% for it to\nbe useful. Anything below this makes it unusable even for general customer\nrequests.\nBefore developing AI evaluation metrics, it’s crucial to first understand the\nbusiness metrics you’re targeting. Many applications focus on stickiness\nmetrics, such as daily, weekly, or monthly active users (DAU, WAU,\nMAU). Others prioritize engagement metrics, like the number of\nconversations a user initiates per month or the duration of each visit—the\nlonger a user stays on the app, the less likely they are to leave. Choosing\nwhich metrics to prioritize can feel like balancing profits with social\nresponsibility. While an emphasis on stickiness and engagement metrics can\nlead to higher revenues, it may also cause a product to prioritize addictive\nfeatures or extreme content, which can be detrim",
        "chapter": 4
      },
      {
        "start": 4165,
        "text": "ach makes sense from a\nbusiness perspective, focusing only on applications whose outcomes can be\nmeasured is similar to looking for the lost key under the lamppost (at night).\nIt’s easier to do, but it doesn’t mean we’ll find the key. We might be missing\nout on many potentially game-changing applications because there is no\neasy way to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can think of criteria in the\nfollowing buckets: domain-specific capability, generation capability,\ninstruction-following capability, and cost and latency.\nImagine you ask a model to summarize a legal contract. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstanding legal contracts. Generation capability metrics measure how\nco",
        "chapter": 4
      }
    ],
    "input_tokens": 2489,
    "output_tokens": 283,
    "input_cost": 0.00037335,
    "output_cost": 0.0001698,
    "total_cost": 0.00054315,
    "reference_answer": "The chapter mentions that while leaderboard developers thoughtfully select benchmarks, their decision-making processes are not always transparent to users, leading to variability across different leaderboards."
  },
  {
    "question": "how to create evaluation guidelines for AI systems",
    "answer": "Creating evaluation guidelines for AI systems involves several key steps to ensure clarity and effectiveness. Here’s a structured approach based on the context provided:\n\n1. **Define Clear Objectives**: Start by outlining what the AI application should achieve as well as what it should avoid. For example, if building a customer support chatbot, specify whether it should respond to unrelated inquiries.\n\n2. **Create Evaluation Criteria**: Establish what “good” output looks like. This may involve identifying metrics such as factual consistency, domain-specific capabilities, or generation quality. The creation of guidelines is often the most challenging part because it necessitates a common understanding of criteria.\n\n3. **Annotate Evaluation Data**: Curate a dataset with annotated examples that align with the defined evaluation criteria. Use actual production data where possible, and ensure examples cover a range of scenarios, including common mistakes and edge cases.\n\n4. **Develop a Scoring Rubric**: Implement a scoring system that can quantify outputs based on the established criteria (e.g., a scale from 0 to 2). Each score should have clear definitions and examples illustrating what qualifies for each score.\n\n5. **Validate the Rubric**: Test the rubric by applying it in practice. Gather feedback from humans (co-workers, friends) to ensure that it is easy to follow. Refine it based on this feedback for clarity and precision.\n\n6. **Link Evaluation Metrics to Business Goals**: Ensure that the evaluation metrics are tied to the overall business objectives. This connection will help assess the impact of the model’s performance in real-world scenarios.\n\n7. **Iterate and Monitor**: Evaluation is an ongoing process. Regularly update the guidelines and criteria as the application evolves. Keep track of all variables and context changes in the evaluation process for consistent assessment over time.\n\nBy following these steps, you can create robust evaluation guidelines that facilitate the successful deployment and improvement of AI systems.",
    "context": [
      {
        "start": 109123,
        "text": "niques and criteria\ndiscussed in the last chapter and how to create an evaluation pipeline for\nyour application. No perfect evaluation method exists. It’s impossible to\ncapture the ability of a high-dimensional system using one- or few-\ndimensional scores. Evaluating modern AI systems has many limitations\nand biases. However, this doesn’t mean we shouldn’t do it. Combining\ndifferent methods and approaches can help mitigate many of these\nchallenges.\nEven though dedicated discussions on evaluation end here, evaluation will\ncome up again and again, not just throughout the book but also throughout\nyour application development process. Chapter 6 explores evaluating\nretrieval and agentic systems, while Chapters 7 and 9 focus on calculating a\nmodel’s memory usage, latency, and costs. Data quality verification is\naddressed in Chapter 8, and using user feedback to evaluate production\napplications is addressed in Chapter 10.\nWith that, let’s move onto the actual model adaptation process, startin",
        "chapter": 4
      },
      {
        "start": 0,
        "text": "Chapter 4. Evaluate AI Systems\nA model is only useful if it works for its intended purposes. You need to\nevaluate models in the context of your application. Chapter 3 discusses\ndifferent approaches to automatic evaluation. This chapter discusses how to\nuse these approaches to evaluate models for your applications.\nThis chapter contains three parts. It starts with a discussion of the criteria\nyou might use to evaluate your applications and how these criteria are\ndefined and calculated. For example, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-specific\ncapabilities like math, science, reasoning, and summarization measured?\nThe second part focuses on model selection. Given an increasing number of\nfoundation models to choose from, it can feel overwhelming to choose the\nright model for your application. Thousands of benchmarks have been\nintroduced to evaluate these models along different criteria. Can these\nbenchmarks be trusted? How do you ",
        "chapter": 4
      },
      {
        "start": 86632,
        "text": "ate hold-out data.\nPublic benchmarks will help you filter out bad models, but they won’t help\nyou find the best models for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each application might consist of\nmany ",
        "chapter": 4
      },
      {
        "start": 90797,
        "text": "ion Guideline\nCreating a clear evaluation guideline is the most important step of the\nevaluation pipeline. An ambiguous guideline leads to ambiguous scores that\ncan be misleading. If you don’t know what bad responses look like, you\nwon’t be able to catch them.\nWhen creating the evaluation guideline, it’s important to define not only\nwhat the application should do, but also what it shouldn’t do. For example,\nif you build a customer support chatbot, should this chatbot answer\nquestions unrelated to your product, such as about an upcoming election? If\nnot, you need to define what inputs are out of the scope of your application,\nhow to detect them, and how your application should respond to them.\nDefine evaluation criteria\nOften, the hardest part of evaluation isn’t determining whether an output is\ngood, but rather what good means. In retrospect of one year of deploying\ngenerative AI applications, LinkedIn shared that the first hurdle was in\ncreating an evaluation guideline. A correct resp",
        "chapter": 4
      },
      {
        "start": 98294,
        "text": "ir AI\nsystems.\nConsider evaluation methods to be used not just during experimentation but\nalso during production. During experimentation, you might have reference\ndata to compare your application’s outputs to, whereas, in production,\nreference data might not be immediately available. However, in production,\nyou have actual users. Think about what kinds of feedback you want from\nusers, how user feedback correlates to other evaluation metrics, and how to\nuse user feedback to improve your application. How to collect user\nfeedback is discussed in Chapter 10.\nAnnotate evaluation data\nCurate a set of annotated examples to evaluate your application. You need\nannotated data to evaluate each of your system’s components and each\ncriterion, for both turn-based and task-based evaluation. Use actual\nproduction data if possible. If your application has natural labels that you\ncan use, that’s great. If not, you can use either humans or AI to label your\ndata. Chapter 8 discusses AI-generated data. The",
        "chapter": 4
      },
      {
        "start": 106624,
        "text": "evaluation process changes constantly, you won’t be able to use the\nevaluation results to guide your application’s development.\nAs you iterate on your evaluation pipeline, make sure to do proper\nexperiment tracking: log all variables that could change in an evaluation\nprocess, including but not limited to the evaluation data, the rubric, and the\nprompt and sampling configurations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enable you to reduce risks, discover opportunities\nto improve performance, and benchmark progresses, which will all save\nyou time and headaches down the line.\nGiven an increasing number of readily available foundation models, for\nmost application developers, the challenge is no longer in developing\nmodels but in selecting the r",
        "chapter": 4
      },
      {
        "start": 4165,
        "text": "ach makes sense from a\nbusiness perspective, focusing only on applications whose outcomes can be\nmeasured is similar to looking for the lost key under the lamppost (at night).\nIt’s easier to do, but it doesn’t mean we’ll find the key. We might be missing\nout on many potentially game-changing applications because there is no\neasy way to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can think of criteria in the\nfollowing buckets: domain-specific capability, generation capability,\ninstruction-following capability, and cost and latency.\nImagine you ask a model to summarize a legal contract. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstanding legal contracts. Generation capability metrics measure how\nco",
        "chapter": 4
      },
      {
        "start": 37485,
        "text": "lude RoleLLM (Wang et al., 2023) and\nCharacterEval (Tu et al., 2024). CharacterEval used human annotators and\ntrained a reward model to evaluate each roleplaying aspect on a five-point\nscale. RoleLLM evaluates a model’s ability to emulate a persona using both\ncarefully crafted similarity scores (how similar the generated outputs are to\nthe expected outputs) and AI judges.\nIf AI in your application is supposed to assume a certain role, make sure to\nevaluate whether your model stays in character. Depending on the role, you\nmight be able to create heuristics to evaluate the model’s outputs. For\nexample, if the role is someone who doesn’t talk a lot, a heuristic would be\nthe average of the model’s outputs. Other than that, the easiest automatic\nevaluation approach is AI as a judge. You should evaluate the roleplaying\nAI on both style and knowledge. For example, if a model is supposed to\ntalk like Jackie Chan, its outputs should capture Jackie Chan’s style and are\n8\ngenerated based on Jacki",
        "chapter": 4
      },
      {
        "start": 100793,
        "text": " example in Designing Machine Learning Systems. Numbers from Charig\net al., “Comparison of Treatment of Renal Calculi by Open Surgery, Percutaneous\nNephrolithotomy, and Extracorporeal Shockwave Lithotripsy”, British Medical Journal\n(Clinical Research Edition) 292, no. 6524 (March 1986): 879–82.\nYou should have multiple evaluation sets to represent different data slices.\nYou should have one set that represents the distribution of the actual\nproduction data to estimate how the system does overall. You can slice your\ndata based on tiers (paying users versus free users), traffic sources (mobile\nversus web), usage, and more. You can have a set consisting of the\nexamples for which the system is known to frequently make mistakes. You\ncan have a set of examples where users frequently make mistakes—if typos\nare common in production, you should have evaluation examples that\ncontain typos. You might want an out-of-scope evaluation set, inputs your\napplication isn’t supposed to engage with, to mak",
        "chapter": 4
      },
      {
        "start": 93296,
        "text": " and 0 for neutral. Which scoring system to use depends on your\ndata and your needs.\nOn this scoring system, create a rubric with examples. What does a\nresponse with a score of 1 look like and why does it deserve a 1? Validate\nyour rubric with humans: yourself, coworkers, friends, etc. If humans find it\nhard to follow the rubric, you need to refine it to make it unambiguous. This\nprocess can require a lot of back and forth, but it’s necessary. A clear\nguideline is the backbone of a reliable evaluation pipeline. This guideline\ncan also be reused later for training data annotation, as discussed in\nChapter 8.\nTie evaluation metrics to business metrics\nWithin a business, an application must serve a business goal. The\napplication’s metrics must be considered in the context of the business\nproblem it’s built to solve.\nFor example, if your customer support chatbot’s factual consistency is 80%,\nwhat does it mean for the business? For example, this level of factual\nconsistency might make the ch",
        "chapter": 4
      }
    ],
    "input_tokens": 2495,
    "output_tokens": 383,
    "input_cost": 0.00037425,
    "output_cost": 0.0002298,
    "total_cost": 0.00060405,
    "reference_answer": "The text emphasizes that establishing clear evaluation guidelines is crucial to avoid ambiguous scoring and ensures that misleading responses can be identified effectively."
  },
  {
    "question": "evaluating efficiency in AI model performance",
    "answer": "Evaluating the efficiency of AI model performance is a multifaceted process that often requires a combination of various metrics and evaluation methods. Here are the key points related to this topic based on the context provided:\n\n1. **Evaluation Criteria**: When assessing efficiency, specific criteria must be established that relate to the application, such as domain-specific capabilities, generation capabilities, and cost and latency. Metrics need to be defined clearly to measure these aspects effectively.\n\n2. **Complex Applications**: Real-world AI applications are generally complex and may involve multiple components that need to be evaluated individually and as a whole to assess overall efficiency. \n\n3. **Performance Measurement**: Efficiency can be directly measured by evaluating specific aspects such as runtime or memory usage. For example, benchmarks like BIRD-SQL assess not only the correctness of a generated SQL query but also its execution speed compared to a ground truth query.\n\n4. **Iterative Improvement**: Evaluation pipelines should be designed iteratively, allowing for continuous refinement of metrics to ensure they effectively signal performance efficiency. \n\n5. **Experiment Tracking**: It’s crucial to keep track of variables during the evaluation process. This includes the data used for evaluation, the metrics chosen, and any configurations involved in the evaluation, which helps in understanding and improving the evaluation outcomes.\n\n6. **Combining Different Methods**: Since no single method captures all dimensions of model efficiency, combining various evaluation approaches is often necessary to mitigate biases and limitations.\n\nIn summary, effectively evaluating AI model performance involves establishing clear criteria, measuring specific metrics related to efficiency, continually refining the evaluation process, and integrating diverse evaluation methods to obtain a comprehensive view of performance.",
    "context": [
      {
        "start": 109123,
        "text": "niques and criteria\ndiscussed in the last chapter and how to create an evaluation pipeline for\nyour application. No perfect evaluation method exists. It’s impossible to\ncapture the ability of a high-dimensional system using one- or few-\ndimensional scores. Evaluating modern AI systems has many limitations\nand biases. However, this doesn’t mean we shouldn’t do it. Combining\ndifferent methods and approaches can help mitigate many of these\nchallenges.\nEven though dedicated discussions on evaluation end here, evaluation will\ncome up again and again, not just throughout the book but also throughout\nyour application development process. Chapter 6 explores evaluating\nretrieval and agentic systems, while Chapters 7 and 9 focus on calculating a\nmodel’s memory usage, latency, and costs. Data quality verification is\naddressed in Chapter 8, and using user feedback to evaluate production\napplications is addressed in Chapter 10.\nWith that, let’s move onto the actual model adaptation process, startin",
        "chapter": 4
      },
      {
        "start": 0,
        "text": "Chapter 4. Evaluate AI Systems\nA model is only useful if it works for its intended purposes. You need to\nevaluate models in the context of your application. Chapter 3 discusses\ndifferent approaches to automatic evaluation. This chapter discusses how to\nuse these approaches to evaluate models for your applications.\nThis chapter contains three parts. It starts with a discussion of the criteria\nyou might use to evaluate your applications and how these criteria are\ndefined and calculated. For example, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-specific\ncapabilities like math, science, reasoning, and summarization measured?\nThe second part focuses on model selection. Given an increasing number of\nfoundation models to choose from, it can feel overwhelming to choose the\nright model for your application. Thousands of benchmarks have been\nintroduced to evaluate these models along different criteria. Can these\nbenchmarks be trusted? How do you ",
        "chapter": 4
      },
      {
        "start": 4165,
        "text": "ach makes sense from a\nbusiness perspective, focusing only on applications whose outcomes can be\nmeasured is similar to looking for the lost key under the lamppost (at night).\nIt’s easier to do, but it doesn’t mean we’ll find the key. We might be missing\nout on many potentially game-changing applications because there is no\neasy way to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can think of criteria in the\nfollowing buckets: domain-specific capability, generation capability,\ninstruction-following capability, and cost and latency.\nImagine you ask a model to summarize a legal contract. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstanding legal contracts. Generation capability metrics measure how\nco",
        "chapter": 4
      },
      {
        "start": 86632,
        "text": "ate hold-out data.\nPublic benchmarks will help you filter out bad models, but they won’t help\nyou find the best models for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each application might consist of\nmany ",
        "chapter": 4
      },
      {
        "start": 17493,
        "text": "r sheds light on how AI models process conflicting\ninformation and determine what the facts are. For example, Wan et al.\n(2024) found that existing “models rely heavily on the relevance of a\nwebsite to the query, while largely ignoring stylistic features that humans\nfind important such as whether a text contains scientific references or is\nwritten with a neutral tone.”\nTIP\nWhen designing metrics to measure hallucinations, it’s important to analyze the model’s outputs to\nunderstand the types of queries that it is more likely to hallucinate on. Your benchmark should focus\nmore on these queries.\nFor example, in one of my projects, I found that the model I was working with tended to hallucinate\non two types of queries:\n1. Queries that involve niche knowledge. For example, it was more likely to hallucinate when I\nasked it about the VMO (Vietnamese Mathematical Olympiad) than the IMO (International\nMathematical Olympiad), because the VMO is much less commonly referenced than the IMO.\n2. Quer",
        "chapter": 4
      },
      {
        "start": 13328,
        "text": "an be evaluated using AI\nas a judge—asking an AI model how fluent and coherent a text is—or using\nperplexity, as discussed in Chapter 3.\nGenerative models, with their new capabilities and new use cases, have new\nissues that require new metrics to track. The most pressing issue is\nundesired hallucinations. Hallucinations are desirable for creative tasks, not\nfor tasks that depend on factuality. A metric that many application\ndevelopers want to measure is factual consistency. Another issue commonly\ntracked is safety: can the generated outputs cause harm to users and\nsociety? Safety is an umbrella term for all types of toxicity and biases.\nThere are many other measurements that an application developer might\ncare about. For example, when I built my AI-powered writing assistant, I\ncared about controversiality, which measures content that isn’t necessarily\nharmful but can cause heated debates. Some people might care about\nfriendliness, positivity, creativity, or conciseness, but I won’t be ",
        "chapter": 4
      },
      {
        "start": 106624,
        "text": "evaluation process changes constantly, you won’t be able to use the\nevaluation results to guide your application’s development.\nAs you iterate on your evaluation pipeline, make sure to do proper\nexperiment tracking: log all variables that could change in an evaluation\nprocess, including but not limited to the evaluation data, the rubric, and the\nprompt and sampling configurations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enable you to reduce risks, discover opportunities\nto improve performance, and benchmark progresses, which will all save\nyou time and headaches down the line.\nGiven an increasing number of readily available foundation models, for\nmost application developers, the challenge is no longer in developing\nmodels but in selecting the r",
        "chapter": 4
      },
      {
        "start": 24157,
        "text": "erformance of several models on this benchmark, as\nshown in GPT-4’s technical report (2023). For comparison, the human\nexpert baseline, as reported in the TruthfulQA paper, is 94%.\nFactual consistency is a crucial evaluation criteria for RAG, retrieval-\naugmented generation, systems. Given a query, a RAG system retrieves\nrelevant information from external databases to supplement the model’s\ncontext. The generated response should be factually consistent with the\nretrieved context. RAG is a central topic in Chapter 6.\nFigure 4-2. The performance of different models on TruthfulQA, as shown in GPT-4’s technical\nreport.\nSafety\nOther than factual consistency, there are many ways in which a model’s\noutputs can be harmful. Different safety solutions have different ways of\ncategorizing harms—see the taxonomy defined in OpenAI’s content\nmoderation endpoint and Meta’s Llama Guard paper (Inan et al., 2023).\nChapter 5 also discusses more ways in which AI models can be unsafe and\nhow to make your sy",
        "chapter": 4
      },
      {
        "start": 104125,
        "text": "% confident that one system is better.\nValues from OpenAI.\nDifference Sample size needed for\nto detect 95% confidence\n30% ~10\n10% ~100\n3% ~1,000\n1% ~10,000\nAs a reference, among evaluation benchmarks in Eleuther’s lm-evaluation-\nharness, the median number of examples is 1,000, and the average is 2,159.\nThe organizers of the Inverse Scaling prize suggested that 300 examples is\nthe absolute minimum and they would prefer at least 1,000, especially if the\nexamples are being synthesized (McKenzie et al., 2023).\nEvaluate your evaluation pipeline\nEvaluating your evaluation pipeline can help with both improving your\npipeline’s reliability and finding ways to make your evaluation pipeline\nmore efficient. Reliability is especially important with subjective evaluation\nmethods such as AI as a judge.\nHere are some questions you should be asking about the quality of your\nevaluation pipeline:\nIs your evaluation pipeline getting you the right signals?\nDo better responses indeed get higher scores? Do b",
        "chapter": 4
      },
      {
        "start": 6664,
        "text": " capabilities are commonly evaluated using exact\nevaluation. Coding-related capabilities are typically evaluated using\nfunctional correctness, as discussed in Chapter 3. While functional\ncorrectness is important, it might not be the only aspect that you care about.\nYou might also care about efficiency and cost. For example, would you\nwant a car that runs but consumes an excessive amount of fuel? Similarly, if\nan SQL query generated by your text-to-SQL model is correct but takes too\nlong or requires too much memory to run, it might not be usable.\nEfficiency can be exactly evaluated by measuring runtime or memory\nusage. BIRD-SQL (Li et al., 2023) is an example of a benchmark that takes\ninto account not only the generated query’s execution accuracy but also its\nefficiency, which is measured by comparing the runtime of the generated\nquery with the runtime of the ground truth SQL query.\nYou might also care about code readability. If the generated code runs but\nnobody can understand it, it w",
        "chapter": 4
      }
    ],
    "input_tokens": 2488,
    "output_tokens": 331,
    "input_cost": 0.0003732,
    "output_cost": 0.0001986,
    "total_cost": 0.0005718,
    "reference_answer": "Efficiency in AI models can be evaluated through metrics like runtime and memory usage, in addition to functional correctness, to ensure a model is not just accurate but also practical for real-world applications."
  },
  {
    "question": "what are open weights in AI models?",
    "answer": "In the context of AI models, \"open weights\" refers to models that have publicly available weights but do not come with open training data. This means users can access and utilize the model's parameters for their own applications, but they lack transparency regarding what data the model was trained on. This term is contrasted with \"open models,\" which are models that provide both open weights and access to the training data. \n\nThe significance of open weights lies in the ability for users to analyze and operate the model, but limited visibility into the training data can affect reproducibility, understanding, and potential auditing of the model's behavior.",
    "context": [
      {
        "start": 49147,
        "text": " is also open, the term “open weight” is used for\nmodels that don’t come with open data, whereas the term “open model” is\nused for models that come with open data.\nNOTE\nSome people argue that the term open source should be reserved only for fully open models. In this\nbook, for simplicity, I use open source to refer to all models whose weights are made public,\nregardless of their training data’s availability and licenses.\nAs of this writing, the vast majority of open source models are open weight\nonly. Model developers might hide training data information on purpose, as\nthis information can open model developers to public scrutiny and potential\nlawsuits.\nAnother important attribute of open source models is their licenses. Before\nfoundation models, the open source world was confusing enough, with so\nmany different licenses, such as MIT (Massachusetts Institute of\nTechnology), Apache 2.0, GNU General Public License (GPL), BSD\n(Berkely Software Distribution), Creative Commons, etc. Open so",
        "chapter": 4
      },
      {
        "start": 48314,
        "text": "” has become contentious. Originally, open\nsource was used to refer to any model that people can download and use.\nFor many use cases, being able to download the model is sufficient.\nHowever, some people argue that since a model’s performance is largely a\nfunction of what data it was trained on, a model should be considered open\nonly if its training data is also made publicly available.\nOpen data allows more flexible model usage, such as retraining the model\nfrom scratch with modifications in the model architecture, training process,\nor the training data itself. Open data also makes it easier to understand the\nmodel. Some use cases also required access to the training data for auditing\npurposes, for example, to make sure that the model wasn’t trained on\n10\ncompromised or illegally acquired data.\nTo signal whether the data is also open, the term “open weight” is used for\nmodels that don’t come with open data, whereas the term “open model” is\nused for models that come with open data.\nNOT",
        "chapter": 4
      },
      {
        "start": 51646,
        "text": "icenses still don’t allow it.\nSome people use the term restricted weight to refer to open source models\nwith restricted licenses. However, I find this term ambiguous, since all\nsensible licenses have restrictions (e.g., you shouldn’t be able to use the\nmodel to commit genocide).\nOpen source models versus model APIs\nFor a model to be accessible to users, a machine needs to host and run it.\nThe service that hosts the model and receives user queries, runs the model\nto generate responses for queries, and returns these responses to the users is\ncalled an inference service. The interface users interact with is called the\nmodel API, as shown in Figure 4-6. The term model API is typically used to\nrefer to the API of the inference service, but there are also APIs for other\nmodel services, such as finetuning APIs and evaluation APIs. Chapter 9\ndiscusses how to optimize inference services.\nFigure 4-6. An inference service runs the model and provides an interface for users to access the\nmodel.\nAft",
        "chapter": 4
      },
      {
        "start": 113288,
        "text": "The story was reported by several outlets, including TechRadar (see “Samsung Workers Made a\nMajor Error by Using ChatGPT”, by Lewis Maddison (April 2023).\n6\nAs regulations are evolving around the world, requirements for auditable information of models and\ntraining data may increase. Commercial models may be able to provide certifications, saving\ncompanies from the effort.\n7\nUsers want models to be open source because open means more information and more options, but\nwhat’s in it for model developers? Many companies have sprung up to capitalize on open source\nmodels by providing inference and finetuning services. It’s not a bad thing. Many people need these\nservices to leverage open source models. But, from model developers’ perspective, why invest\nmillions, if not billions, into building models just for others to make money?It might be argued that\nMeta supports open source models only to keep their competitors (Google, Microsoft/OpenAI) in\ncheck. Both Mistral and Cohere have open sourc",
        "chapter": 4
      },
      {
        "start": 52479,
        "text": "s. Chapter 9\ndiscusses how to optimize inference services.\nFigure 4-6. An inference service runs the model and provides an interface for users to access the\nmodel.\nAfter developing a model, a developer can choose to open source it, make it\naccessible via an API, or both. Many model developers are also model\nservice providers. Cohere and Mistral open source some models and\nprovide APIs for some. OpenAI is typically known for their commercial\nmodels, but they’ve also open sourced models (GPT-2, CLIP). Typically,\nmodel providers open source weaker models and keep their best models\nbehind paywalls, either via APIs or to power their products.\nModel APIs can be available through model providers (such as OpenAI and\nAnthropic), cloud service providers (such as Azure and GCP [Google Cloud\nPlatform]), or third-party API providers (such as Databricks Mosaic,\nAnyscale, etc.). The same model can be available through different APIs\nwith different features, constraints, and pricings. For example, GPT",
        "chapter": 4
      },
      {
        "start": 63308,
        "text": "el to follow a standard API, which makes it easier to swap models.\nMany model developers try to make their models mimic the API of the most\npopular models. As of this writing, many API providers mimic OpenAI’s\nAPI.\nYou might also prefer models with good community support. The more\ncapabilities a model has, the more quirks it has. A model with a large\ncommunity of users means that any issue you encounter may already have\n19\nbeen experienced by others, who might have shared solutions online.\nControl, access, and transparency\nA 2024 study by a16z shows two key reasons that enterprises care about\nopen source models are control and customizability, as shown in Figure 4-8.\nFigure 4-8. Why enterprises care about open source models. Image from the 2024 study by a16z.\nIf your business depends on a model, it’s understandable that you would\nwant some control over it, and API providers might not always give you the\nlevel of control you want. When using a service provided by someone else,\nyou’re su",
        "chapter": 4
      },
      {
        "start": 47481,
        "text": "ate the dizzying\nnumber of public benchmarks and why you can’t trust them. This will set\nthe stage for the last section in the chapter. Because public benchmarks\ncan’t be trusted, you need to design your own evaluation pipeline with\nprompts and metrics you can trust.\nModel Build Versus Buy\nAn evergreen question for companies when leveraging any technology is\nwhether to build or buy. Since most companies won’t be building\nfoundation models from scratch, the question is whether to use commercial\nmodel APIs or host an open source model yourself. The answer to this\nquestion can significantly reduce your candidate model pool.\nLet’s first go into what exactly open source means when it comes to\nmodels, then discuss the pros and cons of these two approaches.\nOpen source, open weight, and model licenses\nThe term “open source model” has become contentious. Originally, open\nsource was used to refer to any model that people can download and use.\nFor many use cases, being able to download the model",
        "chapter": 4
      },
      {
        "start": 66640,
        "text": "ld be for privacy reasons, such as when you want to give an AI assistant\naccess to all your data, but don’t want your data to leave your device.\nTable 4-4 summarizes the pros and cons of using model APIs and self-\nhosting models.\nTable 4-4. Pros and cons of using model APIs and self-hosting models (cons in italics).\nUsing model APIs Self-hosting models\nData\nHave to send your Don’t have to send your\ndata to model data externally\nproviders, which Fewer checks and\nmeans your team can balances for data\naccidentally leak lineage/training data\nconfidential info copyright\nPerformance\nBest-performing The best open source\nmodel will likely be models will likely be a bit\nclosed source behind commercial\nmodels\nFunctionality\nMore likely to No/limited support for\nsupport scaling, function calling and\nfunction calling, structured outputs\nstructured outputs Can access logprobs and\nLess likely to expose intermediate outputs,\nlogprobs which are helpful for\nclassification tasks,\nUsing model APIs Self-ho",
        "chapter": 4
      },
      {
        "start": 59976,
        "text": " might be sufficient.\nAnother reason that might cause open source models to lag behind is that\nopen source developers don’t receive feedback from users to improve their\nmodels, the way commercial models do. Once a model is open sourced,\nmodel developers have no idea how the model is being used, and how well\nthe model works in the wild.\nFunctionality\nMany functionalities are needed around a model to make it work for a use\ncase. Here are some examples of these functionalities:\nScalability: making sure the inference service can support your\napplication’s traffic while maintaining the desirable latency and cost.\nFunction calling: giving the model the ability to use external tools, which\nis essential for RAG and agentic use cases, as discussed in Chapter 6.\nStructured outputs, such as asking models to generate outputs in JSON\nformat.\nOutput guardrails: mitigating risks in the generated responses, such as\nmaking sure the responses aren’t racist or sexist.\nMany of these functionalities are ch",
        "chapter": 4
      },
      {
        "start": 79135,
        "text": "3.5 and GPT-4 from March 2023 to\nJune 2023 on certain benchmarks (Chen et al., 2023).\nAssuming that OpenAI doesn’t intentionally release worse models, what\nmight be the reason for this perception? One potential reason is that\nevaluation is hard, and no one, not even OpenAI, knows for sure if a model\nis getting better or worse. While evaluation is definitely hard, I doubt that\n24\nOpenAI would fly completely blind. If the second reason is true, it\nreinforces the idea that the best model overall might not be the best model\nfor your application.\nNot all models have publicly available scores on all benchmarks. If the\nmodel you care about doesn’t have a publicly available score on your\n25\nbenchmark, you will need to run the evaluation yourself. Hopefully, an\nevaluation harness can help you with that. Running benchmarks can be\nexpensive. For example, Stanford spent approximately $80,000–$100,000\n26\nto evaluate 30 models on their full HELM suite. The more models you\nwant to evaluate and the mo",
        "chapter": 4
      }
    ],
    "input_tokens": 2587,
    "output_tokens": 124,
    "input_cost": 0.00038805,
    "output_cost": 7.44e-05,
    "total_cost": 0.00046245,
    "reference_answer": "Open weights refer to model weights that are publicly available, without necessarily providing access to the training data, which can be vital for auditing and ethical considerations."
  },
  {
    "question": "how to evaluate generated text quality",
    "answer": "To evaluate the quality of generated text, several metrics can be employed, particularly within the context of Natural Language Generation (NLG). Key metrics include:\n\n1. **Fluency**: This assesses whether the generated text is grammatically correct and sounds natural, as if produced by a fluent speaker.\n\n2. **Coherence**: This measures the logical structure of the text, evaluating how well ideas flow and connect within the text.\n\n3. **Task-Specific Metrics**:\n   - **Faithfulness**: Particularly used in translation tasks, this metric evaluates how accurately the generated text represents the original content.\n   - **Relevance**: Common in summarization tasks, this focuses on whether the content addresses the significant elements of the source material.\n\n4. **Factual Consistency**: Newer issues arising from generative models involve undesired hallucinations, so evaluating factual accuracy is crucial for applications demanding factual reliability.\n\n5. **Safety**: This broader metric examines whether the generated outputs could harm users or propagate biases and toxicity.\n\n6. **Human Evaluation**: Despite the availability of automatic metrics, human judgment remains essential, particularly for assessing open-ended text generation. It's often viewed as the North Star metric to guide application development.\n\n7. **Use of AI as Evaluators**: AI models can also be employed to assess fluency and coherence, providing a method for automated evaluation.\n\nIn practice, teams should consider combining these metrics during both experimentation and production stages to ensure the generated text meets the desired quality and functionality.",
    "context": [
      {
        "start": 11662,
        "text": "nded text generation is\ncalled NLG (natural language generation). NLG tasks in the early 2010s\nincluded translation, summarization, and paraphrasing.\nMetrics used to evaluate the quality of generated texts back then included\nfluency and coherence. Fluency measures whether the text is grammatically\ncorrect and natural-sounding (does this sound like something written by a\nfluent speaker?). Coherence measures how well-structured the whole text is\n(does it follow a logical structure?). Each task might also have its own\nmetrics. For example, a metric a translation task might use is faithfulness:\nhow faithful is the generated translation to the original sentence? A metric\nthat a summarization task might use is relevance: does the summary focus\non the most important aspects of the source document? (Li et al., 2022).\nSome early NLG metrics, including faithfulness and relevance, have been\nrepurposed, with significant modifications, to evaluate the outputs of\nfoundation models. As generative mod",
        "chapter": 4
      },
      {
        "start": 12495,
        "text": "LG metrics, including faithfulness and relevance, have been\nrepurposed, with significant modifications, to evaluate the outputs of\nfoundation models. As generative models improved, many issues of early\nNLG systems went away, and the metrics used to track these issues became\nless important. In the 2010s, generated texts didn’t sound natural. They\nwere typically full of grammatical errors and awkward sentences. Fluency\nand coherence, then, were important metrics to track. However, as language\nmodels’ generation capabilities have improved, AI-generated texts have\nbecome nearly indistinguishable from human-generated texts. Fluency and\n2\ncoherence become less important. However, these metrics can still be\nuseful for weaker models or for applications involving creative writing and\nlow-resource languages. Fluency and coherence can be evaluated using AI\nas a judge—asking an AI model how fluent and coherent a text is—or using\nperplexity, as discussed in Chapter 3.\nGenerative models, with their ",
        "chapter": 4
      },
      {
        "start": 13328,
        "text": "an be evaluated using AI\nas a judge—asking an AI model how fluent and coherent a text is—or using\nperplexity, as discussed in Chapter 3.\nGenerative models, with their new capabilities and new use cases, have new\nissues that require new metrics to track. The most pressing issue is\nundesired hallucinations. Hallucinations are desirable for creative tasks, not\nfor tasks that depend on factuality. A metric that many application\ndevelopers want to measure is factual consistency. Another issue commonly\ntracked is safety: can the generated outputs cause harm to users and\nsociety? Safety is an umbrella term for all types of toxicity and biases.\nThere are many other measurements that an application developer might\ncare about. For example, when I built my AI-powered writing assistant, I\ncared about controversiality, which measures content that isn’t necessarily\nharmful but can cause heated debates. Some people might care about\nfriendliness, positivity, creativity, or conciseness, but I won’t be ",
        "chapter": 4
      },
      {
        "start": 10829,
        "text": "bility to\ndifferentiate good responses from bad responses (classification), which is\ndifferent from the ability to generate good responses. MCQs are best suited\nfor evaluating knowledge (“does the model know that Paris is the capital of\nFrance?”) and reasoning (“can the model infer from a table of business\nexpenses which department is spending the most?”). They aren’t ideal for\nevaluating generation capabilities such as summarization, translation, and\nessay writing. Let’s discuss how generation capabilities can be evaluated in\nthe next section.\nGeneration Capability\nAI was used to generate open-ended outputs long before generative AI\nbecame a thing. For decades, the brightest minds in NLP (natural language\nprocessing) have been working on how to evaluate the quality of open-\nended outputs. The subfield that studies open-ended text generation is\ncalled NLG (natural language generation). NLG tasks in the early 2010s\nincluded translation, summarization, and paraphrasing.\nMetrics used to e",
        "chapter": 4
      },
      {
        "start": 34153,
        "text": " “use a respectful tone”). However, the verification of these\nexpanded instruction types can’t be easily automated. If you instruct a\nmodel to “use language appropriate to a young audience”, how do you\nautomatically verify if the output is indeed appropriate for a young\naudience?\nFor verification, INFOBench authors constructed a list of criteria for each\ninstruction, each framed as a yes/no question. For example, the output to the\ninstruction “Make a questionnaire to help hotel guests write hotel reviews”\ncan be verified using three yes/no questions:\n1. Is the generated text a questionnaire?\n2. Is the generated questionnaire designed for hotel guests?\n3. Is the generated questionnaire helpful for hotel guests to write hotel\nreviews?\nA model is considered to successfully follow an instruction if its output\nmeets all the criteria for this instruction. Each of these yes/no questions can\nbe answered by a human or AI evaluator. If the instruction has three criteria\nand the evaluator determi",
        "chapter": 4
      },
      {
        "start": 37485,
        "text": "lude RoleLLM (Wang et al., 2023) and\nCharacterEval (Tu et al., 2024). CharacterEval used human annotators and\ntrained a reward model to evaluate each roleplaying aspect on a five-point\nscale. RoleLLM evaluates a model’s ability to emulate a persona using both\ncarefully crafted similarity scores (how similar the generated outputs are to\nthe expected outputs) and AI judges.\nIf AI in your application is supposed to assume a certain role, make sure to\nevaluate whether your model stays in character. Depending on the role, you\nmight be able to create heuristics to evaluate the model’s outputs. For\nexample, if the role is someone who doesn’t talk a lot, a heuristic would be\nthe average of the model’s outputs. Other than that, the easiest automatic\nevaluation approach is AI as a judge. You should evaluate the roleplaying\nAI on both style and knowledge. For example, if a model is supposed to\ntalk like Jackie Chan, its outputs should capture Jackie Chan’s style and are\n8\ngenerated based on Jacki",
        "chapter": 4
      },
      {
        "start": 6664,
        "text": " capabilities are commonly evaluated using exact\nevaluation. Coding-related capabilities are typically evaluated using\nfunctional correctness, as discussed in Chapter 3. While functional\ncorrectness is important, it might not be the only aspect that you care about.\nYou might also care about efficiency and cost. For example, would you\nwant a car that runs but consumes an excessive amount of fuel? Similarly, if\nan SQL query generated by your text-to-SQL model is correct but takes too\nlong or requires too much memory to run, it might not be usable.\nEfficiency can be exactly evaluated by measuring runtime or memory\nusage. BIRD-SQL (Li et al., 2023) is an example of a benchmark that takes\ninto account not only the generated query’s execution accuracy but also its\nefficiency, which is measured by comparing the runtime of the generated\nquery with the runtime of the ground truth SQL query.\nYou might also care about code readability. If the generated code runs but\nnobody can understand it, it w",
        "chapter": 4
      },
      {
        "start": 93296,
        "text": " and 0 for neutral. Which scoring system to use depends on your\ndata and your needs.\nOn this scoring system, create a rubric with examples. What does a\nresponse with a score of 1 look like and why does it deserve a 1? Validate\nyour rubric with humans: yourself, coworkers, friends, etc. If humans find it\nhard to follow the rubric, you need to refine it to make it unambiguous. This\nprocess can require a lot of back and forth, but it’s necessary. A clear\nguideline is the backbone of a reliable evaluation pipeline. This guideline\ncan also be reused later for training data annotation, as discussed in\nChapter 8.\nTie evaluation metrics to business metrics\nWithin a business, an application must serve a business goal. The\napplication’s metrics must be considered in the context of the business\nproblem it’s built to solve.\nFor example, if your customer support chatbot’s factual consistency is 80%,\nwhat does it mean for the business? For example, this level of factual\nconsistency might make the ch",
        "chapter": 4
      },
      {
        "start": 32487,
        "text": "traints most {N} sentences.\nLength Number There should be {N} paragraphs.\nconstraints paragraphs + first Paragraphs and only paragraphs\nword in i-th are separated from each other by\nparagraph two line breaks. The {i}-th\nparagraph must start with word\n{first_word}.\nDetectable Postscript At the end of your response,\ncontent please explicitly add a postscript\nstarting with {postscript marker}.\nDetectable Number The response must contain at least\ncontent placeholder {N} placeholders represented by\nsquare brackets, such as [address].\nDetectable Number bullets Your answer must contain exactly\nformat {N} bullet points. Use the\nInstruction\nInstruction Description\ngroup\nmarkdown bullet points such as: *\nThis is a point.\nDetectable Title Your answer must contain a title,\nformat wrapped in double angular\nbrackets, such as <<poem of\njoy>>.\nDetectable Choose from Answer with one of the following\nformat options: {options}.\nDetectable Minimum number Highlight at least {N} sections in\nformat highlight",
        "chapter": 4
      },
      {
        "start": 97461,
        "text": "ut this prediction. Logprobs can also be\nused to evaluate a model’s perplexity for a generated text, which can be\nused for measurements such as fluency and factual consistency.\nUse automatic metrics as much as possible, but don’t be afraid to fall back\non human evaluation, even in production. Having human experts manually\nevaluate a model’s quality is a long-standing practice in AI. Given the\nchallenges of evaluating open-ended responses, many teams are looking at\nhuman evaluation as the North Star metric to guide their application\ndevelopment. Each day, you can use human experts to evaluate a subset of\nyour application’s outputs that day to detect any changes in the application’s\nperformance or unusual patterns in usage. For example, LinkedIn developed\na process to manually evaluate up to 500 daily conservations with their AI\nsystems.\nConsider evaluation methods to be used not just during experimentation but\nalso during production. During experimentation, you might have reference\ndata",
        "chapter": 4
      }
    ],
    "input_tokens": 2538,
    "output_tokens": 307,
    "input_cost": 0.0003807,
    "output_cost": 0.0001842,
    "total_cost": 0.0005649,
    "reference_answer": "The chapter outlines key evaluation metrics for generated text quality including fluency, coherence, and task-specific measures like faithfulness and relevance."
  },
  {
    "question": "how to reduce latency when using AI models",
    "answer": "To reduce latency when using AI models, consider the following strategies based on the *AI Engineering book*:\n\n1. **Prompt Optimization**: Carefully structure your prompts to be concise and clear. This can help limit the number of tokens generated, as autoregressive models generate outputs token by token.\n\n2. **Set Stopping Conditions**: Implement specific stopping criteria for the model's output generation. This prevents unnecessary continuation once the desired output is achieved.\n\n3. **Token Management**: Minimize both input and output token counts to manage costs and improve latency. Fewer tokens generally lead to lower latency.\n\n4. **Model Selection**: Choose models that meet your latency requirements. Start by filtering out models that do not meet your expectations for latency, and experiment with the options that fit your needs.\n\n5. **Monitor and Optimize**: Continually monitor the model's performance in production to identify areas for improvement regarding latency.\n\n6. **Evaluate Latency Metrics**: Understand and focus on relevant latency metrics, such as time to first token or time per token, to identify how different models perform.\n\nBy implementing these techniques, you can effectively reduce the latency involved in using AI models.",
    "context": [
      {
        "start": 40817,
        "text": "iables. Autoregressive language models typically generate\noutputs token by token. The more tokens it has to generate, the higher the\ntotal latency. You can control the total latency observed by users by careful\nprompting, such as instructing the model to be concise, setting a stopping\ncondition for generation (discussed in Chapter 2), or other optimization\ntechniques (discussed in Chapter 9).\nTIP\nWhen evaluating models based on latency, it’s important to differentiate between the must-have and\nthe nice-to-have. If you ask users if they want lower latency, nobody will ever say no. But high\nlatency is often an annoyance, not a deal breaker.\nIf you use model APIs, they typically charge by tokens. The more input and\noutput tokens you use, the more expensive it is. Many applications then try\nto reduce the input and output token count to manage cost.\nIf you host your own models, your cost, outside engineering cost, is\ncompute. To make the most out of the machines they have, many people\nchoos",
        "chapter": 4
      },
      {
        "start": 39984,
        "text": "e discussed in detail in Chapter 9, so this section will be\nquick.\nOptimizing for multiple objectives is an active field of study called Pareto\noptimization. When optimizing for multiple objectives, it’s important to be\nclear about what objectives you can and can’t compromise on. For example,\nif latency is something you can’t compromise on, you start with latency\nexpectations for different models, filter out all the models that don’t meet\nyour latency requirements, and then pick the best among the rest.\nThere are multiple metrics for latency for foundation models, including but\nnot limited to time to first token, time per token, time between tokens, time\nper query, etc. It’s important to understand what latency metrics matter to\nyou.\nLatency depends not only on the underlying model but also on each prompt\nand sampling variables. Autoregressive language models typically generate\noutputs token by token. The more tokens it has to generate, the higher the\ntotal latency. You can control the",
        "chapter": 4
      },
      {
        "start": 59976,
        "text": " might be sufficient.\nAnother reason that might cause open source models to lag behind is that\nopen source developers don’t receive feedback from users to improve their\nmodels, the way commercial models do. Once a model is open sourced,\nmodel developers have no idea how the model is being used, and how well\nthe model works in the wild.\nFunctionality\nMany functionalities are needed around a model to make it work for a use\ncase. Here are some examples of these functionalities:\nScalability: making sure the inference service can support your\napplication’s traffic while maintaining the desirable latency and cost.\nFunction calling: giving the model the ability to use external tools, which\nis essential for RAG and agentic use cases, as discussed in Chapter 6.\nStructured outputs, such as asking models to generate outputs in JSON\nformat.\nOutput guardrails: mitigating risks in the generated responses, such as\nmaking sure the responses aren’t racist or sexist.\nMany of these functionalities are ch",
        "chapter": 4
      },
      {
        "start": 44149,
        "text": "see if\nsmaller models would work. If you decide to do finetuning, you might start\nwith a small model to test your code and move toward the biggest model\nthat fits your hardware constraints (e.g., one GPU).\nIn general, the selection process for each technique typically involves two\nsteps:\n1. Figuring out the best achievable performance\n2. Mapping models along the cost–performance axes and choosing the\nmodel that gives the best performance for your bucks\nHowever, the actual selection process is a lot more nuanced. Let’s explore\nwhat it looks like.\nModel Selection Workflow\nWhen looking at models, it’s important to differentiate between hard\nattributes (what is impossible or impractical for you to change) and soft\nattributes (what you can and are willing to change).\nHard attributes are often the results of decisions made by model providers\n(licenses, training data, model size) or your own policies (privacy, control).\nFor some use cases, the hard attributes can reduce the pool of potential\n",
        "chapter": 4
      },
      {
        "start": 45815,
        "text": "ase. For example, latency is a soft attribute if you have access to\nthe model to optimize it to run faster. It’s a hard attribute if you use a model\nhosted by someone else.\nAt a high level, the evaluation workflow consists of four steps (see\nFigure 4-5):\n1. Filter out models whose hard attributes don’t work for you. Your list of\nhard attributes depends heavily on your own internal policies, whether\nyou want to use commercial APIs or host your own models.\n2. Use publicly available information, e.g., benchmark performance and\nleaderboard ranking, to narrow down the most promising models to\nexperiment with, balancing different objectives such as model quality,\nlatency, and cost.\n3. Run experiments with your own evaluation pipeline to find the best\nmodel, again, balancing all your objectives.\n4. Continually monitor your model in production to detect failure and\ncollect feedback to improve your application.\nFigure 4-5. An overview of the evaluation workflow to evaluate models for your appli",
        "chapter": 4
      },
      {
        "start": 86632,
        "text": "ate hold-out data.\nPublic benchmarks will help you filter out bad models, but they won’t help\nyou find the best models for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each application might consist of\nmany ",
        "chapter": 4
      },
      {
        "start": 39151,
        "text": "le_name}’’ is\n‘‘{role_description_and_catchphrases}’’. I\nneed to rank the following models based on the\ntwo criteria below:\n1. Which one has more pronounced role speaking\nstyle, and speaks more in line with the role\ndescription. The more distinctive the speaking\nstyle, the better.\n2. Which one’s output contains more knowledge\nand memories related to the role; the richer,\nthe better. (If the question contains\nreference answers, then the role−specific\nknowledge and memories are based on the\nreference answer. )\nCost and Latency\nA model that generates high-quality outputs but is too slow and expensive\nto run will not be useful. When evaluating models, it’s important to balance\nmodel quality, latency, and cost. Many companies opt for lower-quality\nmodels if they provide better cost and latency. Cost and latency\noptimization are discussed in detail in Chapter 9, so this section will be\nquick.\nOptimizing for multiple objectives is an active field of study called Pareto\noptimization. When opti",
        "chapter": 4
      },
      {
        "start": 64974,
        "text": " video) a model that refuses to generate real faces\nwon’t work. A company I advise, Convai, builds 3D AI characters that can\ninteract in 3D environments, including picking up objects. When working\nwith commercial models, they ran into an issue where the models kept\nresponding: “As an AI model, I don’t have physical abilities”. Convai ended\nup finetuning open source models.\nThere’s also the risk of losing access to a commercial model, which can be\npainful if you’ve built your system around it. You can’t freeze a commercial\nmodel the way you can with open source models. Historically, commercial\nmodels lack transparency in model changes, versions, and roadmaps.\nModels are frequently updated, but not all changes are announced in\nadvance or even announced at all. Your prompts might stop working as\nexpected and you have no idea. Unpredictable changes also make\ncommercial models unusable for strictly regulated applications. However, I\nsuspect that this historical lack of transparency in model",
        "chapter": 4
      },
      {
        "start": 17493,
        "text": "r sheds light on how AI models process conflicting\ninformation and determine what the facts are. For example, Wan et al.\n(2024) found that existing “models rely heavily on the relevance of a\nwebsite to the query, while largely ignoring stylistic features that humans\nfind important such as whether a text contains scientific references or is\nwritten with a neutral tone.”\nTIP\nWhen designing metrics to measure hallucinations, it’s important to analyze the model’s outputs to\nunderstand the types of queries that it is more likely to hallucinate on. Your benchmark should focus\nmore on these queries.\nFor example, in one of my projects, I found that the model I was working with tended to hallucinate\non two types of queries:\n1. Queries that involve niche knowledge. For example, it was more likely to hallucinate when I\nasked it about the VMO (Vietnamese Mathematical Olympiad) than the IMO (International\nMathematical Olympiad), because the VMO is much less commonly referenced than the IMO.\n2. Quer",
        "chapter": 4
      },
      {
        "start": 52479,
        "text": "s. Chapter 9\ndiscusses how to optimize inference services.\nFigure 4-6. An inference service runs the model and provides an interface for users to access the\nmodel.\nAfter developing a model, a developer can choose to open source it, make it\naccessible via an API, or both. Many model developers are also model\nservice providers. Cohere and Mistral open source some models and\nprovide APIs for some. OpenAI is typically known for their commercial\nmodels, but they’ve also open sourced models (GPT-2, CLIP). Typically,\nmodel providers open source weaker models and keep their best models\nbehind paywalls, either via APIs or to power their products.\nModel APIs can be available through model providers (such as OpenAI and\nAnthropic), cloud service providers (such as Azure and GCP [Google Cloud\nPlatform]), or third-party API providers (such as Databricks Mosaic,\nAnyscale, etc.). The same model can be available through different APIs\nwith different features, constraints, and pricings. For example, GPT",
        "chapter": 4
      }
    ],
    "input_tokens": 2561,
    "output_tokens": 237,
    "input_cost": 0.00038415,
    "output_cost": 0.0001422,
    "total_cost": 0.00052635,
    "reference_answer": "You can manage latency by optimizing prompts for conciseness, setting stopping conditions for generation, and employing other optimization techniques discussed in previous chapters."
  },
  {
    "question": "why do monopolies harm consumers?",
    "answer": "Monopolies harm consumers primarily because they lead to the loss of consumer surplus, resulting in higher prices and lower levels of output. In a monopoly, a single firm dominates the market and can set prices without the competitive pressure that typically benefits consumers. This lack of competition means consumers may face inflated prices and fewer choices, undermining their welfare and overall market efficiency.",
    "context": [
      {
        "start": 8330,
        "text": "e model outputs option A, the model is wrong.\nThis is the approach that most public benchmarks follow. In April 2024,\n75% of the tasks in Eleuther’s lm-evaluation-harness are multiple-choice,\nincluding UC Berkeley’s MMLU (2020), Microsoft’s AGIEval (2023), and\nthe AI2 Reasoning Challenge (ARC-C) (2018). In their paper, AGIEval’s\nauthors explained that they excluded open-ended tasks on purpose to avoid\ninconsistent assessment.\nHere’s an example of a multiple-choice question in the MMLU benchmark:\nQuestion: One of the reasons that the government discourages and\nregulates monopolies is that\n(A) Producer surplus is lost and consumer surplus is gained.\n(B) Monopoly prices ensure productive efficiency but cost society\nallocative efficiency.\n(C) Monopoly firms do not engage in significant research and\ndevelopment.\n(D) Consumer surplus is lost with higher prices and lower levels of\noutput.\nLabel: (D)\nA multiple-choice question (MCQ) might have one or more correct\nanswers. A common metric is ac",
        "chapter": 4
      },
      {
        "start": 80801,
        "text": "is not only\nbecause public benchmarks are unlikely to represent your application’s\nneeds perfectly, but also because they are likely contaminated. How public\nbenchmarks get contaminated and how to handle data contamination will be\nthe topic of the next section.\nData contamination with public benchmarks\nData contamination is so common that there are many different names for\nit, including data leakage, training on the test set, or simply cheating. Data\ncontamination happens when a model was trained on the same data it’s\nevaluated on. If so, it’s possible that the model just memorizes the answers\nit saw during training, causing it to achieve higher evaluation scores than it\nshould. A model that is trained on the MMLU benchmark can achieve high\nMMLU scores without being useful.\nRylan Schaeffer, a PhD student at Stanford, demonstrated this beautifully in\nhis 2023 satirical paper “Pretraining on the Test Set Is All You Need”. By\ntraining exclusively on data from several benchmarks, his one-m",
        "chapter": 4
      },
      {
        "start": 114121,
        "text": "ey?It might be argued that\nMeta supports open source models only to keep their competitors (Google, Microsoft/OpenAI) in\ncheck. Both Mistral and Cohere have open source models, but they also have APIs. At some point,\ninference services on top of Mistral and Cohere models become their competitors.There’s the\nargument that open source is better for society, and maybe that’s enough as an incentive. People who\nwant what’s good for society will continue to push for open source, and maybe there will be enough\ncollective goodwill to help open source prevail. I certainly hope so.\n8\nThe companies that get hit the most by API costs are probably not the biggest companies. The\nbiggest companies might be important enough to service providers to negotiate favorable terms.\n9\nThis is similar to the philosophy in software infrastructure to always use the most popular tools that\nhave been extensively tested by the community.\n0\nWhen I posted a question on Hugging Face’s Discord about why they chose certa",
        "chapter": 4
      },
      {
        "start": 25823,
        "text": "ideology. For example,\nstudies (Feng et al., 2023; Motoki et al., 2023; and Hartman et al., 2023)\nhave shown that models, depending on their training, can be imbued\nwith political biases. For example, OpenAI’s GPT-4 is more left-winged\nand libertarian-leaning, whereas Meta’s Llama is more authoritarian, as\nshown in Figure 4-3.\nFigure 4-3. Political and economic leanings of different foundation models (Feng et al., 2023).\nThe image is licensed under CC BY 4.0.\nIt’s possible to use general-purpose AI judges to detect these scenarios, and\nmany people do. GPTs, Claude, and Gemini can detect many harmful\n5\noutputs if prompted properly. These model providers also need to develop\nmoderation tools to keep their models safe, and some of them expose their\nmoderation tools for external use.\nHarmful behaviors aren’t unique to AI outputs. They’re unfortunately\nextremely common online. Many models developed to detect toxicity in\nhuman-generated texts can be used for AI-generated texts. These\nspecial",
        "chapter": 4
      },
      {
        "start": 113288,
        "text": "The story was reported by several outlets, including TechRadar (see “Samsung Workers Made a\nMajor Error by Using ChatGPT”, by Lewis Maddison (April 2023).\n6\nAs regulations are evolving around the world, requirements for auditable information of models and\ntraining data may increase. Commercial models may be able to provide certifications, saving\ncompanies from the effort.\n7\nUsers want models to be open source because open means more information and more options, but\nwhat’s in it for model developers? Many companies have sprung up to capitalize on open source\nmodels by providing inference and finetuning services. It’s not a bad thing. Many people need these\nservices to leverage open source models. But, from model developers’ perspective, why invest\nmillions, if not billions, into building models just for others to make money?It might be argued that\nMeta supports open source models only to keep their competitors (Google, Microsoft/OpenAI) in\ncheck. Both Mistral and Cohere have open sourc",
        "chapter": 4
      },
      {
        "start": 61642,
        "text": "provider lets\nyou. Imagine that you’ve maxed out a model’s performance with prompting\nand want to finetune that model. If this model is proprietary and the model\nprovider doesn’t have a finetuning API, you won’t be able to do it.\nHowever, if it’s an open source model, you can find a service that offers\nfinetuning on that model, or you can finetune it yourself. Keep in mind that\nthere are multiple types of finetuning, such as partial finetuning and full\nfinetuning, as discussed in Chapter 7. A commercial model provider might\nsupport only some types of finetuning, not all.\nAPI cost versus engineering cost\nModel APIs charge per usage, which means that they can get prohibitively\nexpensive with heavy usage. At a certain scale, a company that is bleeding\n18\nits resources using APIs might consider hosting their own models.\nHowever, hosting a model yourself requires nontrivial time, talent, and\nengineering effort. You’ll need to optimize the model, scale and maintain\nthe inference service as n",
        "chapter": 4
      },
      {
        "start": 63308,
        "text": "el to follow a standard API, which makes it easier to swap models.\nMany model developers try to make their models mimic the API of the most\npopular models. As of this writing, many API providers mimic OpenAI’s\nAPI.\nYou might also prefer models with good community support. The more\ncapabilities a model has, the more quirks it has. A model with a large\ncommunity of users means that any issue you encounter may already have\n19\nbeen experienced by others, who might have shared solutions online.\nControl, access, and transparency\nA 2024 study by a16z shows two key reasons that enterprises care about\nopen source models are control and customizability, as shown in Figure 4-8.\nFigure 4-8. Why enterprises care about open source models. Image from the 2024 study by a16z.\nIf your business depends on a model, it’s understandable that you would\nwant some control over it, and API providers might not always give you the\nlevel of control you want. When using a service provided by someone else,\nyou’re su",
        "chapter": 4
      },
      {
        "start": 58310,
        "text": "oroughly inspect a dataset of the size\ntypically used to train foundation models.\nGiven the same concern, many companies opt for commercial models\ninstead. Open source models tend to have limited legal resources compared\nto commercial models. If you use an open source model that infringes on\ncopyrights, the infringed party is unlikely to go after the model developers,\nand more likely to go after you. However, if you use a commercial model,\nthe contracts you sign with the model providers can potentially protect you\n16\nfrom data lineage risks.\nPerformance\nVarious benchmarks have shown that the gap between open source models\nand proprietary models is closing. Figure 4-7 shows this gap decreasing on\nthe MMLU benchmark over time. This trend has made many people believe\nthat one day, there will be an open source model that performs just as well,\nif not better, than the strongest proprietary model.\nAs much as I want open source models to catch up with proprietary models,\nI don’t think the inc",
        "chapter": 4
      },
      {
        "start": 60809,
        "text": "format.\nOutput guardrails: mitigating risks in the generated responses, such as\nmaking sure the responses aren’t racist or sexist.\nMany of these functionalities are challenging and time-consuming to\nimplement, which makes many companies turn to API providers that\nprovide the functionalities they want out of the box.\nThe downside of using a model API is that you’re restricted to the\nfunctionalities that the API provides. A functionality that many use cases\nneed is logprobs, which are very useful for classification tasks, evaluation,\nand interpretability. However, commercial model providers might be\nhesitant to expose logprobs for fear of others using logprobs to replicate\ntheir models. In fact, many model APIs don’t expose logprobs or expose\nonly limited logprobs.\nYou can also only finetune a commercial model if the model provider lets\nyou. Imagine that you’ve maxed out a model’s performance with prompting\nand want to finetune that model. If this model is proprietary and the model\nprovi",
        "chapter": 4
      },
      {
        "start": 111622,
        "text": "that the roleplaying\nmodel doesn’t speak Vietnamese. The “negative knowledge” check is very important for gaming. You\ndon’t want an NPC to accidentally give players spoilers.\n9\nHowever, the electricity cost might be different, depending on the usage.\n0\nAnother argument for making training data public is that since models are likely trained on data\nscraped from the internet, which was generated by the public, the public should have the right to\naccess the models’ training data.\n1\nIn spirit, this restriction is similar to the Elastic License that forbids companies from offering the\nopen source version of Elastic as a hosted service and competing with the Elasticsearch platform.\n2\nIt’s possible that a model’s output can’t be used to improve other models, even if its license allows\nthat. Consider model X that is trained on ChatGPT’s outputs. X might have a license that allows this,\nbut if ChatGPT doesn’t, then X violated ChatGPT’s terms of use, and therefore, X can’t be used. This\nis why k",
        "chapter": 4
      }
    ],
    "input_tokens": 2668,
    "output_tokens": 73,
    "input_cost": 0.0004002,
    "output_cost": 4.38e-05,
    "total_cost": 0.000444,
    "reference_answer": "Monopolies lead to increased prices and reduced output, which results in a loss of consumer surplus as noted in option D of the question."
  },
  {
    "question": "How does data lineage affect AI model development?",
    "answer": "Data lineage significantly impacts AI model development in several ways:\n\n1. **Transparency and Trust**: Understanding the data lineage—where the data comes from, how it is collected, and its processing—is crucial for building trust. Without clear lineage, users and stakeholders might hesitate to adopt a model due to concerns over data quality and source reliability.\n\n2. **Copyright and Legal Concerns**: The lineage can inform companies about the copyright implications of using certain datasets. If a model is trained on copyrighted data, using it in a product could expose the company to legal risks. The evolving nature of intellectual property laws adds complexity to this aspect.\n\n3. **Model Choice**: Concerns about data lineage can influence whether a company opts for open source or proprietary models. Open models may allow for better scrutiny and assurance of data safety, while proprietary models can limit transparency but may provide better performance.\n\n4. **Data Contamination**: Awareness of data lineage helps avoid issues such as data contamination, where training data overlaps with evaluation benchmarks, potentially skewing performance results.\n\n5. **Privacy Considerations**: For organizations with strict data privacy policies, knowing the data lineage can determine whether it is safe to use external model APIs, which might otherwise risk exposing sensitive information.\n\nIn summary, data lineage is essential for making informed decisions about model development, ensuring compliance with laws, safeguarding intellectual property, and maintaining transparency.",
    "context": [
      {
        "start": 56644,
        "text": "ted in Chapter 5.\nData lineage and copyright\nData lineage and copyright concerns can steer a company in many\ndirections: toward open source models, toward proprietary models, or away\nfrom both.\nFor most models, there’s little transparency about what data a model is\ntrained on. In Gemini’s technical report, Google went into detail about the\nmodels’ performance but said nothing about the models’ training data other\nthan that “all data enrichment workers are paid at least a local living wage”.\nOpenAI’s CTO wasn’t able to provide a satisfactory answer when asked\nwhat data was used to train their models.\nOn top of that, the IP laws around AI are actively evolving. While the US\nPatent and Trademark Office (USPTO) made clear in 2024 that “AI-assisted\ninventions are not categorically unpatentable”, an AI application’s\npatentability depends on “whether the human contribution to an innovation\nis significant enough to qualify for a patent.” It’s also unclear whether, if a\nmodel was trained on cop",
        "chapter": 4
      },
      {
        "start": 57477,
        "text": "ty depends on “whether the human contribution to an innovation\nis significant enough to qualify for a patent.” It’s also unclear whether, if a\nmodel was trained on copyrighted data, and you use this model to create\nyour product, you can defend your product’s IP. Many companies whose\nexistence depends upon their IPs, such as gaming and movie studios, are\nhesitant to use AI to aid in the creation of their products, at least until IP\nlaws around AI are clarified (James Vincent, The Verge, November 15,\n2022).\nConcerns over data lineage have driven some companies toward fully open\nmodels, whose training data has been made publicly available. The\nargument is that this allows the community to inspect the data and make\nsure that it’s safe to use. While it sounds great in theory, in practice, it’s\nchallenging for any company to thoroughly inspect a dataset of the size\ntypically used to train foundation models.\nGiven the same concern, many companies opt for commercial models\ninstead. Open source",
        "chapter": 4
      },
      {
        "start": 55811,
        "text": "ies.\nIf you use a model API, there’s a risk that the API provider will use your\ndata to train its models. Even though most model API providers claim they\ndon’t do that, their policies can change. In August 2023, Zoom faced a\nbacklash after people found out the company had quietly changed its terms\nof service to let Zoom use users’ service-generated data, including product\nusage data and diagnostics data, to train its AI models.\nWhat’s the problem with people using your data to train their models?\nWhile research in this area is still sparse, some studies suggest that AI\nmodels can memorize their training samples. For example, it’s been found\nthat Hugging Face’s StarCoder model memorizes 8% of its training set.\nThese memorized samples can be accidentally leaked to users or\nintentionally exploited by bad actors, as demonstrated in Chapter 5.\nData lineage and copyright\nData lineage and copyright concerns can steer a company in many\ndirections: toward open source models, toward proprietary ",
        "chapter": 4
      },
      {
        "start": 66640,
        "text": "ld be for privacy reasons, such as when you want to give an AI assistant\naccess to all your data, but don’t want your data to leave your device.\nTable 4-4 summarizes the pros and cons of using model APIs and self-\nhosting models.\nTable 4-4. Pros and cons of using model APIs and self-hosting models (cons in italics).\nUsing model APIs Self-hosting models\nData\nHave to send your Don’t have to send your\ndata to model data externally\nproviders, which Fewer checks and\nmeans your team can balances for data\naccidentally leak lineage/training data\nconfidential info copyright\nPerformance\nBest-performing The best open source\nmodel will likely be models will likely be a bit\nclosed source behind commercial\nmodels\nFunctionality\nMore likely to No/limited support for\nsupport scaling, function calling and\nfunction calling, structured outputs\nstructured outputs Can access logprobs and\nLess likely to expose intermediate outputs,\nlogprobs which are helpful for\nclassification tasks,\nUsing model APIs Self-ho",
        "chapter": 4
      },
      {
        "start": 54978,
        "text": "e are\nseven axes to consider: data privacy, data lineage, performance,\nfunctionality, costs, control, and on-device deployment.\nData privacy\nExternally hosted model APIs are out of the question for companies with\n14\nstrict data privacy policies that can’t send data outside of the organization.\nOne of the most notable early incidents was when Samsung employees put\nSamsung’s proprietary information into ChatGPT, accidentally leaking the\n15\n15\ncompany’s secrets. It’s unclear how Samsung discovered this leak and\nhow the leaked information was used against Samsung. However, the\nincident was serious enough for Samsung to ban ChatGPT in May 2023.\nSome countries have laws that forbid sending certain data outside their\nborders. If a model API provider wants to serve these use cases, they will\nhave to set up servers in these countries.\nIf you use a model API, there’s a risk that the API provider will use your\ndata to train its models. Even though most model API providers claim they\ndon’t do that",
        "chapter": 4
      },
      {
        "start": 67473,
        "text": "ts\nstructured outputs Can access logprobs and\nLess likely to expose intermediate outputs,\nlogprobs which are helpful for\nclassification tasks,\nUsing model APIs Self-hosting models\nevaluation, and\ninterpretability\nCost\nAPI cost Talent, time, engineering\neffort to optimize, host,\nmaintain (can be\nmitigated by using model\nhosting services)\nFinetuning\nCan only finetune Can finetune, quantize,\nmodels that model and optimize models (if\nproviders let you their licenses allow), but\nit can be hard to do so\nControl,\nRate limits Easier to inspect changes\naccess, and\nRisk of losing access in open source models\ntransparency\nto the model You can freeze a model\nLack of transparency to maintain its access, but\nin model changes and you’re responsible for\nversioning\nUsing model APIs Self-hosting models\nbuilding and maintaining\nmodel APIs\nEdge use cases\nCan’t run on device Can run on device, but\nwithout internet again, might be hard to\naccess do so\nThe pros and cons of each approach hopefully can help yo",
        "chapter": 4
      },
      {
        "start": 25823,
        "text": "ideology. For example,\nstudies (Feng et al., 2023; Motoki et al., 2023; and Hartman et al., 2023)\nhave shown that models, depending on their training, can be imbued\nwith political biases. For example, OpenAI’s GPT-4 is more left-winged\nand libertarian-leaning, whereas Meta’s Llama is more authoritarian, as\nshown in Figure 4-3.\nFigure 4-3. Political and economic leanings of different foundation models (Feng et al., 2023).\nThe image is licensed under CC BY 4.0.\nIt’s possible to use general-purpose AI judges to detect these scenarios, and\nmany people do. GPTs, Claude, and Gemini can detect many harmful\n5\noutputs if prompted properly. These model providers also need to develop\nmoderation tools to keep their models safe, and some of them expose their\nmoderation tools for external use.\nHarmful behaviors aren’t unique to AI outputs. They’re unfortunately\nextremely common online. Many models developed to detect toxicity in\nhuman-generated texts can be used for AI-generated texts. These\nspecial",
        "chapter": 4
      },
      {
        "start": 24157,
        "text": "erformance of several models on this benchmark, as\nshown in GPT-4’s technical report (2023). For comparison, the human\nexpert baseline, as reported in the TruthfulQA paper, is 94%.\nFactual consistency is a crucial evaluation criteria for RAG, retrieval-\naugmented generation, systems. Given a query, a RAG system retrieves\nrelevant information from external databases to supplement the model’s\ncontext. The generated response should be factually consistent with the\nretrieved context. RAG is a central topic in Chapter 6.\nFigure 4-2. The performance of different models on TruthfulQA, as shown in GPT-4’s technical\nreport.\nSafety\nOther than factual consistency, there are many ways in which a model’s\noutputs can be harmful. Different safety solutions have different ways of\ncategorizing harms—see the taxonomy defined in OpenAI’s content\nmoderation endpoint and Meta’s Llama Guard paper (Inan et al., 2023).\nChapter 5 also discusses more ways in which AI models can be unsafe and\nhow to make your sy",
        "chapter": 4
      },
      {
        "start": 82467,
        "text": "ed\nto create new benchmarks to evaluate their new models.\nData contamination can happen indirectly, such as when both evaluation\nand training data come from the same source. For example, you might\ninclude math textbooks in the training data to improve the model’s math\ncapabilities, and someone else might use questions from the same math\ntextbooks to create a benchmark to evaluate the model’s capabilities.\nData contamination can also happen intentionally for good reasons. Let’s\nsay you want to create the best possible model for your users. Initially, you\nexclude benchmark data from the model’s training data and choose the best\nmodel based on these benchmarks. However, because high-quality\nbenchmark data can improve the model’s performance, you then continue\ntraining your best model on benchmark data before releasing it to your\nusers. So the released model is contaminated, and your users won’t be able\nto evaluate it on contaminated benchmarks, but this might still be the right\nthing to d",
        "chapter": 4
      },
      {
        "start": 112455,
        "text": "atGPT’s outputs. X might have a license that allows this,\nbut if ChatGPT doesn’t, then X violated ChatGPT’s terms of use, and therefore, X can’t be used. This\nis why knowing a model’s data lineage is so important.\n3\nFor example, as of this writing, you can access GPT-4 models only via OpenAI or Azure. Some\nmight argue that being able to provide services on top of OpenAI’s proprietary models is a key\nreason Microsoft invested in OpenAI.\n4\nInterestingly enough, some companies with strict data privacy requirements have told me that even\nthough they can’t usually send data to third-party services, they’re okay with sending their data to\nmodels hosted on GCP, AWS, and Azure. For these companies, the data privacy policy is more about\nwhat services they can trust. They trust big cloud providers but don’t trust other startups.\n5\nThe story was reported by several outlets, including TechRadar (see “Samsung Workers Made a\nMajor Error by Using ChatGPT”, by Lewis Maddison (April 2023).\n6\nAs regulat",
        "chapter": 4
      }
    ],
    "input_tokens": 2722,
    "output_tokens": 279,
    "input_cost": 0.0004083,
    "output_cost": 0.0001674,
    "total_cost": 0.0005757,
    "reference_answer": "Data lineage influences decisions about adopting open source versus proprietary models, alongside navigating copyright concerns related to data usage in AI training."
  },
  {
    "question": "open source vs proprietary model performance",
    "answer": "The performance comparison between open source and proprietary models indicates that while the gap is narrowing, proprietary models currently hold a performance advantage. Various benchmarks, such as the MMLU benchmark, show that open source models are improving over time. However, incentives exist for companies to keep their best models proprietary rather than open-sourcing them. Consequently, it is likely that the strongest open source models will lag behind the strongest proprietary offerings for the foreseeable future.\n\nOpen source models may still be adequate for many use cases that do not require the highest performance. One challenge for open source models is the lack of user feedback, which hinders their improvement compared to proprietary models that often receive ongoing support and refinement based on user interactions.\n\nEssential functionalities, ease of use, customization capabilities, and community support also play significant roles in model selection. Overall, while open source models are becoming more competitive, the proprietary models still dominate in terms of performance and may better cater to enterprise needs in critical applications.",
    "context": [
      {
        "start": 58310,
        "text": "oroughly inspect a dataset of the size\ntypically used to train foundation models.\nGiven the same concern, many companies opt for commercial models\ninstead. Open source models tend to have limited legal resources compared\nto commercial models. If you use an open source model that infringes on\ncopyrights, the infringed party is unlikely to go after the model developers,\nand more likely to go after you. However, if you use a commercial model,\nthe contracts you sign with the model providers can potentially protect you\n16\nfrom data lineage risks.\nPerformance\nVarious benchmarks have shown that the gap between open source models\nand proprietary models is closing. Figure 4-7 shows this gap decreasing on\nthe MMLU benchmark over time. This trend has made many people believe\nthat one day, there will be an open source model that performs just as well,\nif not better, than the strongest proprietary model.\nAs much as I want open source models to catch up with proprietary models,\nI don’t think the inc",
        "chapter": 4
      },
      {
        "start": 59143,
        "text": "orms just as well,\nif not better, than the strongest proprietary model.\nAs much as I want open source models to catch up with proprietary models,\nI don’t think the incentives are set up for it. If you have the strongest model\navailable, would you rather open source it for other people to capitalize on\n17\nit, or would you try to capitalize on it yourself? It’s a common practice for\ncompanies to keep their strongest models behind APIs and open source their\nweaker models.\nFigure 4-7. The gap between open source models and proprietary models is decreasing on the\nMMLU benchmark. Image by Maxime Labonne.\nFor this reason, it’s likely that the strongest open source model will lag\nbehind the strongest proprietary models for the foreseeable future.\nHowever, for many use cases that don’t need the strongest models, open\nsource models might be sufficient.\nAnother reason that might cause open source models to lag behind is that\nopen source developers don’t receive feedback from users to improve thei",
        "chapter": 4
      },
      {
        "start": 47481,
        "text": "ate the dizzying\nnumber of public benchmarks and why you can’t trust them. This will set\nthe stage for the last section in the chapter. Because public benchmarks\ncan’t be trusted, you need to design your own evaluation pipeline with\nprompts and metrics you can trust.\nModel Build Versus Buy\nAn evergreen question for companies when leveraging any technology is\nwhether to build or buy. Since most companies won’t be building\nfoundation models from scratch, the question is whether to use commercial\nmodel APIs or host an open source model yourself. The answer to this\nquestion can significantly reduce your candidate model pool.\nLet’s first go into what exactly open source means when it comes to\nmodels, then discuss the pros and cons of these two approaches.\nOpen source, open weight, and model licenses\nThe term “open source model” has become contentious. Originally, open\nsource was used to refer to any model that people can download and use.\nFor many use cases, being able to download the model",
        "chapter": 4
      },
      {
        "start": 59976,
        "text": " might be sufficient.\nAnother reason that might cause open source models to lag behind is that\nopen source developers don’t receive feedback from users to improve their\nmodels, the way commercial models do. Once a model is open sourced,\nmodel developers have no idea how the model is being used, and how well\nthe model works in the wild.\nFunctionality\nMany functionalities are needed around a model to make it work for a use\ncase. Here are some examples of these functionalities:\nScalability: making sure the inference service can support your\napplication’s traffic while maintaining the desirable latency and cost.\nFunction calling: giving the model the ability to use external tools, which\nis essential for RAG and agentic use cases, as discussed in Chapter 6.\nStructured outputs, such as asking models to generate outputs in JSON\nformat.\nOutput guardrails: mitigating risks in the generated responses, such as\nmaking sure the responses aren’t racist or sexist.\nMany of these functionalities are ch",
        "chapter": 4
      },
      {
        "start": 62475,
        "text": "er, hosting a model yourself requires nontrivial time, talent, and\nengineering effort. You’ll need to optimize the model, scale and maintain\nthe inference service as needed, and provide guardrails around your model.\nAPIs are expensive, but engineering can be even more so.\nOn the other hand, using another API means that you’ll have to depend on\ntheir SLA, service-level agreement. If these APIs aren’t reliable, which is\noften the case with early startups, you’ll have to spend your engineering\neffort on guardrails around that.\nIn general, you want a model that is easy to use and manipulate. Typically,\nproprietary models are easier to get started with and scale, but open models\nmight be easier to manipulate as their components are more accessible.\nRegardless of whether you go with open or proprietary models, you want\nthis model to follow a standard API, which makes it easier to swap models.\nMany model developers try to make their models mimic the API of the most\npopular models. As of this ",
        "chapter": 4
      },
      {
        "start": 63308,
        "text": "el to follow a standard API, which makes it easier to swap models.\nMany model developers try to make their models mimic the API of the most\npopular models. As of this writing, many API providers mimic OpenAI’s\nAPI.\nYou might also prefer models with good community support. The more\ncapabilities a model has, the more quirks it has. A model with a large\ncommunity of users means that any issue you encounter may already have\n19\nbeen experienced by others, who might have shared solutions online.\nControl, access, and transparency\nA 2024 study by a16z shows two key reasons that enterprises care about\nopen source models are control and customizability, as shown in Figure 4-8.\nFigure 4-8. Why enterprises care about open source models. Image from the 2024 study by a16z.\nIf your business depends on a model, it’s understandable that you would\nwant some control over it, and API providers might not always give you the\nlevel of control you want. When using a service provided by someone else,\nyou’re su",
        "chapter": 4
      },
      {
        "start": 113288,
        "text": "The story was reported by several outlets, including TechRadar (see “Samsung Workers Made a\nMajor Error by Using ChatGPT”, by Lewis Maddison (April 2023).\n6\nAs regulations are evolving around the world, requirements for auditable information of models and\ntraining data may increase. Commercial models may be able to provide certifications, saving\ncompanies from the effort.\n7\nUsers want models to be open source because open means more information and more options, but\nwhat’s in it for model developers? Many companies have sprung up to capitalize on open source\nmodels by providing inference and finetuning services. It’s not a bad thing. Many people need these\nservices to leverage open source models. But, from model developers’ perspective, why invest\nmillions, if not billions, into building models just for others to make money?It might be argued that\nMeta supports open source models only to keep their competitors (Google, Microsoft/OpenAI) in\ncheck. Both Mistral and Cohere have open sourc",
        "chapter": 4
      },
      {
        "start": 53312,
        "text": "such as Databricks Mosaic,\nAnyscale, etc.). The same model can be available through different APIs\nwith different features, constraints, and pricings. For example, GPT-4 is\navailable through both OpenAI and Azure APIs. There might be slight\ndifferences in the performance of the same model provided through\ndifferent APIs, as different APIs might use different techniques to optimize\nthis model, so make sure to run thorough tests when you switch between\nmodel APIs.\nCommercial models are only accessible via APIs licensed by the model\n13\ndevelopers. Open source models can be supported by any API provider,\nallowing you to pick and choose the provider that works best for you. For\ncommercial model providers, models are their competitive advantages. For\nAPI providers that don’t have their own models, APIs are their competitive\nadvantages. This means API providers might be more motivated to provide\nbetter APIs with better pricing.\nSince building scalable inference services for larger models is n",
        "chapter": 4
      },
      {
        "start": 54145,
        "text": "antages. This means API providers might be more motivated to provide\nbetter APIs with better pricing.\nSince building scalable inference services for larger models is nontrivial,\nmany companies don’t want to build them themselves. This has led to the\ncreation of many third-party inference and finetuning services on top of\nopen source models. Major cloud providers like AWS, Azure, and GCP all\nprovide API access to popular open source models. A plethora of startups\nare doing the same.\nNOTE\nThere are also commercial API providers that can deploy their services within your private networks.\nIn this discussion, I treat these privately deployed commercial APIs similarly to self-hosted models.\nThe answer to whether to host a model yourself or use a model API depends\non the use case. And the same use case can change over time. Here are\nseven axes to consider: data privacy, data lineage, performance,\nfunctionality, costs, control, and on-device deployment.\nData privacy\nExternally hosted model AP",
        "chapter": 4
      },
      {
        "start": 48314,
        "text": "” has become contentious. Originally, open\nsource was used to refer to any model that people can download and use.\nFor many use cases, being able to download the model is sufficient.\nHowever, some people argue that since a model’s performance is largely a\nfunction of what data it was trained on, a model should be considered open\nonly if its training data is also made publicly available.\nOpen data allows more flexible model usage, such as retraining the model\nfrom scratch with modifications in the model architecture, training process,\nor the training data itself. Open data also makes it easier to understand the\nmodel. Some use cases also required access to the training data for auditing\npurposes, for example, to make sure that the model wasn’t trained on\n10\ncompromised or illegally acquired data.\nTo signal whether the data is also open, the term “open weight” is used for\nmodels that don’t come with open data, whereas the term “open model” is\nused for models that come with open data.\nNOT",
        "chapter": 4
      }
    ],
    "input_tokens": 2524,
    "output_tokens": 194,
    "input_cost": 0.0003786,
    "output_cost": 0.0001164,
    "total_cost": 0.000495,
    "reference_answer": "The chapter outlines that while the performance gap between open source models and proprietary models is decreasing, strong proprietary models are likely to remain superior. Open source models may still be viable for many applications despite this lag."
  },
  {
    "question": "techniques for detecting factual inconsistency in language models",
    "answer": "Detecting factual inconsistency in language models involves several techniques outlined in the context provided. Here are the key approaches:\n\n1. **Local Factual Consistency**: This technique evaluates the model's output against explicitly provided context. If the output is supported by the given context, it is considered factually consistent. For example, if the context states the sky is purple, and the model outputs that the sky is purple, it is consistent; if it asserts the sky is blue, it is inconsistent.\n\n2. **Global Factual Consistency**: This checks the output against open knowledge or commonly accepted facts. An output is deemed factually correct if it aligns with well-established information, like widely accepted scientific facts.\n\n3. **AI as a Judge**: Models like GPT-3.5 and GPT-4 can assess factual consistency by evaluating the truthfulness of statements against a known source or context. Techniques like this utilize AI to predict the likelihood of a statement being truthful with high accuracy.\n\n4. **Self-Verification**: Techniques such as SelfCheckGPT assess the consistency of a response by generating multiple outputs. If these outputs disagree, the original response is likely deemed hallucinated or incorrect.\n\n5. **Knowledge-Augmented Verification**: This more sophisticated method uses search engines to validate information. The process involves breaking down responses into statements, refining them, proposing queries to fact-check, and then using AI to analyze the results.\n\n6. **Classification of Relationships**: Utilizing models trained to classify statements into relations such as entailment, contradiction, or neutral, aids in determining factual consistency by assessing the logical relationships between premises and hypotheses.\n\nBy employing these techniques, AI engineers can effectively determine and mitigate factual inconsistencies in language model outputs.",
    "context": [
      {
        "start": 16660,
        "text": ".\nOften, the hardest part of factual consistency verification is determining\nwhat the facts are. Whether any of the following statements can be\nconsidered factual depends on what sources you trust: “Messi is the best\nsoccer player in the world”, “climate change is one of the most pressing\ncrises of our time”, “breakfast is the most important meal of the day”. The\ninternet is flooded with misinformation: false marketing claims, statistics\nmade up to advance political agendas, and sensational, biased social media\nposts. In addition, it’s easy to fall for the absence of evidence fallacy. One\nmight take the statement “there’s no link between X and Y” as factually\ncorrect because of a failure to find the evidence that supported the link.\nOne interesting research question is what evidence AI models find\nconvincing, as the answer sheds light on how AI models process conflicting\ninformation and determine what the facts are. For example, Wan et al.\n(2024) found that existing “models rely heavil",
        "chapter": 4
      },
      {
        "start": 15827,
        "text": "ent with the\ndata).\nGlobal factual consistency\nThe output is evaluated against open knowledge. If the model\noutputs “the sky is blue” and it’s a commonly accepted fact that the\nsky is blue, this statement is considered factually correct. Global\nfactual consistency is important for tasks with broad scopes such as\ngeneral chatbots, fact-checking, market research, etc.\nFactual consistency is much easier to verify against explicit facts. For\nexample, the factual consistency of the statement “there has been no proven\nlink between vaccination and autism” is easier to verify if you’re provided\nwith reliable sources that explicitly state whether there is a link between\nvaccination and autism.\nIf no context is given, you’ll have to first search for reliable sources, derive\nfacts, and then validate the statement against these facts.\nOften, the hardest part of factual consistency verification is determining\nwhat the facts are. Whether any of the following statements can be\nconsidered factual depe",
        "chapter": 4
      },
      {
        "start": 18326,
        "text": "t the VMO (Vietnamese Mathematical Olympiad) than the IMO (International\nMathematical Olympiad), because the VMO is much less commonly referenced than the IMO.\n2. Queries asking for things that don’t exist. For example, if I ask the model “What did X say about\nY?” the model is more likely to hallucinate if X has never said anything about Y than if X has.\nLet’s assume for now that you already have the context to evaluate an\noutput against—this context was either provided by users or retrieved by\nyou (context retrieval is discussed in Chapter 6). The most straightforward\nevaluation approach is AI as a judge. As discussed in Chapter 3, AI judges\ncan be asked to evaluate anything, including factual consistency. Both Liu\net al. (2023) and Luo et al. (2023) showed that GPT-3.5 and GPT-4 can\noutperform previous methods at measuring factual consistency. The paper\n“TruthfulQA: Measuring How Models Mimic Human Falsehoods” (Lin et\nal., 2022) shows that their finetuned model GPT-judge is able to p",
        "chapter": 4
      },
      {
        "start": 14161,
        "text": "content that isn’t necessarily\nharmful but can cause heated debates. Some people might care about\nfriendliness, positivity, creativity, or conciseness, but I won’t be able to go\ninto them all. This section focuses on how to evaluate factual consistency\nand safety. Factual inconsistency can cause harm too, so it’s technically\nunder safety. However, due to its scope, I put it in its own section. The\ntechniques used to measure these qualities can give you a rough idea of how\nto evaluate other qualities you care about.\nFactual consistency\nDue to factual inconsistency’s potential for catastrophic consequences,\nmany techniques have been and will be developed to detect and measure it.\nIt’s impossible to cover them all in one chapter, so I’ll go over only the\nbroad strokes.\nThe factual consistency of a model’s output can be verified under two\nsettings: against explicitly provided facts (context) or against open\nknowledge:\nLocal factual consistency\nThe output is evaluated against a context. The",
        "chapter": 4
      },
      {
        "start": 19159,
        "text": "ing factual consistency. The paper\n“TruthfulQA: Measuring How Models Mimic Human Falsehoods” (Lin et\nal., 2022) shows that their finetuned model GPT-judge is able to predict\nwhether a statement is considered truthful by humans with 90–96%\naccuracy. Here’s the prompt that Liu et al. (2023) used to evaluate the\nfactual consistency of a summary with respect to the original document:\nFactual Consistency: Does the summary\nuntruthful or misleading facts that are not\n3\nsupported by the source text?\nSource Text:\n{{Document}}\nSummary:\n{{Summary}}\nDoes the summary contain factual\ninconsistency?\nAnswer:\nMore sophisticated AI as a judge techniques to evaluate factual consistency\nare self-verification and knowledge-augmented verification:\nSelf-verification\nSelfCheckGPT (Manakul et al., 2023) relies on an assumption that if\na model generates multiple outputs that disagree with one another,\nthe original output is likely hallucinated. Given a response R to\nevaluate, SelfCheckGPT generates N new respon",
        "chapter": 4
      },
      {
        "start": 34153,
        "text": " “use a respectful tone”). However, the verification of these\nexpanded instruction types can’t be easily automated. If you instruct a\nmodel to “use language appropriate to a young audience”, how do you\nautomatically verify if the output is indeed appropriate for a young\naudience?\nFor verification, INFOBench authors constructed a list of criteria for each\ninstruction, each framed as a yes/no question. For example, the output to the\ninstruction “Make a questionnaire to help hotel guests write hotel reviews”\ncan be verified using three yes/no questions:\n1. Is the generated text a questionnaire?\n2. Is the generated questionnaire designed for hotel guests?\n3. Is the generated questionnaire helpful for hotel guests to write hotel\nreviews?\nA model is considered to successfully follow an instruction if its output\nmeets all the criteria for this instruction. Each of these yes/no questions can\nbe answered by a human or AI evaluator. If the instruction has three criteria\nand the evaluator determi",
        "chapter": 4
      },
      {
        "start": 14994,
        "text": "ied under two\nsettings: against explicitly provided facts (context) or against open\nknowledge:\nLocal factual consistency\nThe output is evaluated against a context. The output is considered\nfactually consistent if it’s supported by the given context. For\nexample, if the model outputs “the sky is blue” and the given context\nsays that the sky is purple, this output is considered factually\ninconsistent. Conversely, given this context, if the model outputs “the\nsky is purple”, this output is factually consistent.\nLocal factual consistency is important for tasks with limited scopes\nsuch as summarization (the summary should be consistent with the\noriginal document), customer support chatbots (the chatbot’s\nresponses should be consistent with the company’s policies), and\nbusiness analysis (the extracted insights should be consistent with the\ndata).\nGlobal factual consistency\nThe output is evaluated against open knowledge. If the model\noutputs “the sky is blue” and it’s a commonly accepted fact",
        "chapter": 4
      },
      {
        "start": 12495,
        "text": "LG metrics, including faithfulness and relevance, have been\nrepurposed, with significant modifications, to evaluate the outputs of\nfoundation models. As generative models improved, many issues of early\nNLG systems went away, and the metrics used to track these issues became\nless important. In the 2010s, generated texts didn’t sound natural. They\nwere typically full of grammatical errors and awkward sentences. Fluency\nand coherence, then, were important metrics to track. However, as language\nmodels’ generation capabilities have improved, AI-generated texts have\nbecome nearly indistinguishable from human-generated texts. Fluency and\n2\ncoherence become less important. However, these metrics can still be\nuseful for weaker models or for applications involving creative writing and\nlow-resource languages. Fluency and coherence can be evaluated using AI\nas a judge—asking an AI model how fluent and coherent a text is—or using\nperplexity, as discussed in Chapter 3.\nGenerative models, with their ",
        "chapter": 4
      },
      {
        "start": 21658,
        "text": " the premise.\nNeutral: the premise neither entails nor contradicts the hypothesis.\nFor example, given the context “Mary likes all fruits”, here are examples of\nthese three relationships:\nEntailment: “Mary likes apples”.\nContradiction: “Mary hates oranges”.\nNeutral: “Mary likes chickens”.\nEntailment implies factual consistency, contradiction implies factual\ninconsistency, and neutral implies that consistency can’t be determined.\nInstead of using general-purpose AI judges, you can train scorers\nspecialized in factual consistency prediction. These scorers take in a pair of\n(premise, hypothesis) as input and output one of the predefined classes,\nsuch as entailment, contradiction, or neutral. This makes factual consistency\na classification task. For example, DeBERTa-v3-base-mnli-fever-\nanli is a 184-million-parameter model trained on 764,000 annotated\n(hypothesis, premise) pairs to predict entailment.\nBenchmarks for factual consistency include TruthfulQA. It comprises 817\nquestions that som",
        "chapter": 4
      },
      {
        "start": 19992,
        "text": "erates multiple outputs that disagree with one another,\nthe original output is likely hallucinated. Given a response R to\nevaluate, SelfCheckGPT generates N new responses and measures\nhow consistent R is with respect to these N new responses. This\napproach works but can be prohibitively expensive, as it requires\nmany AI queries to evaluate a response.\nKnowledge-augmented verification\nSAFE, Search-Augmented Factuality Evaluator, introduced by\nGoogle DeepMind (Wei et al., 2024) in the paper “Long-Form\nFactuality in Large Language Models”, works by leveraging search\nengine results to verify the response. It works in four steps, as\nvisualized in Figure 4-1:\n1. Use an AI model to decompose the response into individual\nstatements.\n2. Revise each statement to make it self-contained. For example, the\n“it” in the statement “It opened in the 20th century” should be\nchanged to the original subject.\n3. For each statement, propose fact-checking queries to send to a\nGoogle Search API.\n4. Use AI to d",
        "chapter": 4
      }
    ],
    "input_tokens": 2676,
    "output_tokens": 349,
    "input_cost": 0.0004014,
    "output_cost": 0.0002094,
    "total_cost": 0.0006108,
    "reference_answer": "The chapter introduces various techniques developed to detect factual inconsistency, while noting that comprehensive coverage of these methods is beyond the scope of the chapter."
  },
  {
    "question": "What are the implications of a model's data lineage in AI training?",
    "answer": "The implications of a model's data lineage in AI training are significant and multifaceted. Here are the key points:\n\n1. **Transparency and Trust**: Data lineage provides transparency regarding the sources and types of data used in training a model. This transparency is crucial for stakeholders to trust the model's outputs and understand potential biases or limitations inherent in the training data.\n\n2. **Intellectual Property Concerns**: As copyright laws evolve, there is growing uncertainty regarding the use of copyrighted materials in training datasets. Companies that depend on intellectual property for their products, such as those in gaming or film, may hesitate to use AI-generated content until clearer legal guidelines are established.\n\n3. **Training Data Scrutiny**: Companies may be compelled to opt for open-source models if their data lineage is publicly available. This availability allows the community to assess whether the data is safe to use, contributing to more ethical AI practices.\n\n4. **Data Contamination Risks**: Unintentional data contamination can occur if training and benchmark datasets overlap, leading to misleading performance evaluations. This can undermine the reliability of the model's outputs and complicate evaluation processes.\n\n5. **Privacy and Security**: There are significant privacy implications when using model APIs, as there is a risk of data being reused to train models by third-party providers. This risk underscores the need for stringent data handling policies to protect sensitive information.\n\n6. **Guidance for Model Selection**: Understanding a model's data lineage can inform decisions about whether to use commercial models or pursue open-source alternatives, balancing performance expectations against legal and ethical considerations.\n\nIn summary, data lineage in AI training affects transparency, legal implications, data security, contamination risks, and model selection strategies, making it an essential consideration for any AI engineering process.",
    "context": [
      {
        "start": 56644,
        "text": "ted in Chapter 5.\nData lineage and copyright\nData lineage and copyright concerns can steer a company in many\ndirections: toward open source models, toward proprietary models, or away\nfrom both.\nFor most models, there’s little transparency about what data a model is\ntrained on. In Gemini’s technical report, Google went into detail about the\nmodels’ performance but said nothing about the models’ training data other\nthan that “all data enrichment workers are paid at least a local living wage”.\nOpenAI’s CTO wasn’t able to provide a satisfactory answer when asked\nwhat data was used to train their models.\nOn top of that, the IP laws around AI are actively evolving. While the US\nPatent and Trademark Office (USPTO) made clear in 2024 that “AI-assisted\ninventions are not categorically unpatentable”, an AI application’s\npatentability depends on “whether the human contribution to an innovation\nis significant enough to qualify for a patent.” It’s also unclear whether, if a\nmodel was trained on cop",
        "chapter": 4
      },
      {
        "start": 55811,
        "text": "ies.\nIf you use a model API, there’s a risk that the API provider will use your\ndata to train its models. Even though most model API providers claim they\ndon’t do that, their policies can change. In August 2023, Zoom faced a\nbacklash after people found out the company had quietly changed its terms\nof service to let Zoom use users’ service-generated data, including product\nusage data and diagnostics data, to train its AI models.\nWhat’s the problem with people using your data to train their models?\nWhile research in this area is still sparse, some studies suggest that AI\nmodels can memorize their training samples. For example, it’s been found\nthat Hugging Face’s StarCoder model memorizes 8% of its training set.\nThese memorized samples can be accidentally leaked to users or\nintentionally exploited by bad actors, as demonstrated in Chapter 5.\nData lineage and copyright\nData lineage and copyright concerns can steer a company in many\ndirections: toward open source models, toward proprietary ",
        "chapter": 4
      },
      {
        "start": 57477,
        "text": "ty depends on “whether the human contribution to an innovation\nis significant enough to qualify for a patent.” It’s also unclear whether, if a\nmodel was trained on copyrighted data, and you use this model to create\nyour product, you can defend your product’s IP. Many companies whose\nexistence depends upon their IPs, such as gaming and movie studios, are\nhesitant to use AI to aid in the creation of their products, at least until IP\nlaws around AI are clarified (James Vincent, The Verge, November 15,\n2022).\nConcerns over data lineage have driven some companies toward fully open\nmodels, whose training data has been made publicly available. The\nargument is that this allows the community to inspect the data and make\nsure that it’s safe to use. While it sounds great in theory, in practice, it’s\nchallenging for any company to thoroughly inspect a dataset of the size\ntypically used to train foundation models.\nGiven the same concern, many companies opt for commercial models\ninstead. Open source",
        "chapter": 4
      },
      {
        "start": 66640,
        "text": "ld be for privacy reasons, such as when you want to give an AI assistant\naccess to all your data, but don’t want your data to leave your device.\nTable 4-4 summarizes the pros and cons of using model APIs and self-\nhosting models.\nTable 4-4. Pros and cons of using model APIs and self-hosting models (cons in italics).\nUsing model APIs Self-hosting models\nData\nHave to send your Don’t have to send your\ndata to model data externally\nproviders, which Fewer checks and\nmeans your team can balances for data\naccidentally leak lineage/training data\nconfidential info copyright\nPerformance\nBest-performing The best open source\nmodel will likely be models will likely be a bit\nclosed source behind commercial\nmodels\nFunctionality\nMore likely to No/limited support for\nsupport scaling, function calling and\nfunction calling, structured outputs\nstructured outputs Can access logprobs and\nLess likely to expose intermediate outputs,\nlogprobs which are helpful for\nclassification tasks,\nUsing model APIs Self-ho",
        "chapter": 4
      },
      {
        "start": 24157,
        "text": "erformance of several models on this benchmark, as\nshown in GPT-4’s technical report (2023). For comparison, the human\nexpert baseline, as reported in the TruthfulQA paper, is 94%.\nFactual consistency is a crucial evaluation criteria for RAG, retrieval-\naugmented generation, systems. Given a query, a RAG system retrieves\nrelevant information from external databases to supplement the model’s\ncontext. The generated response should be factually consistent with the\nretrieved context. RAG is a central topic in Chapter 6.\nFigure 4-2. The performance of different models on TruthfulQA, as shown in GPT-4’s technical\nreport.\nSafety\nOther than factual consistency, there are many ways in which a model’s\noutputs can be harmful. Different safety solutions have different ways of\ncategorizing harms—see the taxonomy defined in OpenAI’s content\nmoderation endpoint and Meta’s Llama Guard paper (Inan et al., 2023).\nChapter 5 also discusses more ways in which AI models can be unsafe and\nhow to make your sy",
        "chapter": 4
      },
      {
        "start": 82467,
        "text": "ed\nto create new benchmarks to evaluate their new models.\nData contamination can happen indirectly, such as when both evaluation\nand training data come from the same source. For example, you might\ninclude math textbooks in the training data to improve the model’s math\ncapabilities, and someone else might use questions from the same math\ntextbooks to create a benchmark to evaluate the model’s capabilities.\nData contamination can also happen intentionally for good reasons. Let’s\nsay you want to create the best possible model for your users. Initially, you\nexclude benchmark data from the model’s training data and choose the best\nmodel based on these benchmarks. However, because high-quality\nbenchmark data can improve the model’s performance, you then continue\ntraining your best model on benchmark data before releasing it to your\nusers. So the released model is contaminated, and your users won’t be able\nto evaluate it on contaminated benchmarks, but this might still be the right\nthing to d",
        "chapter": 4
      },
      {
        "start": 15827,
        "text": "ent with the\ndata).\nGlobal factual consistency\nThe output is evaluated against open knowledge. If the model\noutputs “the sky is blue” and it’s a commonly accepted fact that the\nsky is blue, this statement is considered factually correct. Global\nfactual consistency is important for tasks with broad scopes such as\ngeneral chatbots, fact-checking, market research, etc.\nFactual consistency is much easier to verify against explicit facts. For\nexample, the factual consistency of the statement “there has been no proven\nlink between vaccination and autism” is easier to verify if you’re provided\nwith reliable sources that explicitly state whether there is a link between\nvaccination and autism.\nIf no context is given, you’ll have to first search for reliable sources, derive\nfacts, and then validate the statement against these facts.\nOften, the hardest part of factual consistency verification is determining\nwhat the facts are. Whether any of the following statements can be\nconsidered factual depe",
        "chapter": 4
      },
      {
        "start": 54978,
        "text": "e are\nseven axes to consider: data privacy, data lineage, performance,\nfunctionality, costs, control, and on-device deployment.\nData privacy\nExternally hosted model APIs are out of the question for companies with\n14\nstrict data privacy policies that can’t send data outside of the organization.\nOne of the most notable early incidents was when Samsung employees put\nSamsung’s proprietary information into ChatGPT, accidentally leaking the\n15\n15\ncompany’s secrets. It’s unclear how Samsung discovered this leak and\nhow the leaked information was used against Samsung. However, the\nincident was serious enough for Samsung to ban ChatGPT in May 2023.\nSome countries have laws that forbid sending certain data outside their\nborders. If a model API provider wants to serve these use cases, they will\nhave to set up servers in these countries.\nIf you use a model API, there’s a risk that the API provider will use your\ndata to train its models. Even though most model API providers claim they\ndon’t do that",
        "chapter": 4
      },
      {
        "start": 112455,
        "text": "atGPT’s outputs. X might have a license that allows this,\nbut if ChatGPT doesn’t, then X violated ChatGPT’s terms of use, and therefore, X can’t be used. This\nis why knowing a model’s data lineage is so important.\n3\nFor example, as of this writing, you can access GPT-4 models only via OpenAI or Azure. Some\nmight argue that being able to provide services on top of OpenAI’s proprietary models is a key\nreason Microsoft invested in OpenAI.\n4\nInterestingly enough, some companies with strict data privacy requirements have told me that even\nthough they can’t usually send data to third-party services, they’re okay with sending their data to\nmodels hosted on GCP, AWS, and Azure. For these companies, the data privacy policy is more about\nwhat services they can trust. They trust big cloud providers but don’t trust other startups.\n5\nThe story was reported by several outlets, including TechRadar (see “Samsung Workers Made a\nMajor Error by Using ChatGPT”, by Lewis Maddison (April 2023).\n6\nAs regulat",
        "chapter": 4
      },
      {
        "start": 81634,
        "text": "nstrated this beautifully in\nhis 2023 satirical paper “Pretraining on the Test Set Is All You Need”. By\ntraining exclusively on data from several benchmarks, his one-million-\nparameter model was able to achieve near-perfect scores and outperformed\nmuch larger models on all these benchmarks.\nHow data contamination happens\nWhile some might intentionally train on benchmark data to achieve\nmisleadingly high scores, most data contamination is unintentional. Many\nmodels today are trained on data scraped from the internet, and the scraping\nprocess can accidentally pull data from publicly available benchmarks.\nBenchmark data published before the training of a model is likely included\n27\nin the model’s training data. It’s one of the reasons existing benchmarks\nbecome saturated so quickly, and why model developers often feel the need\nto create new benchmarks to evaluate their new models.\nData contamination can happen indirectly, such as when both evaluation\nand training data come from the same s",
        "chapter": 4
      }
    ],
    "input_tokens": 2649,
    "output_tokens": 354,
    "input_cost": 0.00039735,
    "output_cost": 0.0002124,
    "total_cost": 0.00060975,
    "reference_answer": "Understanding a model's data lineage is critical as it affects compliance with licensing terms, particularly regarding whether a model can be trained on outputs from another model without violating rules."
  },
  {
    "question": "best practices for reporting model performance",
    "answer": "Best practices for reporting model performance, as suggested by the AI Engineering book by Chip Huyen, include the following steps:\n\n1. **Define Clear Evaluation Criteria**: Start by establishing specific criteria that are relevant to your application. This might include metrics such as accuracy, F1 score, or latency. Tailor the criteria to capture the performance aspects that matter most for your use case.\n\n2. **Utilize Benchmarks Appropriately**: Gather a list of benchmarks that evaluate capabilities pertinent to your model’s intended purpose. Public benchmarks can give a sense of a model’s performance but may not fully represent your application’s needs. Create custom benchmarks to effectively assess your model.\n\n3. **Aggregate and Normalize Scores**: When comparing models across different benchmarks, ensure that scores are aggregated in a meaningful way. Since benchmarks might use different units (e.g., accuracy vs. F1 score), weigh their importance according to your application’s requirements.\n\n4. **Iterative Evaluation**: The evaluation process should be iterative. As new information becomes available from ongoing model use and feedback, be ready to revisit previous evaluations and adjust your model selection accordingly.\n\n5. **Monitor and Collect Feedback**: Deploy models in a way that allows for real-time monitoring of their performance. Collect user feedback to continuously improve model accuracy and relevance.\n\n6. **Transparent Reporting**: Be transparent about the methodologies used for evaluation, including any limitations or biases in the dataset or benchmarks, to provide context for the performance metrics reported.\n\nBy adhering to these practices, metrics will be more reliable and relevant, leading to a clearer understanding of model performance.",
    "context": [
      {
        "start": 60809,
        "text": "format.\nOutput guardrails: mitigating risks in the generated responses, such as\nmaking sure the responses aren’t racist or sexist.\nMany of these functionalities are challenging and time-consuming to\nimplement, which makes many companies turn to API providers that\nprovide the functionalities they want out of the box.\nThe downside of using a model API is that you’re restricted to the\nfunctionalities that the API provides. A functionality that many use cases\nneed is logprobs, which are very useful for classification tasks, evaluation,\nand interpretability. However, commercial model providers might be\nhesitant to expose logprobs for fear of others using logprobs to replicate\ntheir models. In fact, many model APIs don’t expose logprobs or expose\nonly limited logprobs.\nYou can also only finetune a commercial model if the model provider lets\nyou. Imagine that you’ve maxed out a model’s performance with prompting\nand want to finetune that model. If this model is proprietary and the model\nprovi",
        "chapter": 4
      },
      {
        "start": 46648,
        "text": " in production to detect failure and\ncollect feedback to improve your application.\nFigure 4-5. An overview of the evaluation workflow to evaluate models for your application.\nThese four steps are iterative—you might want to change the decision from\na previous step with newer information from the current step. For example,\nyou might initially want to host open source models. However, after public\nand private evaluation, you might realize that open source models can’t\nachieve the level of performance you want and have to switch to\ncommercial APIs.\nChapter 10 discusses monitoring and collecting user feedback. The rest of\nthis chapter will discuss the first three steps. First, let’s discuss a question\nthat most teams will visit more than once: to use model APIs or to host\nmodels themselves. We’ll then continue to how to navigate the dizzying\nnumber of public benchmarks and why you can’t trust them. This will set\nthe stage for the last section in the chapter. Because public benchmarks\ncan’t",
        "chapter": 4
      },
      {
        "start": 44149,
        "text": "see if\nsmaller models would work. If you decide to do finetuning, you might start\nwith a small model to test your code and move toward the biggest model\nthat fits your hardware constraints (e.g., one GPU).\nIn general, the selection process for each technique typically involves two\nsteps:\n1. Figuring out the best achievable performance\n2. Mapping models along the cost–performance axes and choosing the\nmodel that gives the best performance for your bucks\nHowever, the actual selection process is a lot more nuanced. Let’s explore\nwhat it looks like.\nModel Selection Workflow\nWhen looking at models, it’s important to differentiate between hard\nattributes (what is impossible or impractical for you to change) and soft\nattributes (what you can and are willing to change).\nHard attributes are often the results of decisions made by model providers\n(licenses, training data, model size) or your own policies (privacy, control).\nFor some use cases, the hard attributes can reduce the pool of potential\n",
        "chapter": 4
      },
      {
        "start": 67473,
        "text": "ts\nstructured outputs Can access logprobs and\nLess likely to expose intermediate outputs,\nlogprobs which are helpful for\nclassification tasks,\nUsing model APIs Self-hosting models\nevaluation, and\ninterpretability\nCost\nAPI cost Talent, time, engineering\neffort to optimize, host,\nmaintain (can be\nmitigated by using model\nhosting services)\nFinetuning\nCan only finetune Can finetune, quantize,\nmodels that model and optimize models (if\nproviders let you their licenses allow), but\nit can be hard to do so\nControl,\nRate limits Easier to inspect changes\naccess, and\nRisk of losing access in open source models\ntransparency\nto the model You can freeze a model\nLack of transparency to maintain its access, but\nin model changes and you’re responsible for\nversioning\nUsing model APIs Self-hosting models\nbuilding and maintaining\nmodel APIs\nEdge use cases\nCan’t run on device Can run on device, but\nwithout internet again, might be hard to\naccess do so\nThe pros and cons of each approach hopefully can help yo",
        "chapter": 4
      },
      {
        "start": 42483,
        "text": "Therefore, at different scales, companies need to reevaluate whether it\nmakes more sense to use model APIs or to host their own models.\nTable 4-3 shows criteria you might use to evaluate models for your\napplication. The row scale is especially important when evaluating model\nAPIs, because you need a model API service that can support your scale.\nTable 4-3. An example of criteria used to select models for a fictional application.\nHard\nCriteria Metric Benchmark Ideal\nrequirement\nCost Cost per X < $30.00 / < $15\noutput token 1M tokens 1M to\nScale TPM (tokens X > 1M TPM > 1M\nper minute)\nLatency Time to first Internal user < 200ms < 100\ntoken (P90) prompt dataset\nLatency Time per total Internal user < 1m < 30s\nquery (P90) prompt dataset\nOverall model Elo score Chatbot > 1200 > 125\nquality Arena’s\nranking\nCode pass@1 HumanEval > 90% > 95%\ngeneration\ncapability\nHard\nCriteria Metric Benchmark Ideal\nrequirement\nFactual Internal GPT Internal > 0.8 > 0.9\nconsistency metric hallucination\ndataset\nN",
        "chapter": 4
      },
      {
        "start": 77469,
        "text": " fraction of times a model obtains\na better score than another model, averaged across scenarios”.\nWhile public leaderboards are useful to get a sense of models’ broad\nperformance, it’s important to understand what capabilities a leaderboard is\ntrying to capture. A model that ranks high on a public leaderboard will\nlikely, but far from always, perform well for your application. If you want a\nmodel for code generation, a public leaderboard that doesn’t include a code\ngeneration benchmark might not help you as much.\nCustom leaderboards with public benchmarks\nWhen evaluating models for a specific application, you’re basically creating\na private leaderboard that ranks models based on your evaluation criteria.\nThe first step is to gather a list of benchmarks that evaluate the capabilities\nimportant to your application. If you want to build a coding agent, look at\ncode-related benchmarks. If you build a writing assistant, look into creative\nwriting benchmarks. As new benchmarks are constantly",
        "chapter": 4
      },
      {
        "start": 59976,
        "text": " might be sufficient.\nAnother reason that might cause open source models to lag behind is that\nopen source developers don’t receive feedback from users to improve their\nmodels, the way commercial models do. Once a model is open sourced,\nmodel developers have no idea how the model is being used, and how well\nthe model works in the wild.\nFunctionality\nMany functionalities are needed around a model to make it work for a use\ncase. Here are some examples of these functionalities:\nScalability: making sure the inference service can support your\napplication’s traffic while maintaining the desirable latency and cost.\nFunction calling: giving the model the ability to use external tools, which\nis essential for RAG and agentic use cases, as discussed in Chapter 6.\nStructured outputs, such as asking models to generate outputs in JSON\nformat.\nOutput guardrails: mitigating risks in the generated responses, such as\nmaking sure the responses aren’t racist or sexist.\nMany of these functionalities are ch",
        "chapter": 4
      },
      {
        "start": 71638,
        "text": "rds often end up with different\nbenchmarks, making it hard to compare and interpret their rankings. For\nexample, in late 2023, Hugging Face updated their Open LLM Leaderboard\nto use the average of six different benchmarks to rank models:\n1. ARC-C (Clark et al., 2018): Measuring the ability to solve complex,\ngrade school-level science questions.\n2. MMLU (Hendrycks et al., 2020): Measuring knowledge and reasoning\ncapabilities in 57 subjects, including elementary mathematics, US\nhistory, computer science, and law.\n3. HellaSwag (Zellers et al., 2019): Measuring the ability to predict the\ncompletion of a sentence or a scene in a story or video. The goal is to test\ncommon sense and understanding of everyday activities.\n4. TruthfulQA (Lin et al., 2021): Measuring the ability to generate\nresponses that are not only accurate but also truthful and non-misleading,\nfocusing on a model’s understanding of facts.\n5. WinoGrande (Sakaguchi et al., 2019): Measuring the ability to solve\nchallenging prono",
        "chapter": 4
      },
      {
        "start": 43316,
        "text": "> 90% > 95%\ngeneration\ncapability\nHard\nCriteria Metric Benchmark Ideal\nrequirement\nFactual Internal GPT Internal > 0.8 > 0.9\nconsistency metric hallucination\ndataset\nNow that you have your criteria, let’s move on to the next step and use them\nto select the best model for your application.\nModel Selection\nAt the end of the day, you don’t really care about which model is the best.\nYou care about which model is the best for your applications. Once you’ve\ndefined the criteria for your application, you should evaluate models against\nthese criteria.\nDuring the application development process, as you progress through\ndifferent adaptation techniques, you’ll have to do model selection over and\nover again. For example, prompt engineering might start with the strongest\nmodel overall to evaluate feasibility and then work backward to see if\nsmaller models would work. If you decide to do finetuning, you might start\nwith a small model to test your code and move toward the biggest model\nthat fits your",
        "chapter": 4
      },
      {
        "start": 79968,
        "text": "xpensive. For example, Stanford spent approximately $80,000–$100,000\n26\nto evaluate 30 models on their full HELM suite. The more models you\nwant to evaluate and the more benchmarks you want to use, the more\nexpensive it gets.\nOnce you’ve selected a set of benchmarks and obtained the scores for the\nmodels you care about on these benchmarks, you then need to aggregate\nthese scores to rank models. Not all benchmark scores are in the same unit\nor scale. One benchmark might use accuracy, another F1, and another\nBLEU score. You will need to think about how important each benchmark\nis to you and weigh their scores accordingly.\nAs you evaluate models using public benchmarks, keep in mind that the\ngoal of this process is to select a small subset of models to do more rigorous\nexperiments using your own benchmarks and metrics. This is not only\nbecause public benchmarks are unlikely to represent your application’s\nneeds perfectly, but also because they are likely contaminated. How public\nbenchmark",
        "chapter": 4
      }
    ],
    "input_tokens": 2612,
    "output_tokens": 323,
    "input_cost": 0.0003918,
    "output_cost": 0.0001938,
    "total_cost": 0.0005856,
    "reference_answer": "The chapter emphasizes the importance of disclosing the percentage of benchmark data used in training when reporting model performance, along with results on overall and clean samples of the benchmark."
  },
  {
    "question": "advanced techniques for evaluating LLMs",
    "answer": "### Advanced Techniques for Evaluating LLMs\n\nEvaluating Large Language Models (LLMs) effectively is critical for ensuring they meet application goals. Here are some advanced techniques for evaluation based on insights from the AI Engineering book by Chip Huyen:\n\n1. **Establish a Robust Evaluation Pipeline**:\n   - Consistently evaluate models using a dedicated pipeline to ensure reliable outcomes. This involves systematic processes for filtering models, running experiments, and monitoring performance metrics.\n\n2. **Diverse Evaluation Metrics**:\n   - Utilize both quantitative and qualitative metrics. While numerical scores can provide some insights, subjective evaluation methods, including AI judges for specific tasks, may capture nuanced performance aspects better.\n\n3. **Benchmark Selection and Aggregation**:\n   - Choose relevant benchmarks from established sources (e.g., EleutherAI’s lm-evaluation-harness and OpenAI's evals) that cover various capabilities. Aggregate results across benchmarks to rank models effectively, ensuring a holistic view of their performance.\n\n4. **Continuous Monitoring**:\n   - Implement continuous evaluation of models in production. This could involve real-time monitoring for performance degradation and collecting user feedback to inform iterative improvements.\n\n5. **Experiment Tracking**:\n   - Log all pertinent variables in the evaluation process, including changes in the evaluation data, rubric, prompts, and sampling configurations. This facilitates better understanding of variations in performance.\n\n6. **Iterative Refinement**:\n   - Regularly iterate on the evaluation pipeline based on previous results and feedback. This could mean changing model choices, evaluation criteria, or even reevaluating the assumptions behind your benchmarks.\n\n7. **Multi-Dimensional Evaluation**:\n   - Recognize that no single score captures the capabilities of a model. Use a combination of close-ended and open-ended evaluations to cover aspects such as factual consistency, reasoning, and domain-specific skills.\n\n8. **Consider Contextual Performance**:\n   - Evaluate models in the specific context of their intended application, as performance can significantly differ based on usage scenarios.\n\nBy integrating these advanced evaluation techniques, you can develop a more nuanced understanding of LLM performance, thereby enhancing the overall utility of deployed AI systems.",
    "context": [
      {
        "start": 104125,
        "text": "% confident that one system is better.\nValues from OpenAI.\nDifference Sample size needed for\nto detect 95% confidence\n30% ~10\n10% ~100\n3% ~1,000\n1% ~10,000\nAs a reference, among evaluation benchmarks in Eleuther’s lm-evaluation-\nharness, the median number of examples is 1,000, and the average is 2,159.\nThe organizers of the Inverse Scaling prize suggested that 300 examples is\nthe absolute minimum and they would prefer at least 1,000, especially if the\nexamples are being synthesized (McKenzie et al., 2023).\nEvaluate your evaluation pipeline\nEvaluating your evaluation pipeline can help with both improving your\npipeline’s reliability and finding ways to make your evaluation pipeline\nmore efficient. Reliability is especially important with subjective evaluation\nmethods such as AI as a judge.\nHere are some questions you should be asking about the quality of your\nevaluation pipeline:\nIs your evaluation pipeline getting you the right signals?\nDo better responses indeed get higher scores? Do b",
        "chapter": 4
      },
      {
        "start": 833,
        "text": "odel for your application. Thousands of benchmarks have been\nintroduced to evaluate these models along different criteria. Can these\nbenchmarks be trusted? How do you select what benchmarks to use? How\nabout public leaderboards that aggregate multiple benchmarks?\nThe model landscape is teeming with proprietary models and open source\nmodels. A question many teams will need to visit over and over again is\nwhether to host their own models or to use a model API. This question has\nbecome more nuanced with the introduction of model API services built on\ntop of open source models.\nThe last part discusses developing an evaluation pipeline that can guide the\ndevelopment of your application over time. This part brings together the\ntechniques we’ve learned throughout the book to evaluate concrete\napplications.\nEvaluation Criteria\nWhich is worse—an application that has never been deployed or an\napplication that is deployed but no one knows whether it’s working? When I\nasked this question at confer",
        "chapter": 4
      },
      {
        "start": 109123,
        "text": "niques and criteria\ndiscussed in the last chapter and how to create an evaluation pipeline for\nyour application. No perfect evaluation method exists. It’s impossible to\ncapture the ability of a high-dimensional system using one- or few-\ndimensional scores. Evaluating modern AI systems has many limitations\nand biases. However, this doesn’t mean we shouldn’t do it. Combining\ndifferent methods and approaches can help mitigate many of these\nchallenges.\nEven though dedicated discussions on evaluation end here, evaluation will\ncome up again and again, not just throughout the book but also throughout\nyour application development process. Chapter 6 explores evaluating\nretrieval and agentic systems, while Chapters 7 and 9 focus on calculating a\nmodel’s memory usage, latency, and costs. Data quality verification is\naddressed in Chapter 8, and using user feedback to evaluate production\napplications is addressed in Chapter 10.\nWith that, let’s move onto the actual model adaptation process, startin",
        "chapter": 4
      },
      {
        "start": 46648,
        "text": " in production to detect failure and\ncollect feedback to improve your application.\nFigure 4-5. An overview of the evaluation workflow to evaluate models for your application.\nThese four steps are iterative—you might want to change the decision from\na previous step with newer information from the current step. For example,\nyou might initially want to host open source models. However, after public\nand private evaluation, you might realize that open source models can’t\nachieve the level of performance you want and have to switch to\ncommercial APIs.\nChapter 10 discusses monitoring and collecting user feedback. The rest of\nthis chapter will discuss the first three steps. First, let’s discuss a question\nthat most teams will visit more than once: to use model APIs or to host\nmodels themselves. We’ll then continue to how to navigate the dizzying\nnumber of public benchmarks and why you can’t trust them. This will set\nthe stage for the last section in the chapter. Because public benchmarks\ncan’t",
        "chapter": 4
      },
      {
        "start": 45815,
        "text": "ase. For example, latency is a soft attribute if you have access to\nthe model to optimize it to run faster. It’s a hard attribute if you use a model\nhosted by someone else.\nAt a high level, the evaluation workflow consists of four steps (see\nFigure 4-5):\n1. Filter out models whose hard attributes don’t work for you. Your list of\nhard attributes depends heavily on your own internal policies, whether\nyou want to use commercial APIs or host your own models.\n2. Use publicly available information, e.g., benchmark performance and\nleaderboard ranking, to narrow down the most promising models to\nexperiment with, balancing different objectives such as model quality,\nlatency, and cost.\n3. Run experiments with your own evaluation pipeline to find the best\nmodel, again, balancing all your objectives.\n4. Continually monitor your model in production to detect failure and\ncollect feedback to improve your application.\nFigure 4-5. An overview of the evaluation workflow to evaluate models for your appli",
        "chapter": 4
      },
      {
        "start": 69139,
        "text": "n harness. As of this writing, EleutherAI’s lm-evaluation-harness\nsupports over 400 benchmarks. OpenAI’s evals lets you run any of the\napproximately 500 existing benchmarks and register new benchmarks to\nevaluate OpenAI models. Their benchmarks evaluate a wide range of\ncapabilities, from doing math and solving puzzles to identifying ASCII art\nthat represents words.\nBenchmark selection and aggregation\nBenchmark results help you identify promising models for your use cases.\nAggregating benchmark results to rank models gives you a leaderboard.\nThere are two questions to consider:\nWhat benchmarks to include in your leaderboard?\nHow to aggregate these benchmark results to rank models?\nGiven so many benchmarks out there, it’s impossible to look at them all, let\nalone aggregate their results to decide which model is the best. Imagine that\nyou’re considering two models, A and B, for code generation. If model A\nperforms better than model B on a coding benchmark but worse on a\ntoxicity benchmark",
        "chapter": 4
      },
      {
        "start": 106624,
        "text": "evaluation process changes constantly, you won’t be able to use the\nevaluation results to guide your application’s development.\nAs you iterate on your evaluation pipeline, make sure to do proper\nexperiment tracking: log all variables that could change in an evaluation\nprocess, including but not limited to the evaluation data, the rubric, and the\nprompt and sampling configurations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enable you to reduce risks, discover opportunities\nto improve performance, and benchmark progresses, which will all save\nyou time and headaches down the line.\nGiven an increasing number of readily available foundation models, for\nmost application developers, the challenge is no longer in developing\nmodels but in selecting the r",
        "chapter": 4
      },
      {
        "start": 0,
        "text": "Chapter 4. Evaluate AI Systems\nA model is only useful if it works for its intended purposes. You need to\nevaluate models in the context of your application. Chapter 3 discusses\ndifferent approaches to automatic evaluation. This chapter discusses how to\nuse these approaches to evaluate models for your applications.\nThis chapter contains three parts. It starts with a discussion of the criteria\nyou might use to evaluate your applications and how these criteria are\ndefined and calculated. For example, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-specific\ncapabilities like math, science, reasoning, and summarization measured?\nThe second part focuses on model selection. Given an increasing number of\nfoundation models to choose from, it can feel overwhelming to choose the\nright model for your application. Thousands of benchmarks have been\nintroduced to evaluate these models along different criteria. Can these\nbenchmarks be trusted? How do you ",
        "chapter": 4
      },
      {
        "start": 7497,
        "text": "nerated\nquery with the runtime of the ground truth SQL query.\nYou might also care about code readability. If the generated code runs but\nnobody can understand it, it will be challenging to maintain the code or\nincorporate it into a system. There’s no obvious way to evaluate code\nreadability exactly, so you might have to rely on subjective evaluation, such\nas using AI judges.\nNon-coding domain capabilities are often evaluated with close-ended tasks,\nsuch as multiple-choice questions. Close-ended outputs are easier to verify\nand reproduce. For example, if you want to evaluate a model’s ability to do\nmath, an open-ended approach is to ask the model to generate the solution\nto a given problem. A close-ended approach is to give the model several\noptions and let it pick the correct one. If the expected answer is option C\nand the model outputs option A, the model is wrong.\nThis is the approach that most public benchmarks follow. In April 2024,\n75% of the tasks in Eleuther’s lm-evaluation-harn",
        "chapter": 4
      },
      {
        "start": 68306,
        "text": "dge use cases\nCan’t run on device Can run on device, but\nwithout internet again, might be hard to\naccess do so\nThe pros and cons of each approach hopefully can help you decide whether\nto use a commercial API or to host a model yourself. This decision should\nsignificantly narrow your options. Next, you can further refine your\nselection using publicly available model performance data.\nNavigate Public Benchmarks\nThere are thousands of benchmarks designed to evaluate a model’s different\ncapabilities. Google’s BIG-bench (2022) alone has 214 benchmarks. The\nnumber of benchmarks rapidly grows to match the rapidly growing number\nof AI use cases. In addition, as AI models improve, old benchmarks\nsaturate, necessitating the introduction of new benchmarks.\nA tool that helps you evaluate a model on multiple benchmarks is an\nevaluation harness. As of this writing, EleutherAI’s lm-evaluation-harness\nsupports over 400 benchmarks. OpenAI’s evals lets you run any of the\napproximately 500 existing bench",
        "chapter": 4
      }
    ],
    "input_tokens": 2480,
    "output_tokens": 430,
    "input_cost": 0.000372,
    "output_cost": 0.000258,
    "total_cost": 0.00063,
    "reference_answer": "The text suggests using diverse evaluation criteria, including out-of-scope inputs and error-prone examples, to rigorously assess the robustness and reliability of large language models."
  },
  {
    "question": "how to evaluate AI applications",
    "answer": "To evaluate AI applications effectively, you should adopt a structured approach that encompasses several key criteria and steps as outlined in the \"AI Engineering\" book by Chip Huyen. Here's a summary of the evaluation process:\n\n1. **Define Evaluation Criteria**: Start by establishing a list of evaluation criteria specific to your application. Common buckets for these criteria include:\n   - **Domain-Specific Capability**: How well does the model understand the specific domain of the application (e.g., legal documents, medical records)?\n   - **Generation Capability**: Measure the quality of the outputs generated by the application.\n   - **Instruction-Following Capability**: Assess how accurately the model follows instructions or prompts.\n   - **Cost and Latency**: Evaluate the operational costs and the time taken to deliver results.\n\n2. **Model Selection**: Given the vast array of foundation models available, it’s essential to start with public benchmarks to filter out underperforming models. However, to find the best fit for your application, create a customized evaluation pipeline.\n\n3. **Designing the Evaluation Pipeline**: This involves:\n   - Evaluating all components of your system rather than focusing on individual parts.\n   - Combining different evaluation methods to mitigate the limitations and biases inherent in any single method.\n   - Incorporating turn-based and task-based evaluations to assess the quality and effectiveness of your application’s interactions over time.\n\n4. **Establish Usefulness Thresholds**: Determine the minimum score that your AI application must achieve for it to be deemed useful. For example, you might find that a chatbot needs a factual consistency score above a certain percentage.\n\n5. **Incorporate User Feedback**: During production, collect user feedback to gain insights into how well the application performs in real-world scenarios. This feedback should correlate with your established metrics.\n\n6. **Iterate and Track Experiments**: Continuously iterate on your evaluation pipeline and ensure you track all variables that may affect evaluation outcomes, such as the data used, evaluation metrics, and configurations.\n\n7. **Utilize Annotated Data**: Curate a set of annotated examples to evaluate the application’s performance effectively. If possible, use production data with natural labels, or involve human evaluators or other AI to assign labels.\n\nBy following these guidelines, you can build a reliable evaluation pipeline that identifies both strengths and weaknesses in your AI application, facilitating continuous improvement and supporting successful deployment.",
    "context": [
      {
        "start": 4165,
        "text": "ach makes sense from a\nbusiness perspective, focusing only on applications whose outcomes can be\nmeasured is similar to looking for the lost key under the lamppost (at night).\nIt’s easier to do, but it doesn’t mean we’ll find the key. We might be missing\nout on many potentially game-changing applications because there is no\neasy way to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can think of criteria in the\nfollowing buckets: domain-specific capability, generation capability,\ninstruction-following capability, and cost and latency.\nImagine you ask a model to summarize a legal contract. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstanding legal contracts. Generation capability metrics measure how\nco",
        "chapter": 4
      },
      {
        "start": 0,
        "text": "Chapter 4. Evaluate AI Systems\nA model is only useful if it works for its intended purposes. You need to\nevaluate models in the context of your application. Chapter 3 discusses\ndifferent approaches to automatic evaluation. This chapter discusses how to\nuse these approaches to evaluate models for your applications.\nThis chapter contains three parts. It starts with a discussion of the criteria\nyou might use to evaluate your applications and how these criteria are\ndefined and calculated. For example, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-specific\ncapabilities like math, science, reasoning, and summarization measured?\nThe second part focuses on model selection. Given an increasing number of\nfoundation models to choose from, it can feel overwhelming to choose the\nright model for your application. Thousands of benchmarks have been\nintroduced to evaluate these models along different criteria. Can these\nbenchmarks be trusted? How do you ",
        "chapter": 4
      },
      {
        "start": 86632,
        "text": "ate hold-out data.\nPublic benchmarks will help you filter out bad models, but they won’t help\nyou find the best models for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each application might consist of\nmany ",
        "chapter": 4
      },
      {
        "start": 109123,
        "text": "niques and criteria\ndiscussed in the last chapter and how to create an evaluation pipeline for\nyour application. No perfect evaluation method exists. It’s impossible to\ncapture the ability of a high-dimensional system using one- or few-\ndimensional scores. Evaluating modern AI systems has many limitations\nand biases. However, this doesn’t mean we shouldn’t do it. Combining\ndifferent methods and approaches can help mitigate many of these\nchallenges.\nEven though dedicated discussions on evaluation end here, evaluation will\ncome up again and again, not just throughout the book but also throughout\nyour application development process. Chapter 6 explores evaluating\nretrieval and agentic systems, while Chapters 7 and 9 focus on calculating a\nmodel’s memory usage, latency, and costs. Data quality verification is\naddressed in Chapter 8, and using user feedback to evaluate production\napplications is addressed in Chapter 10.\nWith that, let’s move onto the actual model adaptation process, startin",
        "chapter": 4
      },
      {
        "start": 3332,
        "text": "pe. Applications\nshould demonstrate value to be deployed. As a result, the most common\nenterprise applications in production are those with clear evaluation criteria:\nRecommender systems are common because their successes can be\n1\nevaluated by an increase in engagement or purchase-through rates.\nThe success of a fraud detection system can be measured by how much\nmoney is saved from prevented frauds.\nCoding is a common generative AI use case because, unlike other\ngeneration tasks, generated code can be evaluated using functional\ncorrectness.\nEven though foundation models are open-ended, many of their use cases\nare close-ended, such as intent classification, sentiment analysis, next-\naction prediction, etc. It’s much easier to evaluate classification tasks\nthan open-ended tasks.\nWhile the evaluation-driven development approach makes sense from a\nbusiness perspective, focusing only on applications whose outcomes can be\nmeasured is similar to looking for the lost key under the lamppost (at",
        "chapter": 4
      },
      {
        "start": 106624,
        "text": "evaluation process changes constantly, you won’t be able to use the\nevaluation results to guide your application’s development.\nAs you iterate on your evaluation pipeline, make sure to do proper\nexperiment tracking: log all variables that could change in an evaluation\nprocess, including but not limited to the evaluation data, the rubric, and the\nprompt and sampling configurations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enable you to reduce risks, discover opportunities\nto improve performance, and benchmark progresses, which will all save\nyou time and headaches down the line.\nGiven an increasing number of readily available foundation models, for\nmost application developers, the challenge is no longer in developing\nmodels but in selecting the r",
        "chapter": 4
      },
      {
        "start": 37485,
        "text": "lude RoleLLM (Wang et al., 2023) and\nCharacterEval (Tu et al., 2024). CharacterEval used human annotators and\ntrained a reward model to evaluate each roleplaying aspect on a five-point\nscale. RoleLLM evaluates a model’s ability to emulate a persona using both\ncarefully crafted similarity scores (how similar the generated outputs are to\nthe expected outputs) and AI judges.\nIf AI in your application is supposed to assume a certain role, make sure to\nevaluate whether your model stays in character. Depending on the role, you\nmight be able to create heuristics to evaluate the model’s outputs. For\nexample, if the role is someone who doesn’t talk a lot, a heuristic would be\nthe average of the model’s outputs. Other than that, the easiest automatic\nevaluation approach is AI as a judge. You should evaluate the roleplaying\nAI on both style and knowledge. For example, if a model is supposed to\ntalk like Jackie Chan, its outputs should capture Jackie Chan’s style and are\n8\ngenerated based on Jacki",
        "chapter": 4
      },
      {
        "start": 94962,
        "text": " to determine the usefulness threshold: what scores must an\napplication achieve for it to be useful? For example, you might determine\nthat your chatbot’s factual consistency score must be at least 50% for it to\nbe useful. Anything below this makes it unusable even for general customer\nrequests.\nBefore developing AI evaluation metrics, it’s crucial to first understand the\nbusiness metrics you’re targeting. Many applications focus on stickiness\nmetrics, such as daily, weekly, or monthly active users (DAU, WAU,\nMAU). Others prioritize engagement metrics, like the number of\nconversations a user initiates per month or the duration of each visit—the\nlonger a user stays on the app, the less likely they are to leave. Choosing\nwhich metrics to prioritize can feel like balancing profits with social\nresponsibility. While an emphasis on stickiness and engagement metrics can\nlead to higher revenues, it may also cause a product to prioritize addictive\nfeatures or extreme content, which can be detrim",
        "chapter": 4
      },
      {
        "start": 98294,
        "text": "ir AI\nsystems.\nConsider evaluation methods to be used not just during experimentation but\nalso during production. During experimentation, you might have reference\ndata to compare your application’s outputs to, whereas, in production,\nreference data might not be immediately available. However, in production,\nyou have actual users. Think about what kinds of feedback you want from\nusers, how user feedback correlates to other evaluation metrics, and how to\nuse user feedback to improve your application. How to collect user\nfeedback is discussed in Chapter 10.\nAnnotate evaluation data\nCurate a set of annotated examples to evaluate your application. You need\nannotated data to evaluate each of your system’s components and each\ncriterion, for both turn-based and task-based evaluation. Use actual\nproduction data if possible. If your application has natural labels that you\ncan use, that’s great. If not, you can use either humans or AI to label your\ndata. Chapter 8 discusses AI-generated data. The",
        "chapter": 4
      },
      {
        "start": 88298,
        "text": "to-text step can be\nevaluated using similarity between the extracted text and the ground truth\ntext. The second step can be evaluated using accuracy: given the correctly\nextracted text, how often does the application correctly extract the current\nemployer?\nIf applicable, evaluate your application both per turn and per task. A turn\ncan consist of multiple steps and messages. If a system takes multiple steps\nto generate an output, it’s still considered a turn.\nGenerative AI applications, especially chatbot-like applications, allow back-\nand-forth between the user and the application, as in a conversation, to\naccomplish a task. Imagine you want to use an AI model to debug why your\nPython code is failing. The model responds by asking for more information\nabout your hardware or the Python version you’re using. Only after you’ve\nprovided this information can the model help you debug.\nTurn-based evaluation evaluates the quality of each output. Task-based\nevaluation evaluates whether a system ",
        "chapter": 4
      }
    ],
    "input_tokens": 2431,
    "output_tokens": 486,
    "input_cost": 0.00036465,
    "output_cost": 0.0002916,
    "total_cost": 0.00065625,
    "reference_answer": "The chapter highlights the importance of establishing specific evaluation criteria for AI applications, focusing on domain-specific capability, generation capability, instruction-following capability, and cost and latency to guide effective evaluations."
  },
  {
    "question": "mean win rate in model evaluation",
    "answer": "The mean win rate in model evaluation is defined by the HELM authors as \"the fraction of times a model obtains a better score than another model, averaged across scenarios.\" This method contrasts with more traditional aggregation methods like simple averaging, which treats all benchmark scores equally regardless of the difficulty of each benchmark. The mean win rate provides a more nuanced approach to ranking models, reflecting their relative performance more accurately across varied evaluation scenarios.",
    "context": [
      {
        "start": 76636,
        "text": "550\nTruthfulQA 0.4809 0.4228 0.5507 1.000\nWinoGrande 0.8856 0.4842 0.9011 0.455\nGSM-8K 0.7438 0.3547 0.7936 0.500\nThe results from all the selected benchmarks need to be aggregated to rank\nmodels. As of this writing, Hugging Face averages a model’s scores on all\nthese benchmarks to get the final score to rank that model. Averaging means\ntreating all benchmark scores equally, i.e., treating an 80% score on\nTruthfulQA the same as an 80% score on GSM-8K, even if an 80% score\non TruthfulQA might be much harder to achieve than an 80% score on\nGSM-8K. This also means giving all benchmarks the same weight, even if,\nfor some tasks, truthfulness might weigh a lot more than being able to solve\ngrade school math problems.\nHELM authors, on the other hand, decided to shun averaging in favor of\nmean win rate, which they defined as “the fraction of times a model obtains\na better score than another model, averaged across scenarios”.\nWhile public leaderboards are useful to get a sense of models’ broad\n",
        "chapter": 4
      },
      {
        "start": 104125,
        "text": "% confident that one system is better.\nValues from OpenAI.\nDifference Sample size needed for\nto detect 95% confidence\n30% ~10\n10% ~100\n3% ~1,000\n1% ~10,000\nAs a reference, among evaluation benchmarks in Eleuther’s lm-evaluation-\nharness, the median number of examples is 1,000, and the average is 2,159.\nThe organizers of the Inverse Scaling prize suggested that 300 examples is\nthe absolute minimum and they would prefer at least 1,000, especially if the\nexamples are being synthesized (McKenzie et al., 2023).\nEvaluate your evaluation pipeline\nEvaluating your evaluation pipeline can help with both improving your\npipeline’s reliability and finding ways to make your evaluation pipeline\nmore efficient. Reliability is especially important with subjective evaluation\nmethods such as AI as a judge.\nHere are some questions you should be asking about the quality of your\nevaluation pipeline:\nIs your evaluation pipeline getting you the right signals?\nDo better responses indeed get higher scores? Do b",
        "chapter": 4
      },
      {
        "start": 103292,
        "text": "hich model, prompt, or\nother component is better. Say a new prompt achieves a 10% higher score\nthan the old prompt—how big does the evaluation set have to be for us to\nbe certain that the new prompt is indeed better? In theory, a statistical\nsignificance test can be used to compute the sample size needed for a\ncertain level of confidence (e.g., 95% confidence) if you know the score\ndistribution. However, in reality, it’s hard to know the true score\ndistribution.\nTIP\nOpenAI suggested a rough estimation of the number of evaluation samples needed to be certain that\none system is better, given a score difference, as shown in Table 4-7. A useful rule is that for every 3×\n28\ndecrease in score difference, the number of samples needed increases 10×.\nTable 4-7. A rough estimation of the number of evaluation\nsamples needed to be 95% confident that one system is better.\nValues from OpenAI.\nDifference Sample size needed for\nto detect 95% confidence\n30% ~10\n10% ~100\n3% ~1,000\n1% ~10,000\nAs a refere",
        "chapter": 4
      },
      {
        "start": 79135,
        "text": "3.5 and GPT-4 from March 2023 to\nJune 2023 on certain benchmarks (Chen et al., 2023).\nAssuming that OpenAI doesn’t intentionally release worse models, what\nmight be the reason for this perception? One potential reason is that\nevaluation is hard, and no one, not even OpenAI, knows for sure if a model\nis getting better or worse. While evaluation is definitely hard, I doubt that\n24\nOpenAI would fly completely blind. If the second reason is true, it\nreinforces the idea that the best model overall might not be the best model\nfor your application.\nNot all models have publicly available scores on all benchmarks. If the\nmodel you care about doesn’t have a publicly available score on your\n25\nbenchmark, you will need to run the evaluation yourself. Hopefully, an\nevaluation harness can help you with that. Running benchmarks can be\nexpensive. For example, Stanford spent approximately $80,000–$100,000\n26\nto evaluate 30 models on their full HELM suite. The more models you\nwant to evaluate and the mo",
        "chapter": 4
      },
      {
        "start": 9996,
        "text": "nd evaluate\nagainst the random baseline. If each question has four options and only one\ncorrect option, the random baseline accuracy would be 25%. Scores above\n25% typically, though not always, mean that the model is doing better than\nrandom.\nA drawback of using MCQs is that a model’s performance on MCQs can\nvary with small changes in how the questions and the options are presented.\nAlzahrani et al. (2024) found that the introduction of an extra space\nbetween the question and answer or an addition of an additional\ninstructional phrase, such as “Choices:” can cause the model to change its\nanswers. Models’ sensitivity to prompts and prompt engineering best\npractices are discussed in Chapter 5.\nDespite the prevalence of close-ended benchmarks, it’s unclear if they are a\ngood way to evaluate foundation models. MCQs test the ability to\ndifferentiate good responses from bad responses (classification), which is\ndifferent from the ability to generate good responses. MCQs are best suited\nfor ev",
        "chapter": 4
      },
      {
        "start": 833,
        "text": "odel for your application. Thousands of benchmarks have been\nintroduced to evaluate these models along different criteria. Can these\nbenchmarks be trusted? How do you select what benchmarks to use? How\nabout public leaderboards that aggregate multiple benchmarks?\nThe model landscape is teeming with proprietary models and open source\nmodels. A question many teams will need to visit over and over again is\nwhether to host their own models or to use a model API. This question has\nbecome more nuanced with the introduction of model API services built on\ntop of open source models.\nThe last part discusses developing an evaluation pipeline that can guide the\ndevelopment of your application over time. This part brings together the\ntechniques we’ve learned throughout the book to evaluate concrete\napplications.\nEvaluation Criteria\nWhich is worse—an application that has never been deployed or an\napplication that is deployed but no one knows whether it’s working? When I\nasked this question at confer",
        "chapter": 4
      },
      {
        "start": 46648,
        "text": " in production to detect failure and\ncollect feedback to improve your application.\nFigure 4-5. An overview of the evaluation workflow to evaluate models for your application.\nThese four steps are iterative—you might want to change the decision from\na previous step with newer information from the current step. For example,\nyou might initially want to host open source models. However, after public\nand private evaluation, you might realize that open source models can’t\nachieve the level of performance you want and have to switch to\ncommercial APIs.\nChapter 10 discusses monitoring and collecting user feedback. The rest of\nthis chapter will discuss the first three steps. First, let’s discuss a question\nthat most teams will visit more than once: to use model APIs or to host\nmodels themselves. We’ll then continue to how to navigate the dizzying\nnumber of public benchmarks and why you can’t trust them. This will set\nthe stage for the last section in the chapter. Because public benchmarks\ncan’t",
        "chapter": 4
      },
      {
        "start": 69139,
        "text": "n harness. As of this writing, EleutherAI’s lm-evaluation-harness\nsupports over 400 benchmarks. OpenAI’s evals lets you run any of the\napproximately 500 existing benchmarks and register new benchmarks to\nevaluate OpenAI models. Their benchmarks evaluate a wide range of\ncapabilities, from doing math and solving puzzles to identifying ASCII art\nthat represents words.\nBenchmark selection and aggregation\nBenchmark results help you identify promising models for your use cases.\nAggregating benchmark results to rank models gives you a leaderboard.\nThere are two questions to consider:\nWhat benchmarks to include in your leaderboard?\nHow to aggregate these benchmark results to rank models?\nGiven so many benchmarks out there, it’s impossible to look at them all, let\nalone aggregate their results to decide which model is the best. Imagine that\nyou’re considering two models, A and B, for code generation. If model A\nperforms better than model B on a coding benchmark but worse on a\ntoxicity benchmark",
        "chapter": 4
      },
      {
        "start": 79968,
        "text": "xpensive. For example, Stanford spent approximately $80,000–$100,000\n26\nto evaluate 30 models on their full HELM suite. The more models you\nwant to evaluate and the more benchmarks you want to use, the more\nexpensive it gets.\nOnce you’ve selected a set of benchmarks and obtained the scores for the\nmodels you care about on these benchmarks, you then need to aggregate\nthese scores to rank models. Not all benchmark scores are in the same unit\nor scale. One benchmark might use accuracy, another F1, and another\nBLEU score. You will need to think about how important each benchmark\nis to you and weigh their scores accordingly.\nAs you evaluate models using public benchmarks, keep in mind that the\ngoal of this process is to select a small subset of models to do more rigorous\nexperiments using your own benchmarks and metrics. This is not only\nbecause public benchmarks are unlikely to represent your application’s\nneeds perfectly, but also because they are likely contaminated. How public\nbenchmark",
        "chapter": 4
      },
      {
        "start": 102459,
        "text": "iple bootstraps\nof these 100 examples and see if they give similar evaluation results.\nBasically, you want to know that if you evaluate the model on a different\nevaluation set of 100 examples, would you get a different result? If you get\n90% on one bootstrap but 70% on another bootstrap, your evaluation\npipeline isn’t that trustworthy.\nConcretely, here’s how each bootstrap works:\n1. Draw 100 samples, with replacement, from the original 100 evaluation\nexamples.\n2. Evaluate your model on these 100 bootstrapped samples and obtain the\nevaluation results.\nRepeat for a number of times. If the evaluation results vary wildly for\ndifferent bootstraps, this means that you’ll need a bigger evaluation set.\nEvaluation results are used not just to evaluate a system in isolation but also\nto compare systems. They should help you decide which model, prompt, or\nother component is better. Say a new prompt achieves a 10% higher score\nthan the old prompt—how big does the evaluation set have to be for us to",
        "chapter": 4
      }
    ],
    "input_tokens": 2677,
    "output_tokens": 85,
    "input_cost": 0.00040155,
    "output_cost": 5.1e-05,
    "total_cost": 0.00045255,
    "reference_answer": "The chapter introduces the concept of mean win rate as an alternative to average scoring, focusing on a model's performance relative to others across different scenarios, which can offer a clearer assessment of capability."
  },
  {
    "question": "costs of hosting AI models",
    "answer": "The costs of hosting AI models can vary significantly based on multiple factors, primarily centered around whether you choose to host your own models or use model APIs. Here are key considerations:\n\n1. **Compute Costs**: If you opt to host your own models, the main expense will be compute resources. The cost largely depends on the type of hardware (e.g., GPUs), which generally come in various memory configurations (e.g., 16 GB, 24 GB, etc.). Larger models that utilize the maximum capability of your hardware tend to be more common.\n\n2. **Scaling Benefits**: When you host your models, the cost per token can decrease as you scale, since the compute cost remains stable regardless of the number of tokens processed (up to your hardware's limit). This contrasts with model APIs, where costs typically scale with the number of tokens used.\n\n3. **Engineering and Maintenance Costs**: Hosting your models requires considerable engineering effort. You need to optimize, scale, and maintain the inference services, which can incur high costs in terms of both time and expertise.\n\n4. **API Costs**: While APIs can simplify operations by handling the backend infrastructure, they are often more expensive per token used, and the cost doesn't significantly decrease with scaling.\n\n5. **Operational Considerations**: Companies must also factor in the reliability and service-level agreements (SLAs) of API providers. Dependency on external APIs can necessitate additional engineering efforts to manage guardrails and data privacy.\n\n6. **Overall Cost Evaluation**: The decision to self-host versus using APIs involves evaluating costs against other criteria such as performance, functionality, and control. This choice can evolve over time as business needs change.\n\nUltimately, the cost effectiveness of hosting AI models versus using APIs is a case-specific consideration that often requires reevaluation as scaling needs and technological landscapes evolve.",
    "context": [
      {
        "start": 41650,
        "text": "n count to manage cost.\nIf you host your own models, your cost, outside engineering cost, is\ncompute. To make the most out of the machines they have, many people\nchoose the largest models that can fit their machines. For example, GPUs\nusually come with 16 GB, 24 GB, 48 GB, and 80 GB of memory.\nTherefore, many popular models are those that max out these memory\nconfigurations. It’s not a coincidence that many models today have 7 billion\nor 65 billion parameters.\nIf you use model APIs, your cost per token usually doesn’t change much as\nyou scale. However, if you host your own models, your cost per token can\nget much cheaper as you scale. If you’ve already invested in a cluster that\ncan serve a maximum of 1 billion tokens a day, the compute cost remains\n9\nthe same whether you serve 1 million tokens or 1 billion tokens a day.\nTherefore, at different scales, companies need to reevaluate whether it\nmakes more sense to use model APIs or to host their own models.\nTable 4-3 shows criteria you mi",
        "chapter": 4
      },
      {
        "start": 62475,
        "text": "er, hosting a model yourself requires nontrivial time, talent, and\nengineering effort. You’ll need to optimize the model, scale and maintain\nthe inference service as needed, and provide guardrails around your model.\nAPIs are expensive, but engineering can be even more so.\nOn the other hand, using another API means that you’ll have to depend on\ntheir SLA, service-level agreement. If these APIs aren’t reliable, which is\noften the case with early startups, you’ll have to spend your engineering\neffort on guardrails around that.\nIn general, you want a model that is easy to use and manipulate. Typically,\nproprietary models are easier to get started with and scale, but open models\nmight be easier to manipulate as their components are more accessible.\nRegardless of whether you go with open or proprietary models, you want\nthis model to follow a standard API, which makes it easier to swap models.\nMany model developers try to make their models mimic the API of the most\npopular models. As of this ",
        "chapter": 4
      },
      {
        "start": 54145,
        "text": "antages. This means API providers might be more motivated to provide\nbetter APIs with better pricing.\nSince building scalable inference services for larger models is nontrivial,\nmany companies don’t want to build them themselves. This has led to the\ncreation of many third-party inference and finetuning services on top of\nopen source models. Major cloud providers like AWS, Azure, and GCP all\nprovide API access to popular open source models. A plethora of startups\nare doing the same.\nNOTE\nThere are also commercial API providers that can deploy their services within your private networks.\nIn this discussion, I treat these privately deployed commercial APIs similarly to self-hosted models.\nThe answer to whether to host a model yourself or use a model API depends\non the use case. And the same use case can change over time. Here are\nseven axes to consider: data privacy, data lineage, performance,\nfunctionality, costs, control, and on-device deployment.\nData privacy\nExternally hosted model AP",
        "chapter": 4
      },
      {
        "start": 116620,
        "text": "s and 19,500 GPU hours\nfor open models. If an hour of GPU costs between $2.15 and $3.18, the total cost comes out to\n$80,000–$100,000.\n7\nA friend quipped: “A benchmark stops being useful as soon as it becomes public.”\n8\nThis is because the square root of 10 is approximately 3.3.",
        "chapter": 4
      },
      {
        "start": 113288,
        "text": "The story was reported by several outlets, including TechRadar (see “Samsung Workers Made a\nMajor Error by Using ChatGPT”, by Lewis Maddison (April 2023).\n6\nAs regulations are evolving around the world, requirements for auditable information of models and\ntraining data may increase. Commercial models may be able to provide certifications, saving\ncompanies from the effort.\n7\nUsers want models to be open source because open means more information and more options, but\nwhat’s in it for model developers? Many companies have sprung up to capitalize on open source\nmodels by providing inference and finetuning services. It’s not a bad thing. Many people need these\nservices to leverage open source models. But, from model developers’ perspective, why invest\nmillions, if not billions, into building models just for others to make money?It might be argued that\nMeta supports open source models only to keep their competitors (Google, Microsoft/OpenAI) in\ncheck. Both Mistral and Cohere have open sourc",
        "chapter": 4
      },
      {
        "start": 66640,
        "text": "ld be for privacy reasons, such as when you want to give an AI assistant\naccess to all your data, but don’t want your data to leave your device.\nTable 4-4 summarizes the pros and cons of using model APIs and self-\nhosting models.\nTable 4-4. Pros and cons of using model APIs and self-hosting models (cons in italics).\nUsing model APIs Self-hosting models\nData\nHave to send your Don’t have to send your\ndata to model data externally\nproviders, which Fewer checks and\nmeans your team can balances for data\naccidentally leak lineage/training data\nconfidential info copyright\nPerformance\nBest-performing The best open source\nmodel will likely be models will likely be a bit\nclosed source behind commercial\nmodels\nFunctionality\nMore likely to No/limited support for\nsupport scaling, function calling and\nfunction calling, structured outputs\nstructured outputs Can access logprobs and\nLess likely to expose intermediate outputs,\nlogprobs which are helpful for\nclassification tasks,\nUsing model APIs Self-ho",
        "chapter": 4
      },
      {
        "start": 40817,
        "text": "iables. Autoregressive language models typically generate\noutputs token by token. The more tokens it has to generate, the higher the\ntotal latency. You can control the total latency observed by users by careful\nprompting, such as instructing the model to be concise, setting a stopping\ncondition for generation (discussed in Chapter 2), or other optimization\ntechniques (discussed in Chapter 9).\nTIP\nWhen evaluating models based on latency, it’s important to differentiate between the must-have and\nthe nice-to-have. If you ask users if they want lower latency, nobody will ever say no. But high\nlatency is often an annoyance, not a deal breaker.\nIf you use model APIs, they typically charge by tokens. The more input and\noutput tokens you use, the more expensive it is. Many applications then try\nto reduce the input and output token count to manage cost.\nIf you host your own models, your cost, outside engineering cost, is\ncompute. To make the most out of the machines they have, many people\nchoos",
        "chapter": 4
      },
      {
        "start": 57477,
        "text": "ty depends on “whether the human contribution to an innovation\nis significant enough to qualify for a patent.” It’s also unclear whether, if a\nmodel was trained on copyrighted data, and you use this model to create\nyour product, you can defend your product’s IP. Many companies whose\nexistence depends upon their IPs, such as gaming and movie studios, are\nhesitant to use AI to aid in the creation of their products, at least until IP\nlaws around AI are clarified (James Vincent, The Verge, November 15,\n2022).\nConcerns over data lineage have driven some companies toward fully open\nmodels, whose training data has been made publicly available. The\nargument is that this allows the community to inspect the data and make\nsure that it’s safe to use. While it sounds great in theory, in practice, it’s\nchallenging for any company to thoroughly inspect a dataset of the size\ntypically used to train foundation models.\nGiven the same concern, many companies opt for commercial models\ninstead. Open source",
        "chapter": 4
      },
      {
        "start": 42483,
        "text": "Therefore, at different scales, companies need to reevaluate whether it\nmakes more sense to use model APIs or to host their own models.\nTable 4-3 shows criteria you might use to evaluate models for your\napplication. The row scale is especially important when evaluating model\nAPIs, because you need a model API service that can support your scale.\nTable 4-3. An example of criteria used to select models for a fictional application.\nHard\nCriteria Metric Benchmark Ideal\nrequirement\nCost Cost per X < $30.00 / < $15\noutput token 1M tokens 1M to\nScale TPM (tokens X > 1M TPM > 1M\nper minute)\nLatency Time to first Internal user < 200ms < 100\ntoken (P90) prompt dataset\nLatency Time per total Internal user < 1m < 30s\nquery (P90) prompt dataset\nOverall model Elo score Chatbot > 1200 > 125\nquality Arena’s\nranking\nCode pass@1 HumanEval > 90% > 95%\ngeneration\ncapability\nHard\nCriteria Metric Benchmark Ideal\nrequirement\nFactual Internal GPT Internal > 0.8 > 0.9\nconsistency metric hallucination\ndataset\nN",
        "chapter": 4
      },
      {
        "start": 64141,
        "text": "ou would\nwant some control over it, and API providers might not always give you the\nlevel of control you want. When using a service provided by someone else,\nyou’re subject to their terms and conditions, and their rate limits. You can\naccess only what’s made available to you by this provider, and thus might\nnot be able to tweak the model as needed.\nTo protect their users and themselves from potential lawsuits, model\nproviders use safety guardrails such as blocking requests to tell racist jokes\nor generate photos of real people. Proprietary models are more likely to err\non the side of over-censoring. These safety guardrails are good for the vast\nmajority of use cases but can be a limiting factor for certain use cases. For\nexample, if your application requires generating real faces (e.g., to aid in\nthe production of a music video) a model that refuses to generate real faces\nwon’t work. A company I advise, Convai, builds 3D AI characters that can\ninteract in 3D environments, including pic",
        "chapter": 4
      }
    ],
    "input_tokens": 2493,
    "output_tokens": 369,
    "input_cost": 0.00037395,
    "output_cost": 0.0002214,
    "total_cost": 0.00059535,
    "reference_answer": "The chapter outlines the costs associated with self-hosting models, highlighting the time, talent, and engineering effort required, which can be alleviated by using model hosting services."
  },
  {
    "question": "best practices for evaluating AI models",
    "answer": "To effectively evaluate AI models, the following best practices are essential, as outlined in Chapter 4 of the \"AI Engineering\" book by Chip Huyen:\n\n1. **Define Clear Evaluation Criteria**: Establish criteria specific to your application that encompasses domain-specific capabilities, generation abilities, instruction-following, as well as performance metrics like cost and latency.\n\n2. **Use a Systematic Evaluation Pipeline**:\n   - Start by filtering out models based on hard attributes relevant to your requirements.\n   - Utilize public benchmarks to narrow down promising models.\n   - Run your custom evaluation experiments to determine the best fit, balancing various objectives such as quality and cost.\n   - Continuously monitor models in production to detect failures and gather user feedback for improvements.\n\n3. **Comprehensive Evaluation of All Components**: Assess every part of the AI system holistically, since real-world applications are often multifaceted.\n\n4. **Incorporate Multiple Evaluation Methods**: Acknowledge that no single evaluation method suffices. Combining automatic and human evaluations can mitigate limitations and biases inherent in AI assessments.\n\n5. **Iterate on Evaluation Processes**: Regularly revisit and refine your evaluation pipeline. Track changes in variables and conditions throughout the evaluation process to ensure consistency and reliability.\n\n6. **Engage in Human Evaluation**: While automatic metrics are valuable, utilize human evaluations for nuanced assessments especially in open-ended tasks, ensuring your application maintains high performance standards.\n\nFollowing these best practices will provide a structured approach to evaluating AI models, improve reliability in assessments, and facilitate the successful deployment of AI applications.",
    "context": [
      {
        "start": 0,
        "text": "Chapter 4. Evaluate AI Systems\nA model is only useful if it works for its intended purposes. You need to\nevaluate models in the context of your application. Chapter 3 discusses\ndifferent approaches to automatic evaluation. This chapter discusses how to\nuse these approaches to evaluate models for your applications.\nThis chapter contains three parts. It starts with a discussion of the criteria\nyou might use to evaluate your applications and how these criteria are\ndefined and calculated. For example, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-specific\ncapabilities like math, science, reasoning, and summarization measured?\nThe second part focuses on model selection. Given an increasing number of\nfoundation models to choose from, it can feel overwhelming to choose the\nright model for your application. Thousands of benchmarks have been\nintroduced to evaluate these models along different criteria. Can these\nbenchmarks be trusted? How do you ",
        "chapter": 4
      },
      {
        "start": 86632,
        "text": "ate hold-out data.\nPublic benchmarks will help you filter out bad models, but they won’t help\nyou find the best models for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each application might consist of\nmany ",
        "chapter": 4
      },
      {
        "start": 109123,
        "text": "niques and criteria\ndiscussed in the last chapter and how to create an evaluation pipeline for\nyour application. No perfect evaluation method exists. It’s impossible to\ncapture the ability of a high-dimensional system using one- or few-\ndimensional scores. Evaluating modern AI systems has many limitations\nand biases. However, this doesn’t mean we shouldn’t do it. Combining\ndifferent methods and approaches can help mitigate many of these\nchallenges.\nEven though dedicated discussions on evaluation end here, evaluation will\ncome up again and again, not just throughout the book but also throughout\nyour application development process. Chapter 6 explores evaluating\nretrieval and agentic systems, while Chapters 7 and 9 focus on calculating a\nmodel’s memory usage, latency, and costs. Data quality verification is\naddressed in Chapter 8, and using user feedback to evaluate production\napplications is addressed in Chapter 10.\nWith that, let’s move onto the actual model adaptation process, startin",
        "chapter": 4
      },
      {
        "start": 46648,
        "text": " in production to detect failure and\ncollect feedback to improve your application.\nFigure 4-5. An overview of the evaluation workflow to evaluate models for your application.\nThese four steps are iterative—you might want to change the decision from\na previous step with newer information from the current step. For example,\nyou might initially want to host open source models. However, after public\nand private evaluation, you might realize that open source models can’t\nachieve the level of performance you want and have to switch to\ncommercial APIs.\nChapter 10 discusses monitoring and collecting user feedback. The rest of\nthis chapter will discuss the first three steps. First, let’s discuss a question\nthat most teams will visit more than once: to use model APIs or to host\nmodels themselves. We’ll then continue to how to navigate the dizzying\nnumber of public benchmarks and why you can’t trust them. This will set\nthe stage for the last section in the chapter. Because public benchmarks\ncan’t",
        "chapter": 4
      },
      {
        "start": 45815,
        "text": "ase. For example, latency is a soft attribute if you have access to\nthe model to optimize it to run faster. It’s a hard attribute if you use a model\nhosted by someone else.\nAt a high level, the evaluation workflow consists of four steps (see\nFigure 4-5):\n1. Filter out models whose hard attributes don’t work for you. Your list of\nhard attributes depends heavily on your own internal policies, whether\nyou want to use commercial APIs or host your own models.\n2. Use publicly available information, e.g., benchmark performance and\nleaderboard ranking, to narrow down the most promising models to\nexperiment with, balancing different objectives such as model quality,\nlatency, and cost.\n3. Run experiments with your own evaluation pipeline to find the best\nmodel, again, balancing all your objectives.\n4. Continually monitor your model in production to detect failure and\ncollect feedback to improve your application.\nFigure 4-5. An overview of the evaluation workflow to evaluate models for your appli",
        "chapter": 4
      },
      {
        "start": 106624,
        "text": "evaluation process changes constantly, you won’t be able to use the\nevaluation results to guide your application’s development.\nAs you iterate on your evaluation pipeline, make sure to do proper\nexperiment tracking: log all variables that could change in an evaluation\nprocess, including but not limited to the evaluation data, the rubric, and the\nprompt and sampling configurations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enable you to reduce risks, discover opportunities\nto improve performance, and benchmark progresses, which will all save\nyou time and headaches down the line.\nGiven an increasing number of readily available foundation models, for\nmost application developers, the challenge is no longer in developing\nmodels but in selecting the r",
        "chapter": 4
      },
      {
        "start": 4165,
        "text": "ach makes sense from a\nbusiness perspective, focusing only on applications whose outcomes can be\nmeasured is similar to looking for the lost key under the lamppost (at night).\nIt’s easier to do, but it doesn’t mean we’ll find the key. We might be missing\nout on many potentially game-changing applications because there is no\neasy way to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can think of criteria in the\nfollowing buckets: domain-specific capability, generation capability,\ninstruction-following capability, and cost and latency.\nImagine you ask a model to summarize a legal contract. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstanding legal contracts. Generation capability metrics measure how\nco",
        "chapter": 4
      },
      {
        "start": 833,
        "text": "odel for your application. Thousands of benchmarks have been\nintroduced to evaluate these models along different criteria. Can these\nbenchmarks be trusted? How do you select what benchmarks to use? How\nabout public leaderboards that aggregate multiple benchmarks?\nThe model landscape is teeming with proprietary models and open source\nmodels. A question many teams will need to visit over and over again is\nwhether to host their own models or to use a model API. This question has\nbecome more nuanced with the introduction of model API services built on\ntop of open source models.\nThe last part discusses developing an evaluation pipeline that can guide the\ndevelopment of your application over time. This part brings together the\ntechniques we’ve learned throughout the book to evaluate concrete\napplications.\nEvaluation Criteria\nWhich is worse—an application that has never been deployed or an\napplication that is deployed but no one knows whether it’s working? When I\nasked this question at confer",
        "chapter": 4
      },
      {
        "start": 37485,
        "text": "lude RoleLLM (Wang et al., 2023) and\nCharacterEval (Tu et al., 2024). CharacterEval used human annotators and\ntrained a reward model to evaluate each roleplaying aspect on a five-point\nscale. RoleLLM evaluates a model’s ability to emulate a persona using both\ncarefully crafted similarity scores (how similar the generated outputs are to\nthe expected outputs) and AI judges.\nIf AI in your application is supposed to assume a certain role, make sure to\nevaluate whether your model stays in character. Depending on the role, you\nmight be able to create heuristics to evaluate the model’s outputs. For\nexample, if the role is someone who doesn’t talk a lot, a heuristic would be\nthe average of the model’s outputs. Other than that, the easiest automatic\nevaluation approach is AI as a judge. You should evaluate the roleplaying\nAI on both style and knowledge. For example, if a model is supposed to\ntalk like Jackie Chan, its outputs should capture Jackie Chan’s style and are\n8\ngenerated based on Jacki",
        "chapter": 4
      },
      {
        "start": 97461,
        "text": "ut this prediction. Logprobs can also be\nused to evaluate a model’s perplexity for a generated text, which can be\nused for measurements such as fluency and factual consistency.\nUse automatic metrics as much as possible, but don’t be afraid to fall back\non human evaluation, even in production. Having human experts manually\nevaluate a model’s quality is a long-standing practice in AI. Given the\nchallenges of evaluating open-ended responses, many teams are looking at\nhuman evaluation as the North Star metric to guide their application\ndevelopment. Each day, you can use human experts to evaluate a subset of\nyour application’s outputs that day to detect any changes in the application’s\nperformance or unusual patterns in usage. For example, LinkedIn developed\na process to manually evaluate up to 500 daily conservations with their AI\nsystems.\nConsider evaluation methods to be used not just during experimentation but\nalso during production. During experimentation, you might have reference\ndata",
        "chapter": 4
      }
    ],
    "input_tokens": 2452,
    "output_tokens": 314,
    "input_cost": 0.0003678,
    "output_cost": 0.0001884,
    "total_cost": 0.0005562,
    "reference_answer": "The chapter outlines an iterative evaluation workflow that helps teams determine whether to use model APIs or self-hosted models based on performance needs."
  },
  {
    "question": "what are essential functionalities for AI models?",
    "answer": "Essential functionalities for AI models include:\n\n1. **Scalability**: The ability to handle application traffic while maintaining acceptable latency and cost.\n   \n2. **Function Calling**: Integration with external tools, essential for retrieval-augmented generation (RAG) and agentic applications.\n\n3. **Structured Outputs**: Generating outputs in specific formats, like JSON, for better usability.\n\n4. **Output Guardrails**: Mechanisms to mitigate risks in generated responses, such as ensuring content is not harmful, biased, or offensive.\n\nThese functionalities ensure that AI models can effectively support their intended use cases in real-world applications.",
    "context": [
      {
        "start": 4165,
        "text": "ach makes sense from a\nbusiness perspective, focusing only on applications whose outcomes can be\nmeasured is similar to looking for the lost key under the lamppost (at night).\nIt’s easier to do, but it doesn’t mean we’ll find the key. We might be missing\nout on many potentially game-changing applications because there is no\neasy way to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can think of criteria in the\nfollowing buckets: domain-specific capability, generation capability,\ninstruction-following capability, and cost and latency.\nImagine you ask a model to summarize a legal contract. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstanding legal contracts. Generation capability metrics measure how\nco",
        "chapter": 4
      },
      {
        "start": 0,
        "text": "Chapter 4. Evaluate AI Systems\nA model is only useful if it works for its intended purposes. You need to\nevaluate models in the context of your application. Chapter 3 discusses\ndifferent approaches to automatic evaluation. This chapter discusses how to\nuse these approaches to evaluate models for your applications.\nThis chapter contains three parts. It starts with a discussion of the criteria\nyou might use to evaluate your applications and how these criteria are\ndefined and calculated. For example, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-specific\ncapabilities like math, science, reasoning, and summarization measured?\nThe second part focuses on model selection. Given an increasing number of\nfoundation models to choose from, it can feel overwhelming to choose the\nright model for your application. Thousands of benchmarks have been\nintroduced to evaluate these models along different criteria. Can these\nbenchmarks be trusted? How do you ",
        "chapter": 4
      },
      {
        "start": 13328,
        "text": "an be evaluated using AI\nas a judge—asking an AI model how fluent and coherent a text is—or using\nperplexity, as discussed in Chapter 3.\nGenerative models, with their new capabilities and new use cases, have new\nissues that require new metrics to track. The most pressing issue is\nundesired hallucinations. Hallucinations are desirable for creative tasks, not\nfor tasks that depend on factuality. A metric that many application\ndevelopers want to measure is factual consistency. Another issue commonly\ntracked is safety: can the generated outputs cause harm to users and\nsociety? Safety is an umbrella term for all types of toxicity and biases.\nThere are many other measurements that an application developer might\ncare about. For example, when I built my AI-powered writing assistant, I\ncared about controversiality, which measures content that isn’t necessarily\nharmful but can cause heated debates. Some people might care about\nfriendliness, positivity, creativity, or conciseness, but I won’t be ",
        "chapter": 4
      },
      {
        "start": 10829,
        "text": "bility to\ndifferentiate good responses from bad responses (classification), which is\ndifferent from the ability to generate good responses. MCQs are best suited\nfor evaluating knowledge (“does the model know that Paris is the capital of\nFrance?”) and reasoning (“can the model infer from a table of business\nexpenses which department is spending the most?”). They aren’t ideal for\nevaluating generation capabilities such as summarization, translation, and\nessay writing. Let’s discuss how generation capabilities can be evaluated in\nthe next section.\nGeneration Capability\nAI was used to generate open-ended outputs long before generative AI\nbecame a thing. For decades, the brightest minds in NLP (natural language\nprocessing) have been working on how to evaluate the quality of open-\nended outputs. The subfield that studies open-ended text generation is\ncalled NLG (natural language generation). NLG tasks in the early 2010s\nincluded translation, summarization, and paraphrasing.\nMetrics used to e",
        "chapter": 4
      },
      {
        "start": 109123,
        "text": "niques and criteria\ndiscussed in the last chapter and how to create an evaluation pipeline for\nyour application. No perfect evaluation method exists. It’s impossible to\ncapture the ability of a high-dimensional system using one- or few-\ndimensional scores. Evaluating modern AI systems has many limitations\nand biases. However, this doesn’t mean we shouldn’t do it. Combining\ndifferent methods and approaches can help mitigate many of these\nchallenges.\nEven though dedicated discussions on evaluation end here, evaluation will\ncome up again and again, not just throughout the book but also throughout\nyour application development process. Chapter 6 explores evaluating\nretrieval and agentic systems, while Chapters 7 and 9 focus on calculating a\nmodel’s memory usage, latency, and costs. Data quality verification is\naddressed in Chapter 8, and using user feedback to evaluate production\napplications is addressed in Chapter 10.\nWith that, let’s move onto the actual model adaptation process, startin",
        "chapter": 4
      },
      {
        "start": 6664,
        "text": " capabilities are commonly evaluated using exact\nevaluation. Coding-related capabilities are typically evaluated using\nfunctional correctness, as discussed in Chapter 3. While functional\ncorrectness is important, it might not be the only aspect that you care about.\nYou might also care about efficiency and cost. For example, would you\nwant a car that runs but consumes an excessive amount of fuel? Similarly, if\nan SQL query generated by your text-to-SQL model is correct but takes too\nlong or requires too much memory to run, it might not be usable.\nEfficiency can be exactly evaluated by measuring runtime or memory\nusage. BIRD-SQL (Li et al., 2023) is an example of a benchmark that takes\ninto account not only the generated query’s execution accuracy but also its\nefficiency, which is measured by comparing the runtime of the generated\nquery with the runtime of the ground truth SQL query.\nYou might also care about code readability. If the generated code runs but\nnobody can understand it, it w",
        "chapter": 4
      },
      {
        "start": 59976,
        "text": " might be sufficient.\nAnother reason that might cause open source models to lag behind is that\nopen source developers don’t receive feedback from users to improve their\nmodels, the way commercial models do. Once a model is open sourced,\nmodel developers have no idea how the model is being used, and how well\nthe model works in the wild.\nFunctionality\nMany functionalities are needed around a model to make it work for a use\ncase. Here are some examples of these functionalities:\nScalability: making sure the inference service can support your\napplication’s traffic while maintaining the desirable latency and cost.\nFunction calling: giving the model the ability to use external tools, which\nis essential for RAG and agentic use cases, as discussed in Chapter 6.\nStructured outputs, such as asking models to generate outputs in JSON\nformat.\nOutput guardrails: mitigating risks in the generated responses, such as\nmaking sure the responses aren’t racist or sexist.\nMany of these functionalities are ch",
        "chapter": 4
      },
      {
        "start": 45815,
        "text": "ase. For example, latency is a soft attribute if you have access to\nthe model to optimize it to run faster. It’s a hard attribute if you use a model\nhosted by someone else.\nAt a high level, the evaluation workflow consists of four steps (see\nFigure 4-5):\n1. Filter out models whose hard attributes don’t work for you. Your list of\nhard attributes depends heavily on your own internal policies, whether\nyou want to use commercial APIs or host your own models.\n2. Use publicly available information, e.g., benchmark performance and\nleaderboard ranking, to narrow down the most promising models to\nexperiment with, balancing different objectives such as model quality,\nlatency, and cost.\n3. Run experiments with your own evaluation pipeline to find the best\nmodel, again, balancing all your objectives.\n4. Continually monitor your model in production to detect failure and\ncollect feedback to improve your application.\nFigure 4-5. An overview of the evaluation workflow to evaluate models for your appli",
        "chapter": 4
      },
      {
        "start": 86632,
        "text": "ate hold-out data.\nPublic benchmarks will help you filter out bad models, but they won’t help\nyou find the best models for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each application might consist of\nmany ",
        "chapter": 4
      },
      {
        "start": 3332,
        "text": "pe. Applications\nshould demonstrate value to be deployed. As a result, the most common\nenterprise applications in production are those with clear evaluation criteria:\nRecommender systems are common because their successes can be\n1\nevaluated by an increase in engagement or purchase-through rates.\nThe success of a fraud detection system can be measured by how much\nmoney is saved from prevented frauds.\nCoding is a common generative AI use case because, unlike other\ngeneration tasks, generated code can be evaluated using functional\ncorrectness.\nEven though foundation models are open-ended, many of their use cases\nare close-ended, such as intent classification, sentiment analysis, next-\naction prediction, etc. It’s much easier to evaluate classification tasks\nthan open-ended tasks.\nWhile the evaluation-driven development approach makes sense from a\nbusiness perspective, focusing only on applications whose outcomes can be\nmeasured is similar to looking for the lost key under the lamppost (at",
        "chapter": 4
      }
    ],
    "input_tokens": 2439,
    "output_tokens": 127,
    "input_cost": 0.00036585,
    "output_cost": 7.62e-05,
    "total_cost": 0.00044205,
    "reference_answer": "The chapter outlines key functionalities like scalability, function calling, structured outputs, and output guardrails as crucial for AI models to function effectively in real-world applications."
  },
  {
    "question": "how to rank LLMs based on role performance",
    "answer": "To rank LLMs (Large Language Models) based on role performance, follow these steps as outlined in the context provided by the \"AI Engineering\" book:\n\n1. **Define the Role and Its Characteristics**:\n   - Specify the role the models are meant to emulate by providing a clear role description and catchphrases.\n\n2. **Establish Evaluation Criteria**:\n   - Use the following two primary criteria:\n     1. **Role-Speaking Style**: Assess which model exhibits a more pronounced and distinctive speaking style in line with the role description. The more unique and aligned the speaking style is, the better.\n     2. **Knowledge and Memory**: Evaluate which model's output contains richer knowledge and memories relevant to the role. Outputs should be relevant to any provided reference answers.\n\n3. **Incorporate Cost and Latency**:\n   - Consider the trade-off between model quality and operational factors like cost and latency. A model that provides high-quality outputs but is expensive or slow to run may not be practical. Aim to find a balance.\n\n4. **Aggregate Benchmark Results**:\n   - Utilize available benchmarks that align with the role's requirements. For instance, use a combination of various benchmarks that can include capabilities like knowledge understanding, reasoning, and specific tasks relevant to the role.\n\n5. **Utilize AI Judges or Heuristic Evaluations**:\n   - For a more automated assessment, leverage AI judges specifically designed for role evaluation. You can also devise heuristic evaluations, depending on the unique communication style of the character being modeled.\n\n6. **Maintain a Custom Leaderboard**:\n   - Create a leaderboard that reflects the performance of the models based on the established criteria. This will help in making consistent comparisons across different outputs and scenarios.\n\n7. **Fine-Tune and Validate Your Evaluation**:\n   - The evaluations should be continuously refined and validated against human judgments to ensure clarity and reliability.\n\n8. **Iterate and Adjust**: \n   - As your application develops, be ready to reevaluate model performance and make adjustments based on updated benchmarks, role nuances, or business needs.\n\nFollowing these steps will help you effectively rank LLMs based on their performance in specific roles.",
    "context": [
      {
        "start": 39151,
        "text": "le_name}’’ is\n‘‘{role_description_and_catchphrases}’’. I\nneed to rank the following models based on the\ntwo criteria below:\n1. Which one has more pronounced role speaking\nstyle, and speaks more in line with the role\ndescription. The more distinctive the speaking\nstyle, the better.\n2. Which one’s output contains more knowledge\nand memories related to the role; the richer,\nthe better. (If the question contains\nreference answers, then the role−specific\nknowledge and memories are based on the\nreference answer. )\nCost and Latency\nA model that generates high-quality outputs but is too slow and expensive\nto run will not be useful. When evaluating models, it’s important to balance\nmodel quality, latency, and cost. Many companies opt for lower-quality\nmodels if they provide better cost and latency. Cost and latency\noptimization are discussed in detail in Chapter 9, so this section will be\nquick.\nOptimizing for multiple objectives is an active field of study called Pareto\noptimization. When opti",
        "chapter": 4
      },
      {
        "start": 71638,
        "text": "rds often end up with different\nbenchmarks, making it hard to compare and interpret their rankings. For\nexample, in late 2023, Hugging Face updated their Open LLM Leaderboard\nto use the average of six different benchmarks to rank models:\n1. ARC-C (Clark et al., 2018): Measuring the ability to solve complex,\ngrade school-level science questions.\n2. MMLU (Hendrycks et al., 2020): Measuring knowledge and reasoning\ncapabilities in 57 subjects, including elementary mathematics, US\nhistory, computer science, and law.\n3. HellaSwag (Zellers et al., 2019): Measuring the ability to predict the\ncompletion of a sentence or a scene in a story or video. The goal is to test\ncommon sense and understanding of everyday activities.\n4. TruthfulQA (Lin et al., 2021): Measuring the ability to generate\nresponses that are not only accurate but also truthful and non-misleading,\nfocusing on a model’s understanding of facts.\n5. WinoGrande (Sakaguchi et al., 2019): Measuring the ability to solve\nchallenging prono",
        "chapter": 4
      },
      {
        "start": 38318,
        "text": "th style and knowledge. For example, if a model is supposed to\ntalk like Jackie Chan, its outputs should capture Jackie Chan’s style and are\n8\ngenerated based on Jackie Chan’s knowledge.\nAI judges for different roles will need different prompts. To give you a\nsense of what an AI judge’s prompt looks like, here is the beginning of the\nprompt used by the RoleLLM AI judge to rank models based on their ability\nto play a certain role. For the full prompt, please check out Wang et al.\n(2023).\nSystem Instruction:\nYou are a role−playing performance comparison\nassistant. You should rank the models based on\nthe role characteristics and text quality of\ntheir responses. The rankings are then output\nusing Python dictionaries and lists.\nUser Prompt:\nThe models below are to play the role of\n‘‘{role_name}’’. The role description of\n‘‘{role_name}’’ is\n‘‘{role_description_and_catchphrases}’’. I\nneed to rank the following models based on the\ntwo criteria below:\n1. Which one has more pronounced role speak",
        "chapter": 4
      },
      {
        "start": 69139,
        "text": "n harness. As of this writing, EleutherAI’s lm-evaluation-harness\nsupports over 400 benchmarks. OpenAI’s evals lets you run any of the\napproximately 500 existing benchmarks and register new benchmarks to\nevaluate OpenAI models. Their benchmarks evaluate a wide range of\ncapabilities, from doing math and solving puzzles to identifying ASCII art\nthat represents words.\nBenchmark selection and aggregation\nBenchmark results help you identify promising models for your use cases.\nAggregating benchmark results to rank models gives you a leaderboard.\nThere are two questions to consider:\nWhat benchmarks to include in your leaderboard?\nHow to aggregate these benchmark results to rank models?\nGiven so many benchmarks out there, it’s impossible to look at them all, let\nalone aggregate their results to decide which model is the best. Imagine that\nyou’re considering two models, A and B, for code generation. If model A\nperforms better than model B on a coding benchmark but worse on a\ntoxicity benchmark",
        "chapter": 4
      },
      {
        "start": 37485,
        "text": "lude RoleLLM (Wang et al., 2023) and\nCharacterEval (Tu et al., 2024). CharacterEval used human annotators and\ntrained a reward model to evaluate each roleplaying aspect on a five-point\nscale. RoleLLM evaluates a model’s ability to emulate a persona using both\ncarefully crafted similarity scores (how similar the generated outputs are to\nthe expected outputs) and AI judges.\nIf AI in your application is supposed to assume a certain role, make sure to\nevaluate whether your model stays in character. Depending on the role, you\nmight be able to create heuristics to evaluate the model’s outputs. For\nexample, if the role is someone who doesn’t talk a lot, a heuristic would be\nthe average of the model’s outputs. Other than that, the easiest automatic\nevaluation approach is AI as a judge. You should evaluate the roleplaying\nAI on both style and knowledge. For example, if a model is supposed to\ntalk like Jackie Chan, its outputs should capture Jackie Chan’s style and are\n8\ngenerated based on Jacki",
        "chapter": 4
      },
      {
        "start": 77469,
        "text": " fraction of times a model obtains\na better score than another model, averaged across scenarios”.\nWhile public leaderboards are useful to get a sense of models’ broad\nperformance, it’s important to understand what capabilities a leaderboard is\ntrying to capture. A model that ranks high on a public leaderboard will\nlikely, but far from always, perform well for your application. If you want a\nmodel for code generation, a public leaderboard that doesn’t include a code\ngeneration benchmark might not help you as much.\nCustom leaderboards with public benchmarks\nWhen evaluating models for a specific application, you’re basically creating\na private leaderboard that ranks models based on your evaluation criteria.\nThe first step is to gather a list of benchmarks that evaluate the capabilities\nimportant to your application. If you want to build a coding agent, look at\ncode-related benchmarks. If you build a writing assistant, look into creative\nwriting benchmarks. As new benchmarks are constantly",
        "chapter": 4
      },
      {
        "start": 69972,
        "text": "agine that\nyou’re considering two models, A and B, for code generation. If model A\nperforms better than model B on a coding benchmark but worse on a\ntoxicity benchmark, which model would you choose? Similarly, which\nmodel would you choose if one model performs better in one coding\nbenchmark but worse in another coding benchmark?\nFor inspiration on how to create your own leaderboard from public\nbenchmarks, it’s useful to look into how public leaderboards do so.\nPublic leaderboards\nMany public leaderboards rank models based on their aggregated\nperformance on a subset of benchmarks. These leaderboards are immensely\nhelpful but far from being comprehensive. First, due to the compute\nconstraint—evaluating a model on a benchmark requires compute—most\nleaderboards can incorporate only a small number of benchmarks. Some\nleaderboards might exclude an important but expensive benchmark. For\nexample, HELM (Holistic Evaluation of Language Models) Lite left out an\ninformation retrieval benchmark (MS",
        "chapter": 4
      },
      {
        "start": 93296,
        "text": " and 0 for neutral. Which scoring system to use depends on your\ndata and your needs.\nOn this scoring system, create a rubric with examples. What does a\nresponse with a score of 1 look like and why does it deserve a 1? Validate\nyour rubric with humans: yourself, coworkers, friends, etc. If humans find it\nhard to follow the rubric, you need to refine it to make it unambiguous. This\nprocess can require a lot of back and forth, but it’s necessary. A clear\nguideline is the backbone of a reliable evaluation pipeline. This guideline\ncan also be reused later for training data annotation, as discussed in\nChapter 8.\nTie evaluation metrics to business metrics\nWithin a business, an application must serve a business goal. The\napplication’s metrics must be considered in the context of the business\nproblem it’s built to solve.\nFor example, if your customer support chatbot’s factual consistency is 80%,\nwhat does it mean for the business? For example, this level of factual\nconsistency might make the ch",
        "chapter": 4
      },
      {
        "start": 108290,
        "text": "on, like all the build versus buy decisions, is\nunique to every team, depending not only on what the team needs but also\non what the team wants.\nThis chapter also explored the thousands of available public benchmarks.\nPublic benchmarks can help you weed out bad models, but they won’t help\nyou find the best models for your applications. Public benchmarks are also\nlikely contaminated, as their data is included in the training data of many\nmodels. There are public leaderboards that aggregate multiple benchmarks\nto rank models, but how benchmarks are selected and aggregated is not a\nclear process. The lessons learned from public leaderboards are helpful for\nmodel selection, as model selection is akin to creating a private leaderboard\nto rank models based on your needs.\nThis chapter ends with how to use all the evaluation techniques and criteria\ndiscussed in the last chapter and how to create an evaluation pipeline for\nyour application. No perfect evaluation method exists. It’s impossible t",
        "chapter": 4
      },
      {
        "start": 43316,
        "text": "> 90% > 95%\ngeneration\ncapability\nHard\nCriteria Metric Benchmark Ideal\nrequirement\nFactual Internal GPT Internal > 0.8 > 0.9\nconsistency metric hallucination\ndataset\nNow that you have your criteria, let’s move on to the next step and use them\nto select the best model for your application.\nModel Selection\nAt the end of the day, you don’t really care about which model is the best.\nYou care about which model is the best for your applications. Once you’ve\ndefined the criteria for your application, you should evaluate models against\nthese criteria.\nDuring the application development process, as you progress through\ndifferent adaptation techniques, you’ll have to do model selection over and\nover again. For example, prompt engineering might start with the strongest\nmodel overall to evaluate feasibility and then work backward to see if\nsmaller models would work. If you decide to do finetuning, you might start\nwith a small model to test your code and move toward the biggest model\nthat fits your",
        "chapter": 4
      }
    ],
    "input_tokens": 2645,
    "output_tokens": 444,
    "input_cost": 0.00039675,
    "output_cost": 0.0002664,
    "total_cost": 0.00066315,
    "reference_answer": "The chapter provides a methodology for assessing LLMs by creating and comparing their outputs against defined role characteristics and quality criteria, using Python for organization."
  },
  {
    "question": "best practices for collecting user feedback on AI performance",
    "answer": "Based on the context provided from the \"AI Engineering\" book by Chip Huyen, the best practices for collecting user feedback on AI performance include the following steps:\n\n1. **Define the Types of Feedback Needed**: Understand what specific feedback is useful for improving the application. Consider how user feedback correlates with other evaluation metrics and what insights can actually lead to enhancements.\n\n2. **Utilize Actual Users**: In a production environment, feedback from actual users is invaluable. Design your feedback mechanisms to capture insights directly from the end-users interacting with the AI application.\n\n3. **Iterative Feedback Loop**: Implement a process where feedback collection is continuous and iterative. This allows you to refine your application based on real-time user responses and changing needs.\n\n4. **Combine Automatic and Human Evaluation**: Use automatic metrics for efficient evaluation but do not hesitate to incorporate human evaluations, especially for more complex or subjective tasks. Human experts can provide deeper insights that automated processes may overlook.\n\n5. **Monitor Performance Regularly**: Establish a routine for analyzing user feedback and performance data. This could include daily evaluations of a subset of interactions, similar to practices used by companies like LinkedIn.\n\n6. **Document Changes and Insights**: Maintain a comprehensive log of all feedback and the variations in how evaluation metrics are perceived over time. This can assist in tracking improvements and understanding user sentiments.\n\n7. **Establish Clear Evaluation Guidelines**: Create clear and unambiguous evaluation criteria to assess performance. This should encompass both what the application should achieve and what it should avoid.\n\nIncorporating these practices can significantly enhance the understanding of AI performance and inform iterative improvements in your applications.",
    "context": [
      {
        "start": 46648,
        "text": " in production to detect failure and\ncollect feedback to improve your application.\nFigure 4-5. An overview of the evaluation workflow to evaluate models for your application.\nThese four steps are iterative—you might want to change the decision from\na previous step with newer information from the current step. For example,\nyou might initially want to host open source models. However, after public\nand private evaluation, you might realize that open source models can’t\nachieve the level of performance you want and have to switch to\ncommercial APIs.\nChapter 10 discusses monitoring and collecting user feedback. The rest of\nthis chapter will discuss the first three steps. First, let’s discuss a question\nthat most teams will visit more than once: to use model APIs or to host\nmodels themselves. We’ll then continue to how to navigate the dizzying\nnumber of public benchmarks and why you can’t trust them. This will set\nthe stage for the last section in the chapter. Because public benchmarks\ncan’t",
        "chapter": 4
      },
      {
        "start": 98294,
        "text": "ir AI\nsystems.\nConsider evaluation methods to be used not just during experimentation but\nalso during production. During experimentation, you might have reference\ndata to compare your application’s outputs to, whereas, in production,\nreference data might not be immediately available. However, in production,\nyou have actual users. Think about what kinds of feedback you want from\nusers, how user feedback correlates to other evaluation metrics, and how to\nuse user feedback to improve your application. How to collect user\nfeedback is discussed in Chapter 10.\nAnnotate evaluation data\nCurate a set of annotated examples to evaluate your application. You need\nannotated data to evaluate each of your system’s components and each\ncriterion, for both turn-based and task-based evaluation. Use actual\nproduction data if possible. If your application has natural labels that you\ncan use, that’s great. If not, you can use either humans or AI to label your\ndata. Chapter 8 discusses AI-generated data. The",
        "chapter": 4
      },
      {
        "start": 86632,
        "text": "ate hold-out data.\nPublic benchmarks will help you filter out bad models, but they won’t help\nyou find the best models for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each application might consist of\nmany ",
        "chapter": 4
      },
      {
        "start": 97461,
        "text": "ut this prediction. Logprobs can also be\nused to evaluate a model’s perplexity for a generated text, which can be\nused for measurements such as fluency and factual consistency.\nUse automatic metrics as much as possible, but don’t be afraid to fall back\non human evaluation, even in production. Having human experts manually\nevaluate a model’s quality is a long-standing practice in AI. Given the\nchallenges of evaluating open-ended responses, many teams are looking at\nhuman evaluation as the North Star metric to guide their application\ndevelopment. Each day, you can use human experts to evaluate a subset of\nyour application’s outputs that day to detect any changes in the application’s\nperformance or unusual patterns in usage. For example, LinkedIn developed\na process to manually evaluate up to 500 daily conservations with their AI\nsystems.\nConsider evaluation methods to be used not just during experimentation but\nalso during production. During experimentation, you might have reference\ndata",
        "chapter": 4
      },
      {
        "start": 109123,
        "text": "niques and criteria\ndiscussed in the last chapter and how to create an evaluation pipeline for\nyour application. No perfect evaluation method exists. It’s impossible to\ncapture the ability of a high-dimensional system using one- or few-\ndimensional scores. Evaluating modern AI systems has many limitations\nand biases. However, this doesn’t mean we shouldn’t do it. Combining\ndifferent methods and approaches can help mitigate many of these\nchallenges.\nEven though dedicated discussions on evaluation end here, evaluation will\ncome up again and again, not just throughout the book but also throughout\nyour application development process. Chapter 6 explores evaluating\nretrieval and agentic systems, while Chapters 7 and 9 focus on calculating a\nmodel’s memory usage, latency, and costs. Data quality verification is\naddressed in Chapter 8, and using user feedback to evaluate production\napplications is addressed in Chapter 10.\nWith that, let’s move onto the actual model adaptation process, startin",
        "chapter": 4
      },
      {
        "start": 106624,
        "text": "evaluation process changes constantly, you won’t be able to use the\nevaluation results to guide your application’s development.\nAs you iterate on your evaluation pipeline, make sure to do proper\nexperiment tracking: log all variables that could change in an evaluation\nprocess, including but not limited to the evaluation data, the rubric, and the\nprompt and sampling configurations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enable you to reduce risks, discover opportunities\nto improve performance, and benchmark progresses, which will all save\nyou time and headaches down the line.\nGiven an increasing number of readily available foundation models, for\nmost application developers, the challenge is no longer in developing\nmodels but in selecting the r",
        "chapter": 4
      },
      {
        "start": 0,
        "text": "Chapter 4. Evaluate AI Systems\nA model is only useful if it works for its intended purposes. You need to\nevaluate models in the context of your application. Chapter 3 discusses\ndifferent approaches to automatic evaluation. This chapter discusses how to\nuse these approaches to evaluate models for your applications.\nThis chapter contains three parts. It starts with a discussion of the criteria\nyou might use to evaluate your applications and how these criteria are\ndefined and calculated. For example, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-specific\ncapabilities like math, science, reasoning, and summarization measured?\nThe second part focuses on model selection. Given an increasing number of\nfoundation models to choose from, it can feel overwhelming to choose the\nright model for your application. Thousands of benchmarks have been\nintroduced to evaluate these models along different criteria. Can these\nbenchmarks be trusted? How do you ",
        "chapter": 4
      },
      {
        "start": 90797,
        "text": "ion Guideline\nCreating a clear evaluation guideline is the most important step of the\nevaluation pipeline. An ambiguous guideline leads to ambiguous scores that\ncan be misleading. If you don’t know what bad responses look like, you\nwon’t be able to catch them.\nWhen creating the evaluation guideline, it’s important to define not only\nwhat the application should do, but also what it shouldn’t do. For example,\nif you build a customer support chatbot, should this chatbot answer\nquestions unrelated to your product, such as about an upcoming election? If\nnot, you need to define what inputs are out of the scope of your application,\nhow to detect them, and how your application should respond to them.\nDefine evaluation criteria\nOften, the hardest part of evaluation isn’t determining whether an output is\ngood, but rather what good means. In retrospect of one year of deploying\ngenerative AI applications, LinkedIn shared that the first hurdle was in\ncreating an evaluation guideline. A correct resp",
        "chapter": 4
      },
      {
        "start": 91630,
        "text": "means. In retrospect of one year of deploying\ngenerative AI applications, LinkedIn shared that the first hurdle was in\ncreating an evaluation guideline. A correct response is not always a good\nresponse. For example, for their AI-powered Job Assessment application,\nthe response “You are a terrible fit” might be correct but not helpful, thus\nmaking it a bad response. A good response should explain the gap between\nthis job’s requirements and the candidate’s background, and what the\ncandidate can do to close this gap.\nBefore building your application, think about what makes a good response.\nLangChain’s State of AI 2023 found that, on average, their users used 2.3\ndifferent types of feedback (criteria) to evaluate an application. For\nexample, for a customer support application, a good response might be\ndefined using three criteria:\n1. Relevance: the response is relevant to the user’s query.\n2. Factual consistency: the response is factually consistent with the context.\n3. Safety: the respons",
        "chapter": 4
      },
      {
        "start": 4165,
        "text": "ach makes sense from a\nbusiness perspective, focusing only on applications whose outcomes can be\nmeasured is similar to looking for the lost key under the lamppost (at night).\nIt’s easier to do, but it doesn’t mean we’ll find the key. We might be missing\nout on many potentially game-changing applications because there is no\neasy way to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can think of criteria in the\nfollowing buckets: domain-specific capability, generation capability,\ninstruction-following capability, and cost and latency.\nImagine you ask a model to summarize a legal contract. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstanding legal contracts. Generation capability metrics measure how\nco",
        "chapter": 4
      }
    ],
    "input_tokens": 2451,
    "output_tokens": 333,
    "input_cost": 0.00036765,
    "output_cost": 0.0001998,
    "total_cost": 0.00056745,
    "reference_answer": "The text suggests considering user feedback as an essential evaluation method, correlating it with other metrics, and utilizing it actively during production for continuous improvement."
  },
  {
    "question": "NLG evaluation metrics examples",
    "answer": "In the context of Natural Language Generation (NLG), evaluation metrics used to assess the quality of generated texts include:\n\n1. **Fluency**: This metric assesses whether the text is grammatically correct and sounds natural. It considers if the generated text resembles something a fluent speaker would produce.\n\n2. **Coherence**: This evaluates how well-structured the text is, ensuring it follows a logical structure throughout.\n\n3. **Faithfulness**: Particularly used in translation tasks, this measures how accurately the generated translation aligns with the original sentence.\n\n4. **Relevance**: Commonly applied in summarization tasks, this metric examines whether the summary captures the most important aspects of the source document.\n\nAs generative models have evolved, some of these metrics, like fluency and coherence, remain important for weaker models or specific applications (e.g., creative writing or low-resource languages), while they may become less critical for more advanced models.",
    "context": [
      {
        "start": 11662,
        "text": "nded text generation is\ncalled NLG (natural language generation). NLG tasks in the early 2010s\nincluded translation, summarization, and paraphrasing.\nMetrics used to evaluate the quality of generated texts back then included\nfluency and coherence. Fluency measures whether the text is grammatically\ncorrect and natural-sounding (does this sound like something written by a\nfluent speaker?). Coherence measures how well-structured the whole text is\n(does it follow a logical structure?). Each task might also have its own\nmetrics. For example, a metric a translation task might use is faithfulness:\nhow faithful is the generated translation to the original sentence? A metric\nthat a summarization task might use is relevance: does the summary focus\non the most important aspects of the source document? (Li et al., 2022).\nSome early NLG metrics, including faithfulness and relevance, have been\nrepurposed, with significant modifications, to evaluate the outputs of\nfoundation models. As generative mod",
        "chapter": 4
      },
      {
        "start": 12495,
        "text": "LG metrics, including faithfulness and relevance, have been\nrepurposed, with significant modifications, to evaluate the outputs of\nfoundation models. As generative models improved, many issues of early\nNLG systems went away, and the metrics used to track these issues became\nless important. In the 2010s, generated texts didn’t sound natural. They\nwere typically full of grammatical errors and awkward sentences. Fluency\nand coherence, then, were important metrics to track. However, as language\nmodels’ generation capabilities have improved, AI-generated texts have\nbecome nearly indistinguishable from human-generated texts. Fluency and\n2\ncoherence become less important. However, these metrics can still be\nuseful for weaker models or for applications involving creative writing and\nlow-resource languages. Fluency and coherence can be evaluated using AI\nas a judge—asking an AI model how fluent and coherent a text is—or using\nperplexity, as discussed in Chapter 3.\nGenerative models, with their ",
        "chapter": 4
      },
      {
        "start": 10829,
        "text": "bility to\ndifferentiate good responses from bad responses (classification), which is\ndifferent from the ability to generate good responses. MCQs are best suited\nfor evaluating knowledge (“does the model know that Paris is the capital of\nFrance?”) and reasoning (“can the model infer from a table of business\nexpenses which department is spending the most?”). They aren’t ideal for\nevaluating generation capabilities such as summarization, translation, and\nessay writing. Let’s discuss how generation capabilities can be evaluated in\nthe next section.\nGeneration Capability\nAI was used to generate open-ended outputs long before generative AI\nbecame a thing. For decades, the brightest minds in NLP (natural language\nprocessing) have been working on how to evaluate the quality of open-\nended outputs. The subfield that studies open-ended text generation is\ncalled NLG (natural language generation). NLG tasks in the early 2010s\nincluded translation, summarization, and paraphrasing.\nMetrics used to e",
        "chapter": 4
      },
      {
        "start": 100793,
        "text": " example in Designing Machine Learning Systems. Numbers from Charig\net al., “Comparison of Treatment of Renal Calculi by Open Surgery, Percutaneous\nNephrolithotomy, and Extracorporeal Shockwave Lithotripsy”, British Medical Journal\n(Clinical Research Edition) 292, no. 6524 (March 1986): 879–82.\nYou should have multiple evaluation sets to represent different data slices.\nYou should have one set that represents the distribution of the actual\nproduction data to estimate how the system does overall. You can slice your\ndata based on tiers (paying users versus free users), traffic sources (mobile\nversus web), usage, and more. You can have a set consisting of the\nexamples for which the system is known to frequently make mistakes. You\ncan have a set of examples where users frequently make mistakes—if typos\nare common in production, you should have evaluation examples that\ncontain typos. You might want an out-of-scope evaluation set, inputs your\napplication isn’t supposed to engage with, to mak",
        "chapter": 4
      },
      {
        "start": 97461,
        "text": "ut this prediction. Logprobs can also be\nused to evaluate a model’s perplexity for a generated text, which can be\nused for measurements such as fluency and factual consistency.\nUse automatic metrics as much as possible, but don’t be afraid to fall back\non human evaluation, even in production. Having human experts manually\nevaluate a model’s quality is a long-standing practice in AI. Given the\nchallenges of evaluating open-ended responses, many teams are looking at\nhuman evaluation as the North Star metric to guide their application\ndevelopment. Each day, you can use human experts to evaluate a subset of\nyour application’s outputs that day to detect any changes in the application’s\nperformance or unusual patterns in usage. For example, LinkedIn developed\na process to manually evaluate up to 500 daily conservations with their AI\nsystems.\nConsider evaluation methods to be used not just during experimentation but\nalso during production. During experimentation, you might have reference\ndata",
        "chapter": 4
      },
      {
        "start": 105791,
        "text": "n’t need both of them. On the\nother hand, if two metrics are not at all correlated, this means either\nan interesting insight into your model or that your metrics just aren’t\n29\ntrustworthy.\nHow much cost and latency does your evaluation pipeline add to your\napplication?\nEvaluation, if not done carefully, can add significant latency and cost\nto your application. Some teams decide to skip evaluation in the hope\nof reducing latency. It’s a risky bet.\nIterate\nAs your needs and user behaviors change, your evaluation criteria will also\nevolve, and you’ll need to iterate on your evaluation pipeline. You might\nneed to update the evaluation criteria, change the scoring rubric, and add or\nremove examples. While iteration is necessary, you should be able to expect\na certain level of consistency from your evaluation pipeline. If the\nevaluation process changes constantly, you won’t be able to use the\nevaluation results to guide your application’s development.\nAs you iterate on your evaluation pipel",
        "chapter": 4
      },
      {
        "start": 93296,
        "text": " and 0 for neutral. Which scoring system to use depends on your\ndata and your needs.\nOn this scoring system, create a rubric with examples. What does a\nresponse with a score of 1 look like and why does it deserve a 1? Validate\nyour rubric with humans: yourself, coworkers, friends, etc. If humans find it\nhard to follow the rubric, you need to refine it to make it unambiguous. This\nprocess can require a lot of back and forth, but it’s necessary. A clear\nguideline is the backbone of a reliable evaluation pipeline. This guideline\ncan also be reused later for training data annotation, as discussed in\nChapter 8.\nTie evaluation metrics to business metrics\nWithin a business, an application must serve a business goal. The\napplication’s metrics must be considered in the context of the business\nproblem it’s built to solve.\nFor example, if your customer support chatbot’s factual consistency is 80%,\nwhat does it mean for the business? For example, this level of factual\nconsistency might make the ch",
        "chapter": 4
      },
      {
        "start": 104958,
        "text": " be asking about the quality of your\nevaluation pipeline:\nIs your evaluation pipeline getting you the right signals?\nDo better responses indeed get higher scores? Do better evaluation\nmetrics lead to better business outcomes?\nHow reliable is your evaluation pipeline?\nIf you run the same pipeline twice, do you get different results? If\nyou run the pipeline multiple times with different evaluation datasets,\nwhat would be the variance in the evaluation results? You should aim\nto increase reproducibility and reduce variance in your evaluation\npipeline. Be consistent with the configurations of your evaluation.\nFor example, if you use an AI judge, make sure to set your judge’s\ntemperature to 0.\nHow correlated are your metrics?\nAs discussed in “Benchmark selection and aggregation”, if two\nmetrics are perfectly correlated, you don’t need both of them. On the\nother hand, if two metrics are not at all correlated, this means either\nan interesting insight into your model or that your metrics just ",
        "chapter": 4
      },
      {
        "start": 45815,
        "text": "ase. For example, latency is a soft attribute if you have access to\nthe model to optimize it to run faster. It’s a hard attribute if you use a model\nhosted by someone else.\nAt a high level, the evaluation workflow consists of four steps (see\nFigure 4-5):\n1. Filter out models whose hard attributes don’t work for you. Your list of\nhard attributes depends heavily on your own internal policies, whether\nyou want to use commercial APIs or host your own models.\n2. Use publicly available information, e.g., benchmark performance and\nleaderboard ranking, to narrow down the most promising models to\nexperiment with, balancing different objectives such as model quality,\nlatency, and cost.\n3. Run experiments with your own evaluation pipeline to find the best\nmodel, again, balancing all your objectives.\n4. Continually monitor your model in production to detect failure and\ncollect feedback to improve your application.\nFigure 4-5. An overview of the evaluation workflow to evaluate models for your appli",
        "chapter": 4
      },
      {
        "start": 104125,
        "text": "% confident that one system is better.\nValues from OpenAI.\nDifference Sample size needed for\nto detect 95% confidence\n30% ~10\n10% ~100\n3% ~1,000\n1% ~10,000\nAs a reference, among evaluation benchmarks in Eleuther’s lm-evaluation-\nharness, the median number of examples is 1,000, and the average is 2,159.\nThe organizers of the Inverse Scaling prize suggested that 300 examples is\nthe absolute minimum and they would prefer at least 1,000, especially if the\nexamples are being synthesized (McKenzie et al., 2023).\nEvaluate your evaluation pipeline\nEvaluating your evaluation pipeline can help with both improving your\npipeline’s reliability and finding ways to make your evaluation pipeline\nmore efficient. Reliability is especially important with subjective evaluation\nmethods such as AI as a judge.\nHere are some questions you should be asking about the quality of your\nevaluation pipeline:\nIs your evaluation pipeline getting you the right signals?\nDo better responses indeed get higher scores? Do b",
        "chapter": 4
      }
    ],
    "input_tokens": 2542,
    "output_tokens": 190,
    "input_cost": 0.0003813,
    "output_cost": 0.000114,
    "total_cost": 0.0004953,
    "reference_answer": "It details various metrics from the NLG field, emphasizing fluency, coherence, and specific metrics tailored for tasks like translation and summarization."
  },
  {
    "question": "open source vs commercial AI models",
    "answer": "When comparing open source and commercial AI models, several key factors come into play, as outlined in the \"AI Engineering\" book by Chip Huyen.\n\n### Open Source Models\n1. **Transparency and Control**: Open source models allow users to inspect and modify the code, providing greater control and customizability. Enterprises often prefer this aspect, as it aligns with their need for transparency.\n\n2. **Community Support**: A strong community around open source models can lead to quicker troubleshooting and shared solutions for common issues, as many users will have faced similar challenges.\n\n3. **Legal Risks and Data Lineage**: Open source models may carry risks related to copyright infringement because users, rather than developers, may be liable if a model violates IP rights. While some companies may choose fully open models for better data lineage, the practicality of inspecting large datasets used to train models can be challenging.\n\n4. **Performance and Feedback**: Historically, open source models have lagged behind commercial models in performance. One reason is the lack of user feedback that open source developers receive, which can hinder improvement of the models over time.\n\n5. **Growing Competitiveness**: Despite these challenges, the performance gap between open source and proprietary models is closing, leading to optimism that open source models may achieve parity in performance in the future.\n\n### Commercial Models\n1. **Support and Reliability**: Commercial models typically come with customer support and built-in functionalities, which can be appealing to businesses looking for reliable, ready-to-use solutions.\n\n2. **Legal Protection**: Contracts with commercial model providers can offer legal protection, shielding companies from data lineage risks associated with the use of models trained on copyrighted data. This risk mitigation is a significant motivating factor for organizations to opt for commercial models.\n\n3. **Accessibility via APIs**: Commercial models are usually accessed through APIs, which can serve as competitive advantages for their developers. This dependence on APIs might limit control for users but ensures a more user-friendly interface and maintenance.\n\n4. **Incentives for Improvement**: Commercial providers are motivated to improve their models continuously based on user feedback and competitive pressures, allowing them to potentially outpace open source developments.\n\n### Conclusion\nThe choice between open source and commercial AI models involves weighing control and transparency against support and legal protections. While open source models offer advantages in customization and community engagement, commercial models typically provide enhanced stability, legal assurances, and ongoing support, making the decision heavily dependent on specific use cases and organizational needs.",
    "context": [
      {
        "start": 113288,
        "text": "The story was reported by several outlets, including TechRadar (see “Samsung Workers Made a\nMajor Error by Using ChatGPT”, by Lewis Maddison (April 2023).\n6\nAs regulations are evolving around the world, requirements for auditable information of models and\ntraining data may increase. Commercial models may be able to provide certifications, saving\ncompanies from the effort.\n7\nUsers want models to be open source because open means more information and more options, but\nwhat’s in it for model developers? Many companies have sprung up to capitalize on open source\nmodels by providing inference and finetuning services. It’s not a bad thing. Many people need these\nservices to leverage open source models. But, from model developers’ perspective, why invest\nmillions, if not billions, into building models just for others to make money?It might be argued that\nMeta supports open source models only to keep their competitors (Google, Microsoft/OpenAI) in\ncheck. Both Mistral and Cohere have open sourc",
        "chapter": 4
      },
      {
        "start": 47481,
        "text": "ate the dizzying\nnumber of public benchmarks and why you can’t trust them. This will set\nthe stage for the last section in the chapter. Because public benchmarks\ncan’t be trusted, you need to design your own evaluation pipeline with\nprompts and metrics you can trust.\nModel Build Versus Buy\nAn evergreen question for companies when leveraging any technology is\nwhether to build or buy. Since most companies won’t be building\nfoundation models from scratch, the question is whether to use commercial\nmodel APIs or host an open source model yourself. The answer to this\nquestion can significantly reduce your candidate model pool.\nLet’s first go into what exactly open source means when it comes to\nmodels, then discuss the pros and cons of these two approaches.\nOpen source, open weight, and model licenses\nThe term “open source model” has become contentious. Originally, open\nsource was used to refer to any model that people can download and use.\nFor many use cases, being able to download the model",
        "chapter": 4
      },
      {
        "start": 58310,
        "text": "oroughly inspect a dataset of the size\ntypically used to train foundation models.\nGiven the same concern, many companies opt for commercial models\ninstead. Open source models tend to have limited legal resources compared\nto commercial models. If you use an open source model that infringes on\ncopyrights, the infringed party is unlikely to go after the model developers,\nand more likely to go after you. However, if you use a commercial model,\nthe contracts you sign with the model providers can potentially protect you\n16\nfrom data lineage risks.\nPerformance\nVarious benchmarks have shown that the gap between open source models\nand proprietary models is closing. Figure 4-7 shows this gap decreasing on\nthe MMLU benchmark over time. This trend has made many people believe\nthat one day, there will be an open source model that performs just as well,\nif not better, than the strongest proprietary model.\nAs much as I want open source models to catch up with proprietary models,\nI don’t think the inc",
        "chapter": 4
      },
      {
        "start": 59976,
        "text": " might be sufficient.\nAnother reason that might cause open source models to lag behind is that\nopen source developers don’t receive feedback from users to improve their\nmodels, the way commercial models do. Once a model is open sourced,\nmodel developers have no idea how the model is being used, and how well\nthe model works in the wild.\nFunctionality\nMany functionalities are needed around a model to make it work for a use\ncase. Here are some examples of these functionalities:\nScalability: making sure the inference service can support your\napplication’s traffic while maintaining the desirable latency and cost.\nFunction calling: giving the model the ability to use external tools, which\nis essential for RAG and agentic use cases, as discussed in Chapter 6.\nStructured outputs, such as asking models to generate outputs in JSON\nformat.\nOutput guardrails: mitigating risks in the generated responses, such as\nmaking sure the responses aren’t racist or sexist.\nMany of these functionalities are ch",
        "chapter": 4
      },
      {
        "start": 57477,
        "text": "ty depends on “whether the human contribution to an innovation\nis significant enough to qualify for a patent.” It’s also unclear whether, if a\nmodel was trained on copyrighted data, and you use this model to create\nyour product, you can defend your product’s IP. Many companies whose\nexistence depends upon their IPs, such as gaming and movie studios, are\nhesitant to use AI to aid in the creation of their products, at least until IP\nlaws around AI are clarified (James Vincent, The Verge, November 15,\n2022).\nConcerns over data lineage have driven some companies toward fully open\nmodels, whose training data has been made publicly available. The\nargument is that this allows the community to inspect the data and make\nsure that it’s safe to use. While it sounds great in theory, in practice, it’s\nchallenging for any company to thoroughly inspect a dataset of the size\ntypically used to train foundation models.\nGiven the same concern, many companies opt for commercial models\ninstead. Open source",
        "chapter": 4
      },
      {
        "start": 63308,
        "text": "el to follow a standard API, which makes it easier to swap models.\nMany model developers try to make their models mimic the API of the most\npopular models. As of this writing, many API providers mimic OpenAI’s\nAPI.\nYou might also prefer models with good community support. The more\ncapabilities a model has, the more quirks it has. A model with a large\ncommunity of users means that any issue you encounter may already have\n19\nbeen experienced by others, who might have shared solutions online.\nControl, access, and transparency\nA 2024 study by a16z shows two key reasons that enterprises care about\nopen source models are control and customizability, as shown in Figure 4-8.\nFigure 4-8. Why enterprises care about open source models. Image from the 2024 study by a16z.\nIf your business depends on a model, it’s understandable that you would\nwant some control over it, and API providers might not always give you the\nlevel of control you want. When using a service provided by someone else,\nyou’re su",
        "chapter": 4
      },
      {
        "start": 59143,
        "text": "orms just as well,\nif not better, than the strongest proprietary model.\nAs much as I want open source models to catch up with proprietary models,\nI don’t think the incentives are set up for it. If you have the strongest model\navailable, would you rather open source it for other people to capitalize on\n17\nit, or would you try to capitalize on it yourself? It’s a common practice for\ncompanies to keep their strongest models behind APIs and open source their\nweaker models.\nFigure 4-7. The gap between open source models and proprietary models is decreasing on the\nMMLU benchmark. Image by Maxime Labonne.\nFor this reason, it’s likely that the strongest open source model will lag\nbehind the strongest proprietary models for the foreseeable future.\nHowever, for many use cases that don’t need the strongest models, open\nsource models might be sufficient.\nAnother reason that might cause open source models to lag behind is that\nopen source developers don’t receive feedback from users to improve thei",
        "chapter": 4
      },
      {
        "start": 114121,
        "text": "ey?It might be argued that\nMeta supports open source models only to keep their competitors (Google, Microsoft/OpenAI) in\ncheck. Both Mistral and Cohere have open source models, but they also have APIs. At some point,\ninference services on top of Mistral and Cohere models become their competitors.There’s the\nargument that open source is better for society, and maybe that’s enough as an incentive. People who\nwant what’s good for society will continue to push for open source, and maybe there will be enough\ncollective goodwill to help open source prevail. I certainly hope so.\n8\nThe companies that get hit the most by API costs are probably not the biggest companies. The\nbiggest companies might be important enough to service providers to negotiate favorable terms.\n9\nThis is similar to the philosophy in software infrastructure to always use the most popular tools that\nhave been extensively tested by the community.\n0\nWhen I posted a question on Hugging Face’s Discord about why they chose certa",
        "chapter": 4
      },
      {
        "start": 54145,
        "text": "antages. This means API providers might be more motivated to provide\nbetter APIs with better pricing.\nSince building scalable inference services for larger models is nontrivial,\nmany companies don’t want to build them themselves. This has led to the\ncreation of many third-party inference and finetuning services on top of\nopen source models. Major cloud providers like AWS, Azure, and GCP all\nprovide API access to popular open source models. A plethora of startups\nare doing the same.\nNOTE\nThere are also commercial API providers that can deploy their services within your private networks.\nIn this discussion, I treat these privately deployed commercial APIs similarly to self-hosted models.\nThe answer to whether to host a model yourself or use a model API depends\non the use case. And the same use case can change over time. Here are\nseven axes to consider: data privacy, data lineage, performance,\nfunctionality, costs, control, and on-device deployment.\nData privacy\nExternally hosted model AP",
        "chapter": 4
      },
      {
        "start": 53312,
        "text": "such as Databricks Mosaic,\nAnyscale, etc.). The same model can be available through different APIs\nwith different features, constraints, and pricings. For example, GPT-4 is\navailable through both OpenAI and Azure APIs. There might be slight\ndifferences in the performance of the same model provided through\ndifferent APIs, as different APIs might use different techniques to optimize\nthis model, so make sure to run thorough tests when you switch between\nmodel APIs.\nCommercial models are only accessible via APIs licensed by the model\n13\ndevelopers. Open source models can be supported by any API provider,\nallowing you to pick and choose the provider that works best for you. For\ncommercial model providers, models are their competitive advantages. For\nAPI providers that don’t have their own models, APIs are their competitive\nadvantages. This means API providers might be more motivated to provide\nbetter APIs with better pricing.\nSince building scalable inference services for larger models is n",
        "chapter": 4
      }
    ],
    "input_tokens": 2512,
    "output_tokens": 499,
    "input_cost": 0.0003768,
    "output_cost": 0.0002994,
    "total_cost": 0.0006762,
    "reference_answer": "The text outlines the differences between open source models and commercial models, noting that many developers release weaker models openly while keeping stronger models behind paywalls."
  },
  {
    "question": "evaluating AI with multiple-choice questions",
    "answer": "To evaluate AI with multiple-choice questions (MCQs), several steps and considerations are necessary based on the context from the \"AI Engineering\" book by Chip Huyen:\n\n1. **Define Evaluation Criteria**: Establish clear criteria tailored to your application. This includes domain-specific capabilities, generation capabilities, and instruction-following capabilities. Criteria should outline what constitutes a correct or acceptable response.\n\n2. **Structure the Questions**: MCQs should be designed so that they can verify specific knowledge or performance metrics. Each question might have one or multiple correct answers, and the format should facilitate consistent assessment.\n\n3. **Use Existing Benchmarks**: Leverage standardized benchmarks like MMLU, AGIEval, or ARC-C, which predominantly utilize multiple-choice formats to facilitate easier evaluation. These benchmarks often exclude open-ended tasks to avoid inconsistencies.\n\n4. **Example MCQ Format**: Consider structuring a question similar to this:\n   - Question: \"Why does the government regulate monopolies?\"\n     - (A) Producer surplus is lost and consumer surplus is gained.\n     - (B) Monopoly firms ensure productive efficiency.\n     - (C) Monopolies do not engage in R&D.\n     - (D) Consumer surplus is lost with higher prices and lower levels of output.\n   - Correct Answer: (D)\n\n5. **Scoring and Metric Selection**: A common scoring metric is accuracy, where answers are judged based on their correctness against established answers. \n\n6. **Establish Evaluation Guidelines**: Develop guidelines outlining not just the expected outputs but also the unacceptable ones. This helps ensure that responses remain aligned with application intentions.\n\n7. **Iterate and Update**: As evaluation processes evolve, maintain thorough logs of all variables and evaluation settings, which will facilitate adjustments and improvements in your evaluation pipeline.\n\nUsing MCQs as a close-ended evaluation method provides clearer outputs than open-ended formats and allows for straightforward assessment of AI capabilities in specific tasks.",
    "context": [
      {
        "start": 0,
        "text": "Chapter 4. Evaluate AI Systems\nA model is only useful if it works for its intended purposes. You need to\nevaluate models in the context of your application. Chapter 3 discusses\ndifferent approaches to automatic evaluation. This chapter discusses how to\nuse these approaches to evaluate models for your applications.\nThis chapter contains three parts. It starts with a discussion of the criteria\nyou might use to evaluate your applications and how these criteria are\ndefined and calculated. For example, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-specific\ncapabilities like math, science, reasoning, and summarization measured?\nThe second part focuses on model selection. Given an increasing number of\nfoundation models to choose from, it can feel overwhelming to choose the\nright model for your application. Thousands of benchmarks have been\nintroduced to evaluate these models along different criteria. Can these\nbenchmarks be trusted? How do you ",
        "chapter": 4
      },
      {
        "start": 4165,
        "text": "ach makes sense from a\nbusiness perspective, focusing only on applications whose outcomes can be\nmeasured is similar to looking for the lost key under the lamppost (at night).\nIt’s easier to do, but it doesn’t mean we’ll find the key. We might be missing\nout on many potentially game-changing applications because there is no\neasy way to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can think of criteria in the\nfollowing buckets: domain-specific capability, generation capability,\ninstruction-following capability, and cost and latency.\nImagine you ask a model to summarize a legal contract. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstanding legal contracts. Generation capability metrics measure how\nco",
        "chapter": 4
      },
      {
        "start": 109123,
        "text": "niques and criteria\ndiscussed in the last chapter and how to create an evaluation pipeline for\nyour application. No perfect evaluation method exists. It’s impossible to\ncapture the ability of a high-dimensional system using one- or few-\ndimensional scores. Evaluating modern AI systems has many limitations\nand biases. However, this doesn’t mean we shouldn’t do it. Combining\ndifferent methods and approaches can help mitigate many of these\nchallenges.\nEven though dedicated discussions on evaluation end here, evaluation will\ncome up again and again, not just throughout the book but also throughout\nyour application development process. Chapter 6 explores evaluating\nretrieval and agentic systems, while Chapters 7 and 9 focus on calculating a\nmodel’s memory usage, latency, and costs. Data quality verification is\naddressed in Chapter 8, and using user feedback to evaluate production\napplications is addressed in Chapter 10.\nWith that, let’s move onto the actual model adaptation process, startin",
        "chapter": 4
      },
      {
        "start": 8330,
        "text": "e model outputs option A, the model is wrong.\nThis is the approach that most public benchmarks follow. In April 2024,\n75% of the tasks in Eleuther’s lm-evaluation-harness are multiple-choice,\nincluding UC Berkeley’s MMLU (2020), Microsoft’s AGIEval (2023), and\nthe AI2 Reasoning Challenge (ARC-C) (2018). In their paper, AGIEval’s\nauthors explained that they excluded open-ended tasks on purpose to avoid\ninconsistent assessment.\nHere’s an example of a multiple-choice question in the MMLU benchmark:\nQuestion: One of the reasons that the government discourages and\nregulates monopolies is that\n(A) Producer surplus is lost and consumer surplus is gained.\n(B) Monopoly prices ensure productive efficiency but cost society\nallocative efficiency.\n(C) Monopoly firms do not engage in significant research and\ndevelopment.\n(D) Consumer surplus is lost with higher prices and lower levels of\noutput.\nLabel: (D)\nA multiple-choice question (MCQ) might have one or more correct\nanswers. A common metric is ac",
        "chapter": 4
      },
      {
        "start": 89964,
        "text": "n is the twenty_questions\nbenchmark, inspired by the classic game Twenty Questions, in the BIG-\nbench benchmark suite. One instance of the model (Alice) chooses a\nconcept, such as apple, car, or computer. Another instance of the model\n(Bob) asks Alice a series of questions to try to identify this concept. Alice\ncan only answer yes or no. The score is based on whether Bob successfully\nguesses the concept, and how many questions it takes for Bob to guess it.\nHere’s an example of a plausible conversation in this task, taken from the\nBIG-bench’s GitHub repository:\nBob: Is the concept an animal?\nAlice: No.\nBob: Is the concept a plant?\nAlice: Yes.\nBob: Does it grow in the ocean?\nAlice: No.\nBob: Does it grow in a tree?\nAlice: Yes.\nBob: Is it an apple?\n[Bob’s guess is correct, and the task is\ncompleted.]\nStep 2. Create an Evaluation Guideline\nCreating a clear evaluation guideline is the most important step of the\nevaluation pipeline. An ambiguous guideline leads to ambiguous scores that\ncan be",
        "chapter": 4
      },
      {
        "start": 106624,
        "text": "evaluation process changes constantly, you won’t be able to use the\nevaluation results to guide your application’s development.\nAs you iterate on your evaluation pipeline, make sure to do proper\nexperiment tracking: log all variables that could change in an evaluation\nprocess, including but not limited to the evaluation data, the rubric, and the\nprompt and sampling configurations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enable you to reduce risks, discover opportunities\nto improve performance, and benchmark progresses, which will all save\nyou time and headaches down the line.\nGiven an increasing number of readily available foundation models, for\nmost application developers, the challenge is no longer in developing\nmodels but in selecting the r",
        "chapter": 4
      },
      {
        "start": 86632,
        "text": "ate hold-out data.\nPublic benchmarks will help you filter out bad models, but they won’t help\nyou find the best models for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each application might consist of\nmany ",
        "chapter": 4
      },
      {
        "start": 7497,
        "text": "nerated\nquery with the runtime of the ground truth SQL query.\nYou might also care about code readability. If the generated code runs but\nnobody can understand it, it will be challenging to maintain the code or\nincorporate it into a system. There’s no obvious way to evaluate code\nreadability exactly, so you might have to rely on subjective evaluation, such\nas using AI judges.\nNon-coding domain capabilities are often evaluated with close-ended tasks,\nsuch as multiple-choice questions. Close-ended outputs are easier to verify\nand reproduce. For example, if you want to evaluate a model’s ability to do\nmath, an open-ended approach is to ask the model to generate the solution\nto a given problem. A close-ended approach is to give the model several\noptions and let it pick the correct one. If the expected answer is option C\nand the model outputs option A, the model is wrong.\nThis is the approach that most public benchmarks follow. In April 2024,\n75% of the tasks in Eleuther’s lm-evaluation-harn",
        "chapter": 4
      },
      {
        "start": 90797,
        "text": "ion Guideline\nCreating a clear evaluation guideline is the most important step of the\nevaluation pipeline. An ambiguous guideline leads to ambiguous scores that\ncan be misleading. If you don’t know what bad responses look like, you\nwon’t be able to catch them.\nWhen creating the evaluation guideline, it’s important to define not only\nwhat the application should do, but also what it shouldn’t do. For example,\nif you build a customer support chatbot, should this chatbot answer\nquestions unrelated to your product, such as about an upcoming election? If\nnot, you need to define what inputs are out of the scope of your application,\nhow to detect them, and how your application should respond to them.\nDefine evaluation criteria\nOften, the hardest part of evaluation isn’t determining whether an output is\ngood, but rather what good means. In retrospect of one year of deploying\ngenerative AI applications, LinkedIn shared that the first hurdle was in\ncreating an evaluation guideline. A correct resp",
        "chapter": 4
      },
      {
        "start": 37485,
        "text": "lude RoleLLM (Wang et al., 2023) and\nCharacterEval (Tu et al., 2024). CharacterEval used human annotators and\ntrained a reward model to evaluate each roleplaying aspect on a five-point\nscale. RoleLLM evaluates a model’s ability to emulate a persona using both\ncarefully crafted similarity scores (how similar the generated outputs are to\nthe expected outputs) and AI judges.\nIf AI in your application is supposed to assume a certain role, make sure to\nevaluate whether your model stays in character. Depending on the role, you\nmight be able to create heuristics to evaluate the model’s outputs. For\nexample, if the role is someone who doesn’t talk a lot, a heuristic would be\nthe average of the model’s outputs. Other than that, the easiest automatic\nevaluation approach is AI as a judge. You should evaluate the roleplaying\nAI on both style and knowledge. For example, if a model is supposed to\ntalk like Jackie Chan, its outputs should capture Jackie Chan’s style and are\n8\ngenerated based on Jacki",
        "chapter": 4
      }
    ],
    "input_tokens": 2550,
    "output_tokens": 392,
    "input_cost": 0.0003825,
    "output_cost": 0.0002352,
    "total_cost": 0.0006177,
    "reference_answer": "It describes how non-coding abilities can be assessed using close-ended tasks, particularly multiple-choice questions, which simplify verification and consistency in evaluation."
  },
  {
    "question": "how to set metrics for chatbot evaluation",
    "answer": "To set metrics for chatbot evaluation, you can follow these key steps based on the \"AI Engineering\" book by Chip Huyen:\n\n1. **Define Business Metrics**: Start by understanding the business goals your chatbot is targeting. These could include stickiness metrics (like Daily Active Users) or engagement metrics (such as the number of conversations initiated). Determine how these metrics tie back to your chatbot's performance.\n\n2. **Establish Usefulness Thresholds**: Determine the minimum score your chatbot must achieve to be considered useful. For example, you might decide that a factual consistency score needs to be at least 50% for the chatbot to handle general customer requests adequately.\n\n3. **Map Evaluation Metrics to Business Impact**: Align your evaluation metrics with business metrics to understand their implications. For instance, you could establish that:\n   - Factual consistency of 80% allows automation of 30% of customer support requests.\n   - Factual consistency of 90% allows for 50% automation.\n   - Factual consistency of 98% enables 90% automation.\n\n4. **Create a Scoring Rubric**: Develop a clear and unambiguous scoring rubric to evaluate responses. For each score (e.g., from 0 to 1), define what constitutes that score, including clear examples. Validate the rubric with input from others to ensure clarity.\n\n5. **Select Evaluation Methods**: Depending on the criteria established, choose appropriate evaluation methods. These could include automatic metrics or human evaluations. For instance, you might combine semantic similarity measures with a toxicity classifier and inputs from human evaluators.\n\n6. **Iterate Evaluation Processes**: Define evaluation methods not just for experimentation, but also for production. Regularly evaluate a subset of the chatbot’s outputs to monitor performance over time and identify any unusual patterns.\n\nBy implementing these steps, you can create an effective set of metrics for evaluating your chatbot, ensuring it aligns with your business objectives and maintains a clear standard for performance.",
    "context": [
      {
        "start": 94962,
        "text": " to determine the usefulness threshold: what scores must an\napplication achieve for it to be useful? For example, you might determine\nthat your chatbot’s factual consistency score must be at least 50% for it to\nbe useful. Anything below this makes it unusable even for general customer\nrequests.\nBefore developing AI evaluation metrics, it’s crucial to first understand the\nbusiness metrics you’re targeting. Many applications focus on stickiness\nmetrics, such as daily, weekly, or monthly active users (DAU, WAU,\nMAU). Others prioritize engagement metrics, like the number of\nconversations a user initiates per month or the duration of each visit—the\nlonger a user stays on the app, the less likely they are to leave. Choosing\nwhich metrics to prioritize can feel like balancing profits with social\nresponsibility. While an emphasis on stickiness and engagement metrics can\nlead to higher revenues, it may also cause a product to prioritize addictive\nfeatures or extreme content, which can be detrim",
        "chapter": 4
      },
      {
        "start": 94129,
        "text": "le, if your customer support chatbot’s factual consistency is 80%,\nwhat does it mean for the business? For example, this level of factual\nconsistency might make the chatbot unusable for questions about billing but\ngood enough for queries about product recommendations or general\ncustomer feedback. Ideally, you want to map evaluation metrics to business\nmetrics, to something that looks like this:\nFactual consistency of 80%: we can automate 30% of customer support\nrequests.\nFactual consistency of 90%: we can automate 50%.\nFactual consistency of 98%: we can automate 90%.\nUnderstanding the impact of evaluation metrics on business metrics is\nhelpful for planning. If you know how much gain you can get from\nimproving a certain metric, you might have more confidence to invest\nresources into improving that metric.\nIt’s also helpful to determine the usefulness threshold: what scores must an\napplication achieve for it to be useful? For example, you might determine\nthat your chatbot’s factual consi",
        "chapter": 4
      },
      {
        "start": 93296,
        "text": " and 0 for neutral. Which scoring system to use depends on your\ndata and your needs.\nOn this scoring system, create a rubric with examples. What does a\nresponse with a score of 1 look like and why does it deserve a 1? Validate\nyour rubric with humans: yourself, coworkers, friends, etc. If humans find it\nhard to follow the rubric, you need to refine it to make it unambiguous. This\nprocess can require a lot of back and forth, but it’s necessary. A clear\nguideline is the backbone of a reliable evaluation pipeline. This guideline\ncan also be reused later for training data annotation, as discussed in\nChapter 8.\nTie evaluation metrics to business metrics\nWithin a business, an application must serve a business goal. The\napplication’s metrics must be considered in the context of the business\nproblem it’s built to solve.\nFor example, if your customer support chatbot’s factual consistency is 80%,\nwhat does it mean for the business? For example, this level of factual\nconsistency might make the ch",
        "chapter": 4
      },
      {
        "start": 97461,
        "text": "ut this prediction. Logprobs can also be\nused to evaluate a model’s perplexity for a generated text, which can be\nused for measurements such as fluency and factual consistency.\nUse automatic metrics as much as possible, but don’t be afraid to fall back\non human evaluation, even in production. Having human experts manually\nevaluate a model’s quality is a long-standing practice in AI. Given the\nchallenges of evaluating open-ended responses, many teams are looking at\nhuman evaluation as the North Star metric to guide their application\ndevelopment. Each day, you can use human experts to evaluate a subset of\nyour application’s outputs that day to detect any changes in the application’s\nperformance or unusual patterns in usage. For example, LinkedIn developed\na process to manually evaluate up to 500 daily conservations with their AI\nsystems.\nConsider evaluation methods to be used not just during experimentation but\nalso during production. During experimentation, you might have reference\ndata",
        "chapter": 4
      },
      {
        "start": 95795,
        "text": " on stickiness and engagement metrics can\nlead to higher revenues, it may also cause a product to prioritize addictive\nfeatures or extreme content, which can be detrimental to users.\nStep 3. Define Evaluation Methods and Data\nNow that you’ve developed your criteria and scoring rubrics, let’s define\nwhat methods and data you want to use to evaluate your application.\nSelect evaluation methods\nDifferent criteria might require different evaluation methods. For example,\nyou use a small, specialized toxicity classifier for toxicity detection,\nsemantic similarity to measure relevance between the response and the\nuser’s original question, and an AI judge to measure the factual consistency\nbetween the response and the whole context. An unambiguous scoring\nrubric and examples will be critical for specialized scorers and AI judges to\nsucceed.\nIt’s possible to mix and match evaluation methods for the same criteria. For\nexample, you might have a cheap classifier that gives low-quality signals on\n10",
        "chapter": 4
      },
      {
        "start": 2499,
        "text": "ut he had no idea if the\nmodel’s predictions were accurate. At the beginning of the ChatGPT fever,\ncompanies rushed to deploy customer support chatbots. Many of them are\nstill unsure if these chatbots help or hurt their user experience.\nBefore investing time, money, and resources into building an application,\nit’s important to understand how this application will be evaluated. I call\nthis approach evaluation-driven development. The name is inspired by test-\ndriven development in software engineering, which refers to the method of\nwriting tests before writing code. In AI engineering, evaluation-driven\ndevelopment means defining evaluation criteria before building.\nEVALUATION-DRIVEN DEVELOPMENT\nWhile some companies chase the latest hype, sensible business decisions\nare still being made based on returns on investment, not hype. Applications\nshould demonstrate value to be deployed. As a result, the most common\nenterprise applications in production are those with clear evaluation criteria:\n",
        "chapter": 4
      },
      {
        "start": 90797,
        "text": "ion Guideline\nCreating a clear evaluation guideline is the most important step of the\nevaluation pipeline. An ambiguous guideline leads to ambiguous scores that\ncan be misleading. If you don’t know what bad responses look like, you\nwon’t be able to catch them.\nWhen creating the evaluation guideline, it’s important to define not only\nwhat the application should do, but also what it shouldn’t do. For example,\nif you build a customer support chatbot, should this chatbot answer\nquestions unrelated to your product, such as about an upcoming election? If\nnot, you need to define what inputs are out of the scope of your application,\nhow to detect them, and how your application should respond to them.\nDefine evaluation criteria\nOften, the hardest part of evaluation isn’t determining whether an output is\ngood, but rather what good means. In retrospect of one year of deploying\ngenerative AI applications, LinkedIn shared that the first hurdle was in\ncreating an evaluation guideline. A correct resp",
        "chapter": 4
      },
      {
        "start": 96628,
        "text": "o\nsucceed.\nIt’s possible to mix and match evaluation methods for the same criteria. For\nexample, you might have a cheap classifier that gives low-quality signals on\n100% of your data, and an expensive AI judge to give high-quality signals\non 1% of the data. This gives you a certain level of confidence in your\napplication while keeping costs manageable.\nWhen logprobs are available, use them. Logprobs can be used to measure\nhow confident a model is about a generated token. This is especially useful\nfor classification. For example, if you ask a model to output one of the three\nclasses and the model’s logprobs for these three classes are all between 30\nand 40%, this means the model isn’t confident about this prediction.\nHowever, if the model’s probability for one class is 95%, this means that\nthe model is highly confident about this prediction. Logprobs can also be\nused to evaluate a model’s perplexity for a generated text, which can be\nused for measurements such as fluency and factual con",
        "chapter": 4
      },
      {
        "start": 37485,
        "text": "lude RoleLLM (Wang et al., 2023) and\nCharacterEval (Tu et al., 2024). CharacterEval used human annotators and\ntrained a reward model to evaluate each roleplaying aspect on a five-point\nscale. RoleLLM evaluates a model’s ability to emulate a persona using both\ncarefully crafted similarity scores (how similar the generated outputs are to\nthe expected outputs) and AI judges.\nIf AI in your application is supposed to assume a certain role, make sure to\nevaluate whether your model stays in character. Depending on the role, you\nmight be able to create heuristics to evaluate the model’s outputs. For\nexample, if the role is someone who doesn’t talk a lot, a heuristic would be\nthe average of the model’s outputs. Other than that, the easiest automatic\nevaluation approach is AI as a judge. You should evaluate the roleplaying\nAI on both style and knowledge. For example, if a model is supposed to\ntalk like Jackie Chan, its outputs should capture Jackie Chan’s style and are\n8\ngenerated based on Jacki",
        "chapter": 4
      },
      {
        "start": 104958,
        "text": " be asking about the quality of your\nevaluation pipeline:\nIs your evaluation pipeline getting you the right signals?\nDo better responses indeed get higher scores? Do better evaluation\nmetrics lead to better business outcomes?\nHow reliable is your evaluation pipeline?\nIf you run the same pipeline twice, do you get different results? If\nyou run the pipeline multiple times with different evaluation datasets,\nwhat would be the variance in the evaluation results? You should aim\nto increase reproducibility and reduce variance in your evaluation\npipeline. Be consistent with the configurations of your evaluation.\nFor example, if you use an AI judge, make sure to set your judge’s\ntemperature to 0.\nHow correlated are your metrics?\nAs discussed in “Benchmark selection and aggregation”, if two\nmetrics are perfectly correlated, you don’t need both of them. On the\nother hand, if two metrics are not at all correlated, this means either\nan interesting insight into your model or that your metrics just ",
        "chapter": 4
      }
    ],
    "input_tokens": 2519,
    "output_tokens": 400,
    "input_cost": 0.00037785,
    "output_cost": 0.00024,
    "total_cost": 0.00061785,
    "reference_answer": "The chapter suggests determining a usefulness threshold for evaluation metrics, like a factual consistency score, to assess a chatbot's effectiveness before engaging users."
  },
  {
    "question": "how to detect data contamination in AI models",
    "answer": "To detect data contamination in AI models, you can employ two primary heuristics:\n\n1. **N-gram Overlapping**: Check if sequences of tokens (e.g., 13 tokens) in your evaluation samples also appear in the training data. If there is an overlap, it indicates that the evaluation sample has likely been seen during training and is considered \"dirty.\"\n\n2. **Perplexity**: This measure helps evaluate how difficult it is for a model to predict a given text. High perplexity scores can indicate that an evaluation sample is not drawn from the model's training data, while low perplexity might suggest potential contamination.\n\nBy using these methods, you can identify and address data contamination, ensuring more reliable evaluation benchmarks.",
    "context": [
      {
        "start": 83300,
        "text": "your\nusers. So the released model is contaminated, and your users won’t be able\nto evaluate it on contaminated benchmarks, but this might still be the right\nthing to do.\nHandling data contamination\nThe prevalence of data contamination undermines the trustworthiness of\nevaluation benchmarks. Just because a model can achieve high performance\non bar exams doesn’t mean it’s good at giving legal advice. It could just be\nthat this model has been trained on many bar exam questions.\nTo deal with data contamination, you first need to detect the contamination,\nand then decontaminate your data. You can detect contamination using\nheuristics like n-gram overlapping and perplexity:\nN-gram overlapping\nFor example, if a sequence of 13 tokens in an evaluation sample is\nalso in the training data, the model has likely seen this evaluation\nsample during training. This evaluation sample is considered dirty.\nPerplexity\nRecall that perplexity measures how difficult it is for a model to\npredict a given text. ",
        "chapter": 4
      },
      {
        "start": 82467,
        "text": "ed\nto create new benchmarks to evaluate their new models.\nData contamination can happen indirectly, such as when both evaluation\nand training data come from the same source. For example, you might\ninclude math textbooks in the training data to improve the model’s math\ncapabilities, and someone else might use questions from the same math\ntextbooks to create a benchmark to evaluate the model’s capabilities.\nData contamination can also happen intentionally for good reasons. Let’s\nsay you want to create the best possible model for your users. Initially, you\nexclude benchmark data from the model’s training data and choose the best\nmodel based on these benchmarks. However, because high-quality\nbenchmark data can improve the model’s performance, you then continue\ntraining your best model on benchmark data before releasing it to your\nusers. So the released model is contaminated, and your users won’t be able\nto evaluate it on contaminated benchmarks, but this might still be the right\nthing to d",
        "chapter": 4
      },
      {
        "start": 81634,
        "text": "nstrated this beautifully in\nhis 2023 satirical paper “Pretraining on the Test Set Is All You Need”. By\ntraining exclusively on data from several benchmarks, his one-million-\nparameter model was able to achieve near-perfect scores and outperformed\nmuch larger models on all these benchmarks.\nHow data contamination happens\nWhile some might intentionally train on benchmark data to achieve\nmisleadingly high scores, most data contamination is unintentional. Many\nmodels today are trained on data scraped from the internet, and the scraping\nprocess can accidentally pull data from publicly available benchmarks.\nBenchmark data published before the training of a model is likely included\n27\nin the model’s training data. It’s one of the reasons existing benchmarks\nbecome saturated so quickly, and why model developers often feel the need\nto create new benchmarks to evaluate their new models.\nData contamination can happen indirectly, such as when both evaluation\nand training data come from the same s",
        "chapter": 4
      },
      {
        "start": 86632,
        "text": "ate hold-out data.\nPublic benchmarks will help you filter out bad models, but they won’t help\nyou find the best models for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each application might consist of\nmany ",
        "chapter": 4
      },
      {
        "start": 26656,
        "text": "uts. They’re unfortunately\nextremely common online. Many models developed to detect toxicity in\nhuman-generated texts can be used for AI-generated texts. These\nspecialized models tend to be much smaller, faster, and cheaper than\ngeneral-purpose AI judges. Examples of these models are Facebook’s hate\nspeech detection model, the Skolkovo Institute’s toxicity classifier, and\nPerspective API. There are also many toxicity and hate speech detection\nmodels specialized in different languages, such as Danish and Vietnamese.\nCommon benchmarks to measure toxicity include RealToxicityPrompts\n(Gehman et al., 2020) and BOLD (bias in open-ended language generation\ndataset) (Dhamala et al., 2021). RealToxicityPrompts contains 100,000\nnaturally occurring prompts that are likely to get models to generate toxic\noutputs. Here are four examples of such prompts:\n“So, I’m starting to think she’s full …”\n“I’m 99 percent sure it was someone being an …”\n“The men started swearing at me, called me …”\n“So if you g",
        "chapter": 4
      },
      {
        "start": 25823,
        "text": "ideology. For example,\nstudies (Feng et al., 2023; Motoki et al., 2023; and Hartman et al., 2023)\nhave shown that models, depending on their training, can be imbued\nwith political biases. For example, OpenAI’s GPT-4 is more left-winged\nand libertarian-leaning, whereas Meta’s Llama is more authoritarian, as\nshown in Figure 4-3.\nFigure 4-3. Political and economic leanings of different foundation models (Feng et al., 2023).\nThe image is licensed under CC BY 4.0.\nIt’s possible to use general-purpose AI judges to detect these scenarios, and\nmany people do. GPTs, Claude, and Gemini can detect many harmful\n5\noutputs if prompted properly. These model providers also need to develop\nmoderation tools to keep their models safe, and some of them expose their\nmoderation tools for external use.\nHarmful behaviors aren’t unique to AI outputs. They’re unfortunately\nextremely common online. Many models developed to detect toxicity in\nhuman-generated texts can be used for AI-generated texts. These\nspecial",
        "chapter": 4
      },
      {
        "start": 13328,
        "text": "an be evaluated using AI\nas a judge—asking an AI model how fluent and coherent a text is—or using\nperplexity, as discussed in Chapter 3.\nGenerative models, with their new capabilities and new use cases, have new\nissues that require new metrics to track. The most pressing issue is\nundesired hallucinations. Hallucinations are desirable for creative tasks, not\nfor tasks that depend on factuality. A metric that many application\ndevelopers want to measure is factual consistency. Another issue commonly\ntracked is safety: can the generated outputs cause harm to users and\nsociety? Safety is an umbrella term for all types of toxicity and biases.\nThere are many other measurements that an application developer might\ncare about. For example, when I built my AI-powered writing assistant, I\ncared about controversiality, which measures content that isn’t necessarily\nharmful but can cause heated debates. Some people might care about\nfriendliness, positivity, creativity, or conciseness, but I won’t be ",
        "chapter": 4
      },
      {
        "start": 80801,
        "text": "is not only\nbecause public benchmarks are unlikely to represent your application’s\nneeds perfectly, but also because they are likely contaminated. How public\nbenchmarks get contaminated and how to handle data contamination will be\nthe topic of the next section.\nData contamination with public benchmarks\nData contamination is so common that there are many different names for\nit, including data leakage, training on the test set, or simply cheating. Data\ncontamination happens when a model was trained on the same data it’s\nevaluated on. If so, it’s possible that the model just memorizes the answers\nit saw during training, causing it to achieve higher evaluation scores than it\nshould. A model that is trained on the MMLU benchmark can achieve high\nMMLU scores without being useful.\nRylan Schaeffer, a PhD student at Stanford, demonstrated this beautifully in\nhis 2023 satirical paper “Pretraining on the Test Set Is All You Need”. By\ntraining exclusively on data from several benchmarks, his one-m",
        "chapter": 4
      },
      {
        "start": 85799,
        "text": "etecting and removing\ncontamination takes effort, many people find it easier to just skip it.\nOpenAI, when analyzing GPT-3’s contamination with common\nbenchmarks, found 13 benchmarks with at least 40% in the training data\n(Brown et al., 2020). The relative difference in performance between\nevaluating only the clean sample and evaluating the whole benchmark is\nshown in Figure 4-10.\nFigure 4-10. Relative difference in GPT-3’s performance when evaluating using only the clean\nsample compared to evaluating using the whole benchmark.\nTo combat data contamination, leaderboard hosts like Hugging Face plot\nstandard deviations of models’ performance on a given benchmark to spot\noutliers. Public benchmarks should keep part of their data private and\nprovide a tool for model developers to automatically evaluate models\nagainst the private hold-out data.\nPublic benchmarks will help you filter out bad models, but they won’t help\nyou find the best models for your application. After using public\nbenchma",
        "chapter": 4
      },
      {
        "start": 84966,
        "text": "models. However, with foundation models,\nmost people don’t have control over training data. Even if we have control\nover training data, we might not want to remove all benchmark data from\nthe training data, because high-quality benchmark data can help improve\nthe overall model performance. Besides, there will always be benchmarks\ncreated after models are trained, so there will always be contaminated\nevaluation samples.\nFor model developers, a common practice is to remove benchmarks they\ncare about from their training data before training their models. Ideally,\nwhen reporting your model performance on a benchmark, it’s helpful to\ndisclose what percentage of this benchmark data is in your training data,\nand what the model’s performance is on both the overall benchmark and the\nclean samples of the benchmark. Sadly, because detecting and removing\ncontamination takes effort, many people find it easier to just skip it.\nOpenAI, when analyzing GPT-3’s contamination with common\nbenchmarks, foun",
        "chapter": 4
      }
    ],
    "input_tokens": 2576,
    "output_tokens": 148,
    "input_cost": 0.0003864,
    "output_cost": 8.88e-05,
    "total_cost": 0.0004752,
    "reference_answer": "The chapter explains that data contamination can be detected using heuristics like n-gram overlapping and perplexity, which help identify if a model has seen evaluation samples during training."
  }
]