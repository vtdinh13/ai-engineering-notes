{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a7b1bd",
   "metadata": {},
   "source": [
    "This notebook demonstrates a simple synchronous RAG smoke test:\n",
    "\n",
    "- It loads a handful of ground-truth questions from sample_gt.csv into test_dict, giving you a short list of prompts to exercise the pipeline.\n",
    "`ProcessChunks` is instantiated with default_config, then get_chunks/embed_chunks/index_chunks run sequentially over a few PDF pages to build the in-memory VectorSearch. The tqdm progress bar reflects the blocking loop, so you can see embedding progress even in a synchronous run.\n",
    "\n",
    "- The test loop is a plain for over test_dict wrapped with tqdm. Each iteration calls run_rag(question) synchronously, waits for the OpenAI response, and appends the resulting RAGResult to all_results. Because the work happens serially, the bar advances after every completed call, giving you immediate feedback on latency per query.\n",
    "\n",
    "- Finally, it converts all_results into a pandas DataFrame to inspect question, answer, retrieved context, and the token/cost metrics populated inside RAGResult. That table is the quick health check confirming the synchronous pipeline works end-to-end before trying any async batching or larger evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86ef9c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'performance differences in model APIs',\n",
       " 'summary_answer': 'The excerpt mentions that the same AI model may perform differently across various APIs due to optimization techniques used, which necessitates thorough testing when switching APIs.',\n",
       " 'difficulty': 'intermediate',\n",
       " 'text': 'After developing a model, a developer can choose to open source it, make it\\naccessible via an API, or both. Many model developers are also model\\nservice providers. Cohere and Mistral open source some models and\\nprovide APIs for some. OpenAI is typically known for their commercial\\nmodels, but they’ve also open sourced models (GPT-2, CLIP). Typically,\\nmodel providers open source weaker models and keep their best models\\nbehind paywalls, either via APIs or to power their products.\\nModel APIs can be available through model providers (such as OpenAI and\\nAnthropic), cloud service providers (such as Azure and GCP [Google Cloud\\nPlatform]), or third-party API providers (such as Databricks Mosaic,\\nAnyscale, etc.). The same model can be available through different APIs\\nwith different features, constraints, and pricings. For example, GPT-4 is\\navailable through both OpenAI and Azure APIs. There might be slight\\ndifferences in the performance of the same model provided through\\ndifferent APIs, as different APIs might use different techniques to optimize\\nthis model, so make sure to run thorough tests when you switch between\\nmodel APIs.\\nCommercial models are only accessible via APIs licensed by the model\\n13\\ndevelopers. Open source models can be supported by any API provider,\\nallowing you to pick and choose the provider that works best for you. For\\ncommercial model providers, models are their competitive advantages. For\\nAPI providers that don’t have their own models, APIs are their competitive',\n",
       " 'id': 'db22bdaa'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gt_sample = pd.read_csv(\"sample_gt.csv\")\n",
    "gt_dict = gt_sample.to_dict(orient=\"records\")\n",
    "gt_dict[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b61d3fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'importance of reliable benchmarks in AI',\n",
       "  'summary_answer': 'It emphasizes that many benchmarks may not accurately measure the intended metrics, stressing the need for careful evaluation.',\n",
       "  'difficulty': 'beginner',\n",
       "  'text': 'writing benchmarks. As new benchmarks are constantly introduced and old\\nbenchmarks become saturated, you should look for the latest benchmarks.\\nMake sure to evaluate how reliable a benchmark is. Because anyone can\\ncreate and publish a benchmark, many benchmarks might not be measuring\\nwhat you expect them to measure.',\n",
       "  'id': 'a762b76e'},\n",
       " {'question': 'what is evaluation-driven development in ai?',\n",
       "  'summary_answer': 'Evaluation-driven development refers to the approach of establishing evaluation criteria before building an AI application, similar to how test-driven development focuses on writing tests before code.',\n",
       "  'difficulty': 'beginner',\n",
       "  'text': 'Before investing time, money, and resources into building an application,\\nit’s important to understand how this application will be evaluated. I call\\nthis approach evaluation-driven development. The name is inspired by test-\\ndriven development in software engineering, which refers to the method of\\nwriting tests before writing code. In AI engineering, evaluation-driven\\ndevelopment means defining evaluation criteria before building.',\n",
       "  'id': 'aa6103f5'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict = gt_dict[:2]\n",
    "test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d56637da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vancescadinh/Documents/AI/ai-engineering-notes/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 11/11 [00:01<00:00,  8.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from rag import ProcessChunks, default_config\n",
    "\n",
    "pc = ProcessChunks(config=default_config)\n",
    "chunks = pc.get_chunks(300, 250, start_page=1, end_page=3)\n",
    "embeddings = pc.embed_chunks(chunks)\n",
    "vector_index = pc.index_chunks(embeddings, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2330f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# test_question = 'importance of reliable benchmarks in AI'\n",
    "\n",
    "# from rag import search\n",
    "# import json\n",
    "\n",
    "# result = search(user_query=test_question)\n",
    "# print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d16c9a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:11<00:00,  5.70s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RAGResult(question='importance of reliable benchmarks in AI', answer=\"Reliable benchmarks in AI are crucial for several reasons:\\n\\n1. **Model Selection**: With an overwhelming number of foundation models available, benchmarks help in evaluating and comparing these models based on different criteria relevant to specific applications. This aids teams in making informed choices regarding which models to deploy.\\n\\n2. **Evaluation of Performance**: Trustworthy benchmarks enable developers to assess how well a model performs on tasks critical to its intended uses, such as factual consistency, domain-specific capabilities (like math and reasoning), and overall effectiveness in real-world applications.\\n\\n3. **Visibility and Evaluation**: Many AI applications struggle with uncertain returns on investment, often due to a lack of clarity around how these applications are performing. Reliable benchmarks provide a means for continuous evaluation, ensuring that deployed models can be monitored effectively.\\n\\n4. **Guidance Over Time**: Establishing a proper evaluation pipeline using benchmarks helps in tracking the performance of an application over time, which is vital for improvement and adaptation in response to user feedback or changes in application needs.\\n\\n5. **Avoiding Misleading Outcomes**: Utilizing benchmarks prevents the common pitfall of deploying models without understanding their effectiveness, which can result in unnecessary costs and lack of actionable insights regarding the model's deployment.\\n\\nIn summary, reliable benchmarks are essential for ensuring that AI models not only function as expected but also contribute positively to their applications, allowing developers to evaluate, optimize, and adapt models effectively.\", context=[{'start': 750, 'text': 'of\\nfoundation models to choose from, it can feel overwhelming to choose the\\nright model for your application. Thousands of benchmarks have been\\nintroduced to evaluate these models along different criteria. Can these\\nbenchmarks be trusted? How do you select what benchmarks to use? How\\nabout public le', 'chapter': 4}, {'start': 500, 'text': 'e, many people worry about AI making\\nup facts—how is factual consistency detected? How are domain-specific\\ncapabilities like math, science, reasoning, and summarization measured?\\nThe second part focuses on model selection. Given an increasing number of\\nfoundation models to choose from, it can feel o', 'chapter': 4}, {'start': 2000, 'text': 'st even more.\\nAI applications with questionable returns on investment are, unfortunately,\\nquite common. This happens not only because the application is hard to\\nevaluate but also because application developers don’t have visibility into\\nhow their applications are being used. An ML engineer at a used', 'chapter': 4}, {'start': 1000, 'text': 'select what benchmarks to use? How\\nabout public leaderboards that aggregate multiple benchmarks?\\nThe model landscape is teeming with proprietary models and open source\\nmodels. A question many teams will need to visit over and over again is\\nwhether to host their own models or to use a model API. This', 'chapter': 4}, {'start': 1750, 'text': 'eployed but no one knows whether it’s working? When I\\nasked this question at conferences, most people said the latter. An\\napplication that is deployed but can’t be evaluated is worse. It costs to\\nmaintain, but if you want to take it down, it might cost even more.\\nAI applications with questionable re', 'chapter': 4}, {'start': 0, 'text': 'Chapter 4. Evaluate AI Systems\\nA model is only useful if it works for its intended purposes. You need to\\nevaluate models in the context of your application. Chapter 3 discusses\\ndifferent approaches to automatic evaluation. This chapter discusses how to\\nuse these approaches to evaluate models for you', 'chapter': 4}, {'start': 250, 'text': 'to\\nuse these approaches to evaluate models for your applications.\\nThis chapter contains three parts. It starts with a discussion of the criteria\\nyou might use to evaluate your applications and how these criteria are\\ndefined and calculated. For example, many people worry about AI making\\nup facts—how ', 'chapter': 4}, {'start': 1500, 'text': 'nt of your application over time. This part brings together the\\ntechniques we’ve learned throughout the book to evaluate concrete\\napplications.\\nEvaluation Criteria\\nWhich is worse—an application that has never been deployed or an\\napplication that is deployed but no one knows whether it’s working? Whe', 'chapter': 4}, {'start': 1250, 'text': ' host their own models or to use a model API. This question has\\nbecome more nuanced with the introduction of model API services built on\\ntop of open source models.\\nThe last part discusses developing an evaluation pipeline that can guide the\\ndevelopment of your application over time. This part brings', 'chapter': 4}, {'start': 2250, 'text': 'lications are being used. An ML engineer at a used car\\ndealership told me that his team built a model to predict the value of a car\\nbased on the specs given by the owner. A year after the model was\\ndeployed, their users seemed to like the feature, but he had no idea if the\\nmodel’s predictions were a', 'chapter': 4}], input_tokens=910, output_tokens=282, input_cost=Decimal('0.0001365'), output_cost=Decimal('0.0001692'), total_cost=Decimal('0.0003057')),\n",
       " RAGResult(question='what is evaluation-driven development in ai?', answer='**Evaluation-driven development** in AI is an approach that emphasizes the importance of evaluating AI models in the context of their specific applications. This concept is fundamental to ensuring that a model is genuinely effective for its intended purpose.\\n\\nIn this framework, the evaluation process includes:\\n\\n1. **Defining Evaluation Criteria:** Establishing clear criteria that are relevant to the application, which could include aspects like factual consistency, reasoning abilities, and domain-specific capabilities.\\n\\n2. **Continuous Assessment:** Regularly evaluating models to monitor their performance and ensure they meet the defined criteria over time. This is crucial to avoid situations where an application is deployed without a clear understanding of its effectiveness.\\n\\n3. **Feedback Loop:** Utilizing the evaluation results to guide ongoing development and refinement of the application. This helps in making informed decisions about model selection and modifications based on real-world usage.\\n\\nUltimately, evaluation-driven development aims to bridge the gap between model performance and user satisfaction, ensuring that AI applications deliver tangible value.', context=[{'start': 2000, 'text': 'st even more.\\nAI applications with questionable returns on investment are, unfortunately,\\nquite common. This happens not only because the application is hard to\\nevaluate but also because application developers don’t have visibility into\\nhow their applications are being used. An ML engineer at a used', 'chapter': 4}, {'start': 0, 'text': 'Chapter 4. Evaluate AI Systems\\nA model is only useful if it works for its intended purposes. You need to\\nevaluate models in the context of your application. Chapter 3 discusses\\ndifferent approaches to automatic evaluation. This chapter discusses how to\\nuse these approaches to evaluate models for you', 'chapter': 4}, {'start': 250, 'text': 'to\\nuse these approaches to evaluate models for your applications.\\nThis chapter contains three parts. It starts with a discussion of the criteria\\nyou might use to evaluate your applications and how these criteria are\\ndefined and calculated. For example, many people worry about AI making\\nup facts—how ', 'chapter': 4}, {'start': 1750, 'text': 'eployed but no one knows whether it’s working? When I\\nasked this question at conferences, most people said the latter. An\\napplication that is deployed but can’t be evaluated is worse. It costs to\\nmaintain, but if you want to take it down, it might cost even more.\\nAI applications with questionable re', 'chapter': 4}, {'start': 1500, 'text': 'nt of your application over time. This part brings together the\\ntechniques we’ve learned throughout the book to evaluate concrete\\napplications.\\nEvaluation Criteria\\nWhich is worse—an application that has never been deployed or an\\napplication that is deployed but no one knows whether it’s working? Whe', 'chapter': 4}, {'start': 500, 'text': 'e, many people worry about AI making\\nup facts—how is factual consistency detected? How are domain-specific\\ncapabilities like math, science, reasoning, and summarization measured?\\nThe second part focuses on model selection. Given an increasing number of\\nfoundation models to choose from, it can feel o', 'chapter': 4}, {'start': 1250, 'text': ' host their own models or to use a model API. This question has\\nbecome more nuanced with the introduction of model API services built on\\ntop of open source models.\\nThe last part discusses developing an evaluation pipeline that can guide the\\ndevelopment of your application over time. This part brings', 'chapter': 4}, {'start': 2250, 'text': 'lications are being used. An ML engineer at a used car\\ndealership told me that his team built a model to predict the value of a car\\nbased on the specs given by the owner. A year after the model was\\ndeployed, their users seemed to like the feature, but he had no idea if the\\nmodel’s predictions were a', 'chapter': 4}, {'start': 750, 'text': 'of\\nfoundation models to choose from, it can feel overwhelming to choose the\\nright model for your application. Thousands of benchmarks have been\\nintroduced to evaluate these models along different criteria. Can these\\nbenchmarks be trusted? How do you select what benchmarks to use? How\\nabout public le', 'chapter': 4}, {'start': 1000, 'text': 'select what benchmarks to use? How\\nabout public leaderboards that aggregate multiple benchmarks?\\nThe model landscape is teeming with proprietary models and open source\\nmodels. A question many teams will need to visit over and over again is\\nwhether to host their own models or to use a model API. This', 'chapter': 4}], input_tokens=911, output_tokens=193, input_cost=Decimal('0.00013665'), output_cost=Decimal('0.0001158'), total_cost=Decimal('0.00025245'))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rag import run_rag\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_results = []\n",
    "for q in tqdm(test_dict, total=len(test_dict)):\n",
    "    result = run_rag(q['question'])\n",
    "    all_results.append(result)\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6af418d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>context</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "      <th>input_cost</th>\n",
       "      <th>output_cost</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>importance of reliable benchmarks in AI</td>\n",
       "      <td>Reliable benchmarks in AI are crucial for seve...</td>\n",
       "      <td>[{'start': 750, 'text': 'of\n",
       "foundation models ...</td>\n",
       "      <td>910</td>\n",
       "      <td>282</td>\n",
       "      <td>0.0001365</td>\n",
       "      <td>0.0001692</td>\n",
       "      <td>0.0003057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is evaluation-driven development in ai?</td>\n",
       "      <td>**Evaluation-driven development** in AI is an ...</td>\n",
       "      <td>[{'start': 2000, 'text': 'st even more.\n",
       "AI app...</td>\n",
       "      <td>911</td>\n",
       "      <td>193</td>\n",
       "      <td>0.00013665</td>\n",
       "      <td>0.0001158</td>\n",
       "      <td>0.00025245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       question  \\\n",
       "0       importance of reliable benchmarks in AI   \n",
       "1  what is evaluation-driven development in ai?   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Reliable benchmarks in AI are crucial for seve...   \n",
       "1  **Evaluation-driven development** in AI is an ...   \n",
       "\n",
       "                                             context  input_tokens  \\\n",
       "0  [{'start': 750, 'text': 'of\n",
       "foundation models ...           910   \n",
       "1  [{'start': 2000, 'text': 'st even more.\n",
       "AI app...           911   \n",
       "\n",
       "   output_tokens  input_cost output_cost  total_cost  \n",
       "0            282   0.0001365   0.0001692   0.0003057  \n",
       "1            193  0.00013665   0.0001158  0.00025245  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e435067e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-notes (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
