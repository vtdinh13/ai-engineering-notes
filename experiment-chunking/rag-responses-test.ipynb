{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a7b1bd",
   "metadata": {},
   "source": [
    "This notebook demonstrates a simple synchronous RAG smoke test:\n",
    "\n",
    "- It loads a handful of ground-truth questions from sample_gt.csv into test_dict, giving you a short list of prompts to exercise the pipeline.\n",
    "`ProcessChunks` is instantiated with default_config, then get_chunks/embed_chunks/index_chunks run sequentially over a few PDF pages to build the in-memory VectorSearch. The tqdm progress bar reflects the blocking loop, so you can see embedding progress even in a synchronous run.\n",
    "\n",
    "- The test loop is a plain for over test_dict wrapped with tqdm. Each iteration calls run_rag(question) synchronously, waits for the OpenAI response, and appends the resulting RAGResult to all_results. Because the work happens serially, the bar advances after every completed call, giving you immediate feedback on latency per query.\n",
    "\n",
    "- Finally, it converts all_results into a pandas DataFrame to inspect question, answer, retrieved context, and the token/cost metrics populated inside RAGResult. That table is the quick health check confirming the synchronous pipeline works end-to-end before trying any async batching or larger evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86ef9c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'performance differences in model APIs',\n",
       " 'summary_answer': 'The excerpt mentions that the same AI model may perform differently across various APIs due to optimization techniques used, which necessitates thorough testing when switching APIs.',\n",
       " 'difficulty': 'intermediate',\n",
       " 'text': 'After developing a model, a developer can choose to open source it, make it\\naccessible via an API, or both. Many model developers are also model\\nservice providers. Cohere and Mistral open source some models and\\nprovide APIs for some. OpenAI is typically known for their commercial\\nmodels, but they’ve also open sourced models (GPT-2, CLIP). Typically,\\nmodel providers open source weaker models and keep their best models\\nbehind paywalls, either via APIs or to power their products.\\nModel APIs can be available through model providers (such as OpenAI and\\nAnthropic), cloud service providers (such as Azure and GCP [Google Cloud\\nPlatform]), or third-party API providers (such as Databricks Mosaic,\\nAnyscale, etc.). The same model can be available through different APIs\\nwith different features, constraints, and pricings. For example, GPT-4 is\\navailable through both OpenAI and Azure APIs. There might be slight\\ndifferences in the performance of the same model provided through\\ndifferent APIs, as different APIs might use different techniques to optimize\\nthis model, so make sure to run thorough tests when you switch between\\nmodel APIs.\\nCommercial models are only accessible via APIs licensed by the model\\n13\\ndevelopers. Open source models can be supported by any API provider,\\nallowing you to pick and choose the provider that works best for you. For\\ncommercial model providers, models are their competitive advantages. For\\nAPI providers that don’t have their own models, APIs are their competitive',\n",
       " 'id': 'db22bdaa'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gt_sample = pd.read_csv(\"sample_gt.csv\")\n",
    "gt_dict = gt_sample.to_dict(orient=\"records\")\n",
    "gt_dict[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b61d3fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'importance of reliable benchmarks in AI',\n",
       "  'summary_answer': 'It emphasizes that many benchmarks may not accurately measure the intended metrics, stressing the need for careful evaluation.',\n",
       "  'difficulty': 'beginner',\n",
       "  'text': 'writing benchmarks. As new benchmarks are constantly introduced and old\\nbenchmarks become saturated, you should look for the latest benchmarks.\\nMake sure to evaluate how reliable a benchmark is. Because anyone can\\ncreate and publish a benchmark, many benchmarks might not be measuring\\nwhat you expect them to measure.',\n",
       "  'id': 'a762b76e'},\n",
       " {'question': 'what is evaluation-driven development in ai?',\n",
       "  'summary_answer': 'Evaluation-driven development refers to the approach of establishing evaluation criteria before building an AI application, similar to how test-driven development focuses on writing tests before code.',\n",
       "  'difficulty': 'beginner',\n",
       "  'text': 'Before investing time, money, and resources into building an application,\\nit’s important to understand how this application will be evaluated. I call\\nthis approach evaluation-driven development. The name is inspired by test-\\ndriven development in software engineering, which refers to the method of\\nwriting tests before writing code. In AI engineering, evaluation-driven\\ndevelopment means defining evaluation criteria before building.',\n",
       "  'id': 'aa6103f5'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict = gt_dict[:2]\n",
    "test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d56637da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:02<00:00,  4.94it/s]\n"
     ]
    }
   ],
   "source": [
    "from rag import ProcessChunks, default_config\n",
    "\n",
    "pc = ProcessChunks(config=default_config)\n",
    "chunks = pc.get_chunks(300, 250, start_page=1, end_page=3)\n",
    "embeddings = pc.embed_chunks(chunks)\n",
    "vector_index = pc.index_chunks(embeddings, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2330f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# test_question = 'importance of reliable benchmarks in AI'\n",
    "\n",
    "# from rag import search\n",
    "# import json\n",
    "\n",
    "# result = search(user_query=test_question)\n",
    "# print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d16c9a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:11<00:00,  5.70s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RAGResult(question='importance of reliable benchmarks in AI', answer=\"Reliable benchmarks in AI are crucial for several reasons:\\n\\n1. **Model Selection**: With an overwhelming number of foundation models available, benchmarks help in evaluating and comparing these models based on different criteria relevant to specific applications. This aids teams in making informed choices regarding which models to deploy.\\n\\n2. **Evaluation of Performance**: Trustworthy benchmarks enable developers to assess how well a model performs on tasks critical to its intended uses, such as factual consistency, domain-specific capabilities (like math and reasoning), and overall effectiveness in real-world applications.\\n\\n3. **Visibility and Evaluation**: Many AI applications struggle with uncertain returns on investment, often due to a lack of clarity around how these applications are performing. Reliable benchmarks provide a means for continuous evaluation, ensuring that deployed models can be monitored effectively.\\n\\n4. **Guidance Over Time**: Establishing a proper evaluation pipeline using benchmarks helps in tracking the performance of an application over time, which is vital for improvement and adaptation in response to user feedback or changes in application needs.\\n\\n5. **Avoiding Misleading Outcomes**: Utilizing benchmarks prevents the common pitfall of deploying models without understanding their effectiveness, which can result in unnecessary costs and lack of actionable insights regarding the model's deployment.\\n\\nIn summary, reliable benchmarks are essential for ensuring that AI models not only function as expected but also contribute positively to their applications, allowing developers to evaluate, optimize, and adapt models effectively.\", context=[{'start': 750, 'text': 'of\\nfoundation models to choose from, it can feel overwhelming to choose the\\nright model for your application. Thousands of benchmarks have been\\nintroduced to evaluate these models along different criteria. Can these\\nbenchmarks be trusted? How do you select what benchmarks to use? How\\nabout public le', 'chapter': 4}, {'start': 500, 'text': 'e, many people worry about AI making\\nup facts—how is factual consistency detected? How are domain-specific\\ncapabilities like math, science, reasoning, and summarization measured?\\nThe second part focuses on model selection. Given an increasing number of\\nfoundation models to choose from, it can feel o', 'chapter': 4}, {'start': 2000, 'text': 'st even more.\\nAI applications with questionable returns on investment are, unfortunately,\\nquite common. This happens not only because the application is hard to\\nevaluate but also because application developers don’t have visibility into\\nhow their applications are being used. An ML engineer at a used', 'chapter': 4}, {'start': 1000, 'text': 'select what benchmarks to use? How\\nabout public leaderboards that aggregate multiple benchmarks?\\nThe model landscape is teeming with proprietary models and open source\\nmodels. A question many teams will need to visit over and over again is\\nwhether to host their own models or to use a model API. This', 'chapter': 4}, {'start': 1750, 'text': 'eployed but no one knows whether it’s working? When I\\nasked this question at conferences, most people said the latter. An\\napplication that is deployed but can’t be evaluated is worse. It costs to\\nmaintain, but if you want to take it down, it might cost even more.\\nAI applications with questionable re', 'chapter': 4}, {'start': 0, 'text': 'Chapter 4. Evaluate AI Systems\\nA model is only useful if it works for its intended purposes. You need to\\nevaluate models in the context of your application. Chapter 3 discusses\\ndifferent approaches to automatic evaluation. This chapter discusses how to\\nuse these approaches to evaluate models for you', 'chapter': 4}, {'start': 250, 'text': 'to\\nuse these approaches to evaluate models for your applications.\\nThis chapter contains three parts. It starts with a discussion of the criteria\\nyou might use to evaluate your applications and how these criteria are\\ndefined and calculated. For example, many people worry about AI making\\nup facts—how ', 'chapter': 4}, {'start': 1500, 'text': 'nt of your application over time. This part brings together the\\ntechniques we’ve learned throughout the book to evaluate concrete\\napplications.\\nEvaluation Criteria\\nWhich is worse—an application that has never been deployed or an\\napplication that is deployed but no one knows whether it’s working? Whe', 'chapter': 4}, {'start': 1250, 'text': ' host their own models or to use a model API. This question has\\nbecome more nuanced with the introduction of model API services built on\\ntop of open source models.\\nThe last part discusses developing an evaluation pipeline that can guide the\\ndevelopment of your application over time. This part brings', 'chapter': 4}, {'start': 2250, 'text': 'lications are being used. An ML engineer at a used car\\ndealership told me that his team built a model to predict the value of a car\\nbased on the specs given by the owner. A year after the model was\\ndeployed, their users seemed to like the feature, but he had no idea if the\\nmodel’s predictions were a', 'chapter': 4}], input_tokens=910, output_tokens=282, input_cost=Decimal('0.0001365'), output_cost=Decimal('0.0001692'), total_cost=Decimal('0.0003057')),\n",
       " RAGResult(question='what is evaluation-driven development in ai?', answer='**Evaluation-driven development** in AI is an approach that emphasizes the importance of evaluating AI models in the context of their specific applications. This concept is fundamental to ensuring that a model is genuinely effective for its intended purpose.\\n\\nIn this framework, the evaluation process includes:\\n\\n1. **Defining Evaluation Criteria:** Establishing clear criteria that are relevant to the application, which could include aspects like factual consistency, reasoning abilities, and domain-specific capabilities.\\n\\n2. **Continuous Assessment:** Regularly evaluating models to monitor their performance and ensure they meet the defined criteria over time. This is crucial to avoid situations where an application is deployed without a clear understanding of its effectiveness.\\n\\n3. **Feedback Loop:** Utilizing the evaluation results to guide ongoing development and refinement of the application. This helps in making informed decisions about model selection and modifications based on real-world usage.\\n\\nUltimately, evaluation-driven development aims to bridge the gap between model performance and user satisfaction, ensuring that AI applications deliver tangible value.', context=[{'start': 2000, 'text': 'st even more.\\nAI applications with questionable returns on investment are, unfortunately,\\nquite common. This happens not only because the application is hard to\\nevaluate but also because application developers don’t have visibility into\\nhow their applications are being used. An ML engineer at a used', 'chapter': 4}, {'start': 0, 'text': 'Chapter 4. Evaluate AI Systems\\nA model is only useful if it works for its intended purposes. You need to\\nevaluate models in the context of your application. Chapter 3 discusses\\ndifferent approaches to automatic evaluation. This chapter discusses how to\\nuse these approaches to evaluate models for you', 'chapter': 4}, {'start': 250, 'text': 'to\\nuse these approaches to evaluate models for your applications.\\nThis chapter contains three parts. It starts with a discussion of the criteria\\nyou might use to evaluate your applications and how these criteria are\\ndefined and calculated. For example, many people worry about AI making\\nup facts—how ', 'chapter': 4}, {'start': 1750, 'text': 'eployed but no one knows whether it’s working? When I\\nasked this question at conferences, most people said the latter. An\\napplication that is deployed but can’t be evaluated is worse. It costs to\\nmaintain, but if you want to take it down, it might cost even more.\\nAI applications with questionable re', 'chapter': 4}, {'start': 1500, 'text': 'nt of your application over time. This part brings together the\\ntechniques we’ve learned throughout the book to evaluate concrete\\napplications.\\nEvaluation Criteria\\nWhich is worse—an application that has never been deployed or an\\napplication that is deployed but no one knows whether it’s working? Whe', 'chapter': 4}, {'start': 500, 'text': 'e, many people worry about AI making\\nup facts—how is factual consistency detected? How are domain-specific\\ncapabilities like math, science, reasoning, and summarization measured?\\nThe second part focuses on model selection. Given an increasing number of\\nfoundation models to choose from, it can feel o', 'chapter': 4}, {'start': 1250, 'text': ' host their own models or to use a model API. This question has\\nbecome more nuanced with the introduction of model API services built on\\ntop of open source models.\\nThe last part discusses developing an evaluation pipeline that can guide the\\ndevelopment of your application over time. This part brings', 'chapter': 4}, {'start': 2250, 'text': 'lications are being used. An ML engineer at a used car\\ndealership told me that his team built a model to predict the value of a car\\nbased on the specs given by the owner. A year after the model was\\ndeployed, their users seemed to like the feature, but he had no idea if the\\nmodel’s predictions were a', 'chapter': 4}, {'start': 750, 'text': 'of\\nfoundation models to choose from, it can feel overwhelming to choose the\\nright model for your application. Thousands of benchmarks have been\\nintroduced to evaluate these models along different criteria. Can these\\nbenchmarks be trusted? How do you select what benchmarks to use? How\\nabout public le', 'chapter': 4}, {'start': 1000, 'text': 'select what benchmarks to use? How\\nabout public leaderboards that aggregate multiple benchmarks?\\nThe model landscape is teeming with proprietary models and open source\\nmodels. A question many teams will need to visit over and over again is\\nwhether to host their own models or to use a model API. This', 'chapter': 4}], input_tokens=911, output_tokens=193, input_cost=Decimal('0.00013665'), output_cost=Decimal('0.0001158'), total_cost=Decimal('0.00025245'))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rag import run_rag\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_results = []\n",
    "for q in tqdm(test_dict, total=len(test_dict)):\n",
    "    result = run_rag(q['question'])\n",
    "    all_results.append(result)\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6af418d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>context</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "      <th>input_cost</th>\n",
       "      <th>output_cost</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>importance of reliable benchmarks in AI</td>\n",
       "      <td>Reliable benchmarks in AI are crucial for seve...</td>\n",
       "      <td>[{'start': 750, 'text': 'of\n",
       "foundation models ...</td>\n",
       "      <td>910</td>\n",
       "      <td>282</td>\n",
       "      <td>0.0001365</td>\n",
       "      <td>0.0001692</td>\n",
       "      <td>0.0003057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is evaluation-driven development in ai?</td>\n",
       "      <td>**Evaluation-driven development** in AI is an ...</td>\n",
       "      <td>[{'start': 2000, 'text': 'st even more.\n",
       "AI app...</td>\n",
       "      <td>911</td>\n",
       "      <td>193</td>\n",
       "      <td>0.00013665</td>\n",
       "      <td>0.0001158</td>\n",
       "      <td>0.00025245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       question  \\\n",
       "0       importance of reliable benchmarks in AI   \n",
       "1  what is evaluation-driven development in ai?   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Reliable benchmarks in AI are crucial for seve...   \n",
       "1  **Evaluation-driven development** in AI is an ...   \n",
       "\n",
       "                                             context  input_tokens  \\\n",
       "0  [{'start': 750, 'text': 'of\n",
       "foundation models ...           910   \n",
       "1  [{'start': 2000, 'text': 'st even more.\n",
       "AI app...           911   \n",
       "\n",
       "   output_tokens  input_cost output_cost  total_cost  \n",
       "0            282   0.0001365   0.0001692   0.0003057  \n",
       "1            193  0.00013665   0.0001158  0.00025245  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac1b9c7",
   "metadata": {},
   "source": [
    "# Asyncrony\n",
    "1. ThreadPoolExecutor\n",
    "    - ThreadPoolExecutor spins up worker threads and runs your blocking function (e.g., ask_llm) in parallel. Each thread still blocks while waiting for I/O, but other threads keep working, so you get concurrency without rewriting your functions—just submit callables to the pool. \n",
    "        - threads = drop-in parallelism for blocking code; asyncio = cooperative concurrency for non-blocking code with more control but higher upfront integration cost.\n",
    "        - you spin up several helper workers (threads). Each worker runs a normal function. When one worker blocks on a slow API call, another worker can keep going. It’s easy—wrap your existing function in pool.submit. Downsides: threads consume more memory, and you can’t easily fine-tune rate limits or cancellation.\n",
    "2. Full async (asyncio) keeps everything in a single thread with an event loop. Functions become async def, they await network calls, and the loop interleaves tasks cooperatively. It’s more scalable (no thread overhead, easier to throttle) and gives finer control over concurrency, but requires async-aware libraries (OpenAI async client, async HTTP, etc.) and more refactoring.\n",
    "    - you stay in one worker but slice your time carefully. Functions become async def, and whenever they hit I/O they await it; the event loop then lets another task run meanwhile. It’s lighter weight and gives you precise control (semaphores, cancellation, timeouts) but only works if the underlying libraries offer async APIs, and you have to refactor your code to use await.\n",
    "3. what's the difference between `results = map_progress(pool, page_docs, process_document)` and `results = [f.result() for f in tqdm(futures)]`\n",
    "    1. map_progress(pool, page_docs, process_document) wraps everything: it submits each doc, attaches a tqdm progress bar that updates as futures finish, waits for all .result() calls, and returns results in submission order. You get progress feedback plus consistent ordering in one helper.\n",
    "        - map_progress is a higher-level utility that bundles submission, live progress updates, and result collection\n",
    "        - Use map_progress when you want a reusable helper that handles submission, live progress updates, and preserving order—all you provide is the pool, your iterable, and the function. It’s great for batch jobs where you need consistent behavior every time.\n",
    "        - map_progress is your all-in-one helper. You hand it the executor, the list of work, and the function to run. It submits tasks, shows a live progress bar as each finishes, and hands back results in the same order you gave them—so you don’t have to write that glue code every time.\n",
    "        - use map_progress when you want a ready-made, consistent pattern for batch jobs\n",
    "\n",
    "    2. results = [f.result() for f in tqdm(futures)] assumes you already created futures elsewhere and just iterates over them sequentially. The tqdm bar only advances once each .result() returns (no as‑completed updates), and you’re responsible for building the futures list and tracking order yourself.\n",
    "        - the list comprehension is a manual, one-off way to wait on pre-existing futures\n",
    "        - use this list comprehension when: \n",
    "            - Futures are created elsewhere and you just need to wait for them.           \n",
    "            - You’re debugging or doing a quick one-off and don’t care about as-completed updates.\n",
    "            - You need custom ordering or handling that doesn’t fit the helper.\n",
    "        - The manual [f.result() for f in futures] approach is bare bones. You already built the futures somewhere else, and now you’re just waiting on them one by one. It’s fine for quick experiments or special cases, but there’s no built-in progress feedback or ordering guarantees beyond the sequence you iterate.\n",
    "        - use the simple loop when you need custom behavior or you already have the futures and just want to block until they’re done\n",
    "4. _client_local is a threading.local() object; each thread sees its own independent attributes on it.\n",
    "getattr(_client_local, \"client\", None) checks whether the current thread already has a client attribute. If not, it returns None.\n",
    "If client is missing, the function creates a fresh OpenAI() instance and stores it on _client_local.client. Because this storage lives inside threading.local(), each thread sets up its own client the first time it calls get_client.\n",
    "Subsequent calls on the same thread reuse that thread’s client, so there’s no shared httpx connection between threads—eliminating the “Already borrowed” runtime error.\n",
    "Finally, get_client returns the per-thread client for ask_llm to use.\n",
    "\n",
    "        ``` \n",
    "        _client_local = threading.local()\n",
    "\n",
    "        def get_client():\n",
    "            client = getattr(_client_local, \"client\", None)\n",
    "            if client is None:\n",
    "                client = OpenAI()\n",
    "                _client_local.client = client\n",
    "            return client\n",
    "\n",
    "        ```\n",
    "    - The leading underscore is just a convention: _client_local signals “module-private”–it’s meant for internal use inside rag.py, not exported as part of the public API. Functionally it’s the same as naming it client_local; the underscore just hints to readers that they shouldn’t access it from outside the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79ab828f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['importance of reliable benchmarks in AI',\n",
       " 'what is evaluation-driven development in ai?']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[q[\"question\"] for q in test_dict][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5436ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:09<00:00,  4.74s/it]\n"
     ]
    }
   ],
   "source": [
    "#test asyncrony\n",
    "\n",
    "from tqdm import tqdm\n",
    "from rag import run_rag\n",
    "\n",
    "questions = [q[\"question\"] for q in test_dict]\n",
    "\n",
    "def map_progress(pool, seq, f):\n",
    "    \"\"\"Map function f over seq using the provided executor pool while\n",
    "    displaying a tqdm progress bar. Returns a list of results in submission order.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    with tqdm(total=len(seq)) as progress:\n",
    "        futures = []\n",
    "    \n",
    "        for el in seq:\n",
    "            future = pool.submit(f, el)\n",
    "            future.add_done_callback(lambda p: progress.update())\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=6) as pool:\n",
    "    results = map_progress(pool, questions, run_rag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6e62485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>context</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "      <th>input_cost</th>\n",
       "      <th>output_cost</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>importance of reliable benchmarks in AI</td>\n",
       "      <td>Reliable benchmarks in AI are essential for se...</td>\n",
       "      <td>[{'start': 750, 'text': 'of\n",
       "foundation models ...</td>\n",
       "      <td>910</td>\n",
       "      <td>230</td>\n",
       "      <td>0.0001365</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.0002745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is evaluation-driven development in ai?</td>\n",
       "      <td>Evaluation-driven development in AI focuses on...</td>\n",
       "      <td>[{'start': 2000, 'text': 'st even more.\n",
       "AI app...</td>\n",
       "      <td>911</td>\n",
       "      <td>199</td>\n",
       "      <td>0.00013665</td>\n",
       "      <td>0.0001194</td>\n",
       "      <td>0.00025605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       question  \\\n",
       "0       importance of reliable benchmarks in AI   \n",
       "1  what is evaluation-driven development in ai?   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Reliable benchmarks in AI are essential for se...   \n",
       "1  Evaluation-driven development in AI focuses on...   \n",
       "\n",
       "                                             context  input_tokens  \\\n",
       "0  [{'start': 750, 'text': 'of\n",
       "foundation models ...           910   \n",
       "1  [{'start': 2000, 'text': 'st even more.\n",
       "AI app...           911   \n",
       "\n",
       "   output_tokens  input_cost output_cost  total_cost  \n",
       "0            230   0.0001365    0.000138   0.0002745  \n",
       "1            199  0.00013665   0.0001194  0.00025605  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e435067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test asyncrony\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=6) as pool:\n",
    "    futures = [pool.submit(run_rag, q[\"question\"]) for q in test_dict]\n",
    "    results = [f.result() for f in tqdm(futures)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f818bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:07<00:00,  3.97s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RAGResult(question='importance of reliable benchmarks in AI', answer='Reliable benchmarks are crucial in AI for several reasons:\\n\\n1. **Model Evaluation**: Benchmarks provide a standardized way to evaluate the performance of different AI models. Given the vast landscape of foundation models, benchmarks help in comparing models based on consistent criteria. This enables engineers to select the most suitable model for a specific application by relying on trustworthy performance metrics.\\n\\n2. **Guided Model Selection**: With an increasing number of models to choose from, benchmarks assist developers in deciding which models to use. They help in filtering down options based on the application’s requirements, thus making the selection process more efficient and informed.\\n\\n3. **Visibility into Performance**: Reliable benchmarks offer insights into how AI applications are functioning. When applications are deployed, knowing whether they are working effectively is critical. Without this knowledge, teams might continue maintaining applications that are underperforming or misaligned with their goals.\\n\\n4. **Evaluation Over Time**: Benchmarks also facilitate the development of evaluation pipelines, which are essential for tracking the performance of models throughout the lifecycle of an application. This ongoing evaluation can identify areas for improvement and ensure models remain relevant and effective.\\n\\n5. **Trust and Consistency**: The integrity of benchmarks is vital; they need to be trustworthy to guide decisions. The increasing proliferation of benchmarks raises concerns about which ones to trust. Therefore, selecting reliable benchmarks is essential for ensuring consistent and accurate evaluations across different models and applications.\\n\\nIn essence, reliable benchmarks are foundational for effective AI engineering, enabling informed decision-making and continuous improvement in AI applications.', context=[{'start': 750, 'text': 'of\\nfoundation models to choose from, it can feel overwhelming to choose the\\nright model for your application. Thousands of benchmarks have been\\nintroduced to evaluate these models along different criteria. Can these\\nbenchmarks be trusted? How do you select what benchmarks to use? How\\nabout public le', 'chapter': 4}, {'start': 500, 'text': 'e, many people worry about AI making\\nup facts—how is factual consistency detected? How are domain-specific\\ncapabilities like math, science, reasoning, and summarization measured?\\nThe second part focuses on model selection. Given an increasing number of\\nfoundation models to choose from, it can feel o', 'chapter': 4}, {'start': 2000, 'text': 'st even more.\\nAI applications with questionable returns on investment are, unfortunately,\\nquite common. This happens not only because the application is hard to\\nevaluate but also because application developers don’t have visibility into\\nhow their applications are being used. An ML engineer at a used', 'chapter': 4}, {'start': 1000, 'text': 'select what benchmarks to use? How\\nabout public leaderboards that aggregate multiple benchmarks?\\nThe model landscape is teeming with proprietary models and open source\\nmodels. A question many teams will need to visit over and over again is\\nwhether to host their own models or to use a model API. This', 'chapter': 4}, {'start': 1750, 'text': 'eployed but no one knows whether it’s working? When I\\nasked this question at conferences, most people said the latter. An\\napplication that is deployed but can’t be evaluated is worse. It costs to\\nmaintain, but if you want to take it down, it might cost even more.\\nAI applications with questionable re', 'chapter': 4}, {'start': 0, 'text': 'Chapter 4. Evaluate AI Systems\\nA model is only useful if it works for its intended purposes. You need to\\nevaluate models in the context of your application. Chapter 3 discusses\\ndifferent approaches to automatic evaluation. This chapter discusses how to\\nuse these approaches to evaluate models for you', 'chapter': 4}, {'start': 250, 'text': 'to\\nuse these approaches to evaluate models for your applications.\\nThis chapter contains three parts. It starts with a discussion of the criteria\\nyou might use to evaluate your applications and how these criteria are\\ndefined and calculated. For example, many people worry about AI making\\nup facts—how ', 'chapter': 4}, {'start': 1500, 'text': 'nt of your application over time. This part brings together the\\ntechniques we’ve learned throughout the book to evaluate concrete\\napplications.\\nEvaluation Criteria\\nWhich is worse—an application that has never been deployed or an\\napplication that is deployed but no one knows whether it’s working? Whe', 'chapter': 4}, {'start': 1250, 'text': ' host their own models or to use a model API. This question has\\nbecome more nuanced with the introduction of model API services built on\\ntop of open source models.\\nThe last part discusses developing an evaluation pipeline that can guide the\\ndevelopment of your application over time. This part brings', 'chapter': 4}, {'start': 2250, 'text': 'lications are being used. An ML engineer at a used car\\ndealership told me that his team built a model to predict the value of a car\\nbased on the specs given by the owner. A year after the model was\\ndeployed, their users seemed to like the feature, but he had no idea if the\\nmodel’s predictions were a', 'chapter': 4}], input_tokens=910, output_tokens=307, input_cost=0.0001365, output_cost=0.0001842, total_cost=0.0003207),\n",
       " RAGResult(question='what is evaluation-driven development in ai?', answer=\"Evaluation-driven development in AI refers to a methodology where the effectiveness and performance of AI models are assessed in the context of their specific applications throughout the development process. This approach emphasizes that a model is only deemed useful if it meets the defined criteria for its intended purpose.\\n\\nKey aspects of evaluation-driven development include:\\n\\n1. **Use of Evaluation Criteria**: It's essential to establish clear criteria that define how the success of the AI application will be measured, which may include factors like accuracy, reliability, and user satisfaction.\\n\\n2. **Model Evaluation Pipeline**: Developing an evaluation pipeline helps in continuously assessing the performance of models over time, ensuring that they adapt and remain effective as application requirements and user needs evolve.\\n\\n3. **Feedback Loop**: The process encourages a feedback loop where insights from evaluations inform further development, refinements, and model selection, ultimately guiding the engineering decisions surrounding the AI system.\\n\\nOverall, the focus is on creating an iterative process that integrates evaluation as a fundamental part of AI development, helping to mitigate the risks associated with deploying models that may not perform effectively in real-world scenarios.\", context=[{'start': 2000, 'text': 'st even more.\\nAI applications with questionable returns on investment are, unfortunately,\\nquite common. This happens not only because the application is hard to\\nevaluate but also because application developers don’t have visibility into\\nhow their applications are being used. An ML engineer at a used', 'chapter': 4}, {'start': 0, 'text': 'Chapter 4. Evaluate AI Systems\\nA model is only useful if it works for its intended purposes. You need to\\nevaluate models in the context of your application. Chapter 3 discusses\\ndifferent approaches to automatic evaluation. This chapter discusses how to\\nuse these approaches to evaluate models for you', 'chapter': 4}, {'start': 250, 'text': 'to\\nuse these approaches to evaluate models for your applications.\\nThis chapter contains three parts. It starts with a discussion of the criteria\\nyou might use to evaluate your applications and how these criteria are\\ndefined and calculated. For example, many people worry about AI making\\nup facts—how ', 'chapter': 4}, {'start': 1750, 'text': 'eployed but no one knows whether it’s working? When I\\nasked this question at conferences, most people said the latter. An\\napplication that is deployed but can’t be evaluated is worse. It costs to\\nmaintain, but if you want to take it down, it might cost even more.\\nAI applications with questionable re', 'chapter': 4}, {'start': 1500, 'text': 'nt of your application over time. This part brings together the\\ntechniques we’ve learned throughout the book to evaluate concrete\\napplications.\\nEvaluation Criteria\\nWhich is worse—an application that has never been deployed or an\\napplication that is deployed but no one knows whether it’s working? Whe', 'chapter': 4}, {'start': 500, 'text': 'e, many people worry about AI making\\nup facts—how is factual consistency detected? How are domain-specific\\ncapabilities like math, science, reasoning, and summarization measured?\\nThe second part focuses on model selection. Given an increasing number of\\nfoundation models to choose from, it can feel o', 'chapter': 4}, {'start': 1250, 'text': ' host their own models or to use a model API. This question has\\nbecome more nuanced with the introduction of model API services built on\\ntop of open source models.\\nThe last part discusses developing an evaluation pipeline that can guide the\\ndevelopment of your application over time. This part brings', 'chapter': 4}, {'start': 2250, 'text': 'lications are being used. An ML engineer at a used car\\ndealership told me that his team built a model to predict the value of a car\\nbased on the specs given by the owner. A year after the model was\\ndeployed, their users seemed to like the feature, but he had no idea if the\\nmodel’s predictions were a', 'chapter': 4}, {'start': 750, 'text': 'of\\nfoundation models to choose from, it can feel overwhelming to choose the\\nright model for your application. Thousands of benchmarks have been\\nintroduced to evaluate these models along different criteria. Can these\\nbenchmarks be trusted? How do you select what benchmarks to use? How\\nabout public le', 'chapter': 4}, {'start': 1000, 'text': 'select what benchmarks to use? How\\nabout public leaderboards that aggregate multiple benchmarks?\\nThe model landscape is teeming with proprietary models and open source\\nmodels. A question many teams will need to visit over and over again is\\nwhether to host their own models or to use a model API. This', 'chapter': 4}], input_tokens=911, output_tokens=218, input_cost=0.00013665, output_cost=0.0001308, total_cost=0.00026745)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test function\n",
    "from rag import run_rag_concurrent\n",
    "\n",
    "gt_path = \"sample_gt.csv\"\n",
    "run_rag_concurrent(gt_path, \"test.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-notes (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
