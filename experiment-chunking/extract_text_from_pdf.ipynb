{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "192654d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caa4a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_text_pdfplumber(pdf_path: Path, start_page: int, end_page: int) -> dict[int, str]:\n",
    "#     \"\"\"\n",
    "#     Extract text from [start_page, end_page] inclusive.\n",
    "#     Pages are 1-indexed for user convenience.\n",
    "\n",
    "#     Returns:\n",
    "#         {page_number: page_text}\n",
    "#     \"\"\"\n",
    "#     if start_page < 1:\n",
    "#         raise ValueError(\"start_page must be >= 1\")\n",
    "#     if end_page < start_page:\n",
    "#         raise ValueError(\"end_page must be >= start_page\")\n",
    "\n",
    "#     parts: dict[int, str] = {}\n",
    "#     with pdfplumber.open(str(pdf_path)) as pdf:\n",
    "#         total_pages = len(pdf.pages)\n",
    "#         if end_page > total_pages:\n",
    "#             raise ValueError(f\"end_page ({end_page}) exceeds total pages ({total_pages})\")\n",
    "\n",
    "#         for page_num in range(start_page, end_page + 1):\n",
    "#             page = pdf.pages[page_num - 1]  # pdf.pages is 0-indexed\n",
    "#             text = (page.extract_text() or \"\").strip()\n",
    "#             parts[page_num] = text\n",
    "\n",
    "#     return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1015e404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'chapter': 4,\n",
       "  'text': 'Chapter 4. Evaluate AI Systems\\nA model is only useful if it works for its intended purposes. You need to\\nevaluate models in the context of your application. Chapter 3 discusses\\ndifferent approaches to automatic evaluation. This chapter discusses how to\\nuse these approaches to evaluate models for your applications.\\nThis chapter contains three parts. It starts with a discussion of the criteria\\nyou might use to evaluate your applications and how these criteria are\\ndefined and calculated. For example, many people worry about AI making\\nup facts—how is factual consistency detected? How are domain-specific\\ncapabilities like math, science, reasoning, and summarization measured?\\nThe second part focuses on model selection. Given an increasing number of\\nfoundation models to choose from, it can feel overwhelming to choose the\\nright model for your application. Thousands of benchmarks have been\\nintroduced to evaluate these models along different criteria. Can these\\nbenchmarks be trusted? How do you select what benchmarks to use? How\\nabout public leaderboards that aggregate multiple benchmarks?\\nThe model landscape is teeming with proprietary models and open source\\nmodels. A question many teams will need to visit over and over again is\\nwhether to host their own models or to use a model API. This question has\\nbecome more nuanced with the introduction of model API services built on\\ntop of open source models.'},\n",
       " 2: {'chapter': 4,\n",
       "  'text': 'The last part discusses developing an evaluation pipeline that can guide the\\ndevelopment of your application over time. This part brings together the\\ntechniques we’ve learned throughout the book to evaluate concrete\\napplications.\\nEvaluation Criteria\\nWhich is worse—an application that has never been deployed or an\\napplication that is deployed but no one knows whether it’s working? When I\\nasked this question at conferences, most people said the latter. An\\napplication that is deployed but can’t be evaluated is worse. It costs to\\nmaintain, but if you want to take it down, it might cost even more.\\nAI applications with questionable returns on investment are, unfortunately,\\nquite common. This happens not only because the application is hard to\\nevaluate but also because application developers don’t have visibility into\\nhow their applications are being used. An ML engineer at a used car\\ndealership told me that his team built a model to predict the value of a car\\nbased on the specs given by the owner. A year after the model was\\ndeployed, their users seemed to like the feature, but he had no idea if the\\nmodel’s predictions were accurate. At the beginning of the ChatGPT fever,\\ncompanies rushed to deploy customer support chatbots. Many of them are\\nstill unsure if these chatbots help or hurt their user experience.'},\n",
       " 3: {'chapter': 4,\n",
       "  'text': 'Before investing time, money, and resources into building an application,\\nit’s important to understand how this application will be evaluated. I call\\nthis approach evaluation-driven development. The name is inspired by test-\\ndriven development in software engineering, which refers to the method of\\nwriting tests before writing code. In AI engineering, evaluation-driven\\ndevelopment means defining evaluation criteria before building.'},\n",
       " 4: {'chapter': 4,\n",
       "  'text': 'EVALUATION-DRIVEN DEVELOPMENT\\nWhile some companies chase the latest hype, sensible business decisions\\nare still being made based on returns on investment, not hype. Applications\\nshould demonstrate value to be deployed. As a result, the most common\\nenterprise applications in production are those with clear evaluation criteria:\\nRecommender systems are common because their successes can be\\n1\\nevaluated by an increase in engagement or purchase-through rates.\\nThe success of a fraud detection system can be measured by how much\\nmoney is saved from prevented frauds.\\nCoding is a common generative AI use case because, unlike other\\ngeneration tasks, generated code can be evaluated using functional\\ncorrectness.\\nEven though foundation models are open-ended, many of their use cases\\nare close-ended, such as intent classification, sentiment analysis, next-\\naction prediction, etc. It’s much easier to evaluate classification tasks\\nthan open-ended tasks.\\nWhile the evaluation-driven development approach makes sense from a\\nbusiness perspective, focusing only on applications whose outcomes can be\\nmeasured is similar to looking for the lost key under the lamppost (at night).\\nIt’s easier to do, but it doesn’t mean we’ll find the key. We might be missing\\nout on many potentially game-changing applications because there is no\\neasy way to evaluate them.'},\n",
       " 5: {'chapter': 4,\n",
       "  'text': 'I believe that evaluation is the biggest bottleneck to AI adoption. Being able\\nto build reliable evaluation pipelines will unlock many new applications.\\nAn AI application, therefore, should start with a list of evaluation criteria\\nspecific to the application. In general, you can think of criteria in the\\nfollowing buckets: domain-specific capability, generation capability,\\ninstruction-following capability, and cost and latency.\\nImagine you ask a model to summarize a legal contract. At a high level,\\ndomain-specific capability metrics tell you how good the model is at\\nunderstanding legal contracts. Generation capability metrics measure how\\ncoherent or faithful the summary is. Instruction-following capability\\ndetermines whether the summary is in the requested format, such as\\nmeeting your length constraints. Cost and latency metrics tell you how\\nmuch this summary will cost you and how long you will have to wait for it.\\nThe last chapter started with an evaluation approach and discussed what\\ncriteria a given approach can evaluate. This section takes a different angle:\\ngiven a criterion, what approaches can you use to evaluate it?\\nDomain-Specific Capability\\nTo build a coding agent, you need a model that can write code. To build an\\napplication to translate from Latin to English, you need a model that\\nunderstands both Latin and English. Coding and English–Latin'},\n",
       " 6: {'chapter': 4,\n",
       "  'text': 'understanding are domain-specific capabilities. A model’s domain-specific\\ncapabilities are constrained by its configuration (such as model architecture\\nand size) and training data. If a model never saw Latin during its training\\nprocess, it won’t be able to understand Latin. Models that don’t have the\\ncapabilities your application requires won’t work for you.\\nTo evaluate whether a model has the necessary capabilities, you can rely on\\ndomain-specific benchmarks, either public or private. Thousands of public\\nbenchmarks have been introduced to evaluate seemingly endless\\ncapabilities, including code generation, code debugging, grade school math,\\nscience knowledge, common sense, reasoning, legal knowledge, tool use,\\ngame playing, etc. The list goes on.\\nDomain-specific capabilities are commonly evaluated using exact\\nevaluation. Coding-related capabilities are typically evaluated using\\nfunctional correctness, as discussed in Chapter 3. While functional\\ncorrectness is important, it might not be the only aspect that you care about.\\nYou might also care about efficiency and cost. For example, would you\\nwant a car that runs but consumes an excessive amount of fuel? Similarly, if\\nan SQL query generated by your text-to-SQL model is correct but takes too\\nlong or requires too much memory to run, it might not be usable.\\nEfficiency can be exactly evaluated by measuring runtime or memory\\nusage. BIRD-SQL (Li et al., 2023) is an example of a benchmark that takes\\ninto account not only the generated query’s execution accuracy but also its'}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract text from pdf \n",
    "pdf_path = \"AI_Engineering_Building_Applications_Chip_Huyen.pdf\"\n",
    "\n",
    "parts = {}\n",
    "with pdfplumber.open(str(pdf_path)) as pdf:\n",
    "    total_pages = len(pdf.pages)\n",
    "    \n",
    "    for page_num in range(1, 7):\n",
    "        page = pdf.pages[page_num-1]\n",
    "        text = (page.extract_text() or \"\")\n",
    "        parts[page_num] = {\n",
    "            \"chapter\": 4, \n",
    "            \"text\": text\n",
    "        }\n",
    "parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa0da1f",
   "metadata": {},
   "source": [
    "- `sections_re` splits whenever the preceding character is . ! or ? and the break contains one or more \\n (optional whitespace in between). Adjust the regex if you want other punctuation or need to keep blank lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c559946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # paragraph chunking\n",
    "# import re\n",
    "# from typing import Dict, List\n",
    "\n",
    "# SECTION_BREAK_RE = re.compile(r\"([.!?]\\s*\\n+)\")\n",
    "\n",
    "\n",
    "# class SectionChunker:\n",
    "#     def __init__(self, parts: Dict[int, dict]):\n",
    "#         self.parts = parts\n",
    "#         self.state = {\"buffer\": [], \"pages\": [], \"section_id\": 0}\n",
    "#         self.sections: List[dict] = []\n",
    "\n",
    "#     @staticmethod\n",
    "#     def flush(state: dict, parts: Dict[int, dict], sections: List[dict]) -> None:\n",
    "#         text = \"\".join(state[\"buffer\"]).strip()\n",
    "#         pages = state[\"pages\"]\n",
    "#         if not text or not pages:\n",
    "#             state[\"buffer\"].clear()\n",
    "#             state[\"pages\"].clear()\n",
    "#             return\n",
    "#         state[\"section_id\"] += 1\n",
    "#         sections.append(\n",
    "#             {   \"chapter\": parts[pages[0]][\"chapter\"],\n",
    "#                 \"pages\": pages.copy(),\n",
    "#                 \"section\": state[\"section_id\"],\n",
    "#                 \"text\": text,\n",
    "#             }\n",
    "#         )\n",
    "#         state[\"buffer\"].clear()\n",
    "#         state[\"pages\"].clear()\n",
    "\n",
    "#     def chunk_sections(self) -> List[dict]:\n",
    "#         for page_num in sorted(self.parts):\n",
    "#             text = self.parts[page_num].get(\"text\", \"\")\n",
    "#             if not text:\n",
    "#                 continue\n",
    "\n",
    "#             for piece in SECTION_BREAK_RE.split(text):\n",
    "#                 if not piece:\n",
    "#                     continue\n",
    "#                 if SECTION_BREAK_RE.fullmatch(piece):\n",
    "#                     self.state[\"buffer\"].append(piece.rstrip(\"\\n\"))\n",
    "#                     SectionChunker.flush(self.state, self.parts, self.sections)\n",
    "#                 else:\n",
    "#                     if not self.state[\"pages\"] or self.state[\"pages\"][-1] != page_num:\n",
    "#                         self.state[\"pages\"].append(page_num)\n",
    "#                     self.state[\"buffer\"].append(piece)\n",
    "\n",
    "#         SectionChunker.flush(self.state, self.parts, self.sections)\n",
    "#         return self.sections\n",
    "\n",
    "# chunker = SectionChunker(parts)\n",
    "# sections = chunker.chunk_sections()\n",
    "# sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812a5af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paragraph chunking - use this one \n",
    "\n",
    "import re \n",
    "\n",
    "SECTION_BREAK_RE = re.compile(r\"([.!?]\\s*\\n+)\")\n",
    "\n",
    "sections = []\n",
    "current_section_id = 0\n",
    "\n",
    "for page_num, payload in parts.items():\n",
    "    text = payload[\"text\"]\n",
    "    pieces = SECTION_BREAK_RE.split(text)\n",
    "\n",
    "    buffer = \"\"\n",
    "    for piece in pieces:\n",
    "        if SECTION_BREAK_RE.fullmatch(piece):\n",
    "            buffer += piece.strip(\"\\n\")  # keep punctuation/whitespace\n",
    "            current_section_id += 1\n",
    "            sections.append(\n",
    "                {\n",
    "                    \"chapter\": payload[\"chapter\"],\n",
    "                    \"pages\": [page_num],\n",
    "                    \"section\": current_section_id,\n",
    "                    \"text\": buffer.strip(),\n",
    "                }\n",
    "            )\n",
    "            buffer = \"\"\n",
    "        else:\n",
    "            buffer += piece\n",
    "\n",
    "    if buffer.strip():\n",
    "        current_section_id += 1\n",
    "        sections.append(\n",
    "            {\n",
    "                \"section\": current_section_id,\n",
    "                \"pages\": [page_num],\n",
    "                \"chapter\": payload[\"chapter\"],\n",
    "                \"text\": buffer.strip(),\n",
    "            }\n",
    "        )\n",
    "sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ab61d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = {}\n",
    "with pdfplumber.open(str(pdf_path)) as pdf:\n",
    "    total_pages = len(pdf.pages)\n",
    "\n",
    "    for page_num in range(1, 7):\n",
    "        page = pdf.pages[page_num - 1]\n",
    "        text = page.extract_text() or \"\"\n",
    "        parts[page_num] = {\n",
    "            \"chapter\": 4,\n",
    "            \"text\": text,\n",
    "        }\n",
    "\n",
    "# collapse into a single entry\n",
    "all_text = []\n",
    "for page_num in sorted(parts):\n",
    "    payload = parts[page_num]\n",
    "    all_text.append(payload[\"text\"])\n",
    "\n",
    "combined = {\n",
    "    \"chapter\": 4,         # or derive from whatever logic you need\n",
    "    \"text\": \"\\n\".join(all_text),\n",
    "}\n",
    "\n",
    "# print(combined[\"chapter\"])\n",
    "# print(len(combined[\"text\"]))\n",
    "chap4 = [combined]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "016684b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'text': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result\n",
    "\n",
    "def chunk_documents(docs:list, size=500, step=300) -> list:\n",
    "    doc_chunks = []\n",
    "\n",
    "    for doc in docs:\n",
    "        doc_copy = doc.copy()\n",
    "        doc_content = doc_copy.pop('text')\n",
    "        chunks = sliding_window(doc_content, size, step)\n",
    "        for chunk in chunks:\n",
    "            chunk.update(doc_copy)\n",
    "        doc_chunks.extend(chunks)\n",
    "    return doc_chunks\n",
    "\n",
    "chunks = chunk_documents(chap4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cd825fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vancescadinh/Documents/AI/ai-engineering-notes/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from minsearch import VectorSearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')\n",
    "v_index = VectorSearch(keyword_fields = [])\n",
    "\n",
    "def create_doc_embeddings(chunks:list):\n",
    "    embeddings = []\n",
    "\n",
    "    for d in tqdm(chunks):\n",
    "        v = embedding_model.encode(d['text'])\n",
    "        embeddings.append(v)\n",
    "\n",
    "    return np.array(embeddings)\n",
    "\n",
    "\n",
    "def create_vector_index(chunks:list):\n",
    "    emb_array = create_doc_embeddings(chunks)\n",
    "    return v_index.fit(emb_array, chunks)\n",
    "\n",
    "\n",
    "\n",
    "def text_embedding_search(query:str):\n",
    "    query_embedding = embedding_model.encode(query)\n",
    "    return v_index.search(query_embedding, num_results=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2855677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 25.89it/s]\n"
     ]
    }
   ],
   "source": [
    "vector_store = create_vector_index(chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-notes (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
