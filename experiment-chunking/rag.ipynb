{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a124a13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'performance differences in model APIs',\n",
       " 'summary_answer': 'The excerpt mentions that the same AI model may perform differently across various APIs due to optimization techniques used, which necessitates thorough testing when switching APIs.',\n",
       " 'difficulty': 'intermediate',\n",
       " 'text': 'After developing a model, a developer can choose to open source it, make it\\naccessible via an API, or both. Many model developers are also model\\nservice providers. Cohere and Mistral open source some models and\\nprovide APIs for some. OpenAI is typically known for their commercial\\nmodels, but they’ve also open sourced models (GPT-2, CLIP). Typically,\\nmodel providers open source weaker models and keep their best models\\nbehind paywalls, either via APIs or to power their products.\\nModel APIs can be available through model providers (such as OpenAI and\\nAnthropic), cloud service providers (such as Azure and GCP [Google Cloud\\nPlatform]), or third-party API providers (such as Databricks Mosaic,\\nAnyscale, etc.). The same model can be available through different APIs\\nwith different features, constraints, and pricings. For example, GPT-4 is\\navailable through both OpenAI and Azure APIs. There might be slight\\ndifferences in the performance of the same model provided through\\ndifferent APIs, as different APIs might use different techniques to optimize\\nthis model, so make sure to run thorough tests when you switch between\\nmodel APIs.\\nCommercial models are only accessible via APIs licensed by the model\\n13\\ndevelopers. Open source models can be supported by any API provider,\\nallowing you to pick and choose the provider that works best for you. For\\ncommercial model providers, models are their competitive advantages. For\\nAPI providers that don’t have their own models, APIs are their competitive',\n",
       " 'id': 'db22bdaa'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample_questions = pd.read_csv(\"sample_gt.csv\")\n",
    "sample_dict = sample_questions.to_dict(orient=\"records\")\n",
    "sample_dict[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a32fd03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0,\n",
       "  'text': 'Chapter 4. Evaluate AI Systems\\nA model is only useful if it works for its intended purposes. You need to\\nevaluate models in the context of your application. Chapter 3 discusses\\ndifferent approaches to automatic evaluation. This chapter discusses how to\\nuse these approaches to evaluate models for you',\n",
       "  'chapter': 4},\n",
       " {'start': 250,\n",
       "  'text': 'to\\nuse these approaches to evaluate models for your applications.\\nThis chapter contains three parts. It starts with a discussion of the criteria\\nyou might use to evaluate your applications and how these criteria are\\ndefined and calculated. For example, many people worry about AI making\\nup facts—how ',\n",
       "  'chapter': 4}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from chunking import extract_text_from_pdf, chunk_sliding_window\n",
    "\n",
    "pages = extract_text_from_pdf(start_page=1)\n",
    "chunks = chunk_sliding_window(pages, size=300, step=250)\n",
    "chunks[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c5c1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vancescadinh/Documents/AI/ai-engineering-notes/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 468/468 [00:08<00:00, 53.07it/s]\n"
     ]
    }
   ],
   "source": [
    "from minsearch import VectorSearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')\n",
    "v_index = VectorSearch(keyword_fields = [])\n",
    "\n",
    "\n",
    "embeddings = []\n",
    "for chunk in tqdm(chunks):\n",
    "    vector = embedding_model.encode(chunk[\"text\"])\n",
    "    embeddings.append(vector)\n",
    "\n",
    "embeddings_array = np.array(embeddings)\n",
    "vector_store = v_index.fit(embeddings_array, chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "371dd5c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 69000,\n",
       "  'text': 's\\nsaturate, necessitating the introduction of new benchmarks.\\nA tool that helps you evaluate a model on multiple benchmarks is an\\nevaluation harness. As of this writing, EleutherAI’s lm-evaluation-harness\\nsupports over 400 benchmarks. OpenAI’s evals lets you run any of the\\napproximately 500 existing',\n",
       "  'chapter': 4},\n",
       " {'start': 71750,\n",
       "  'text': ' in late 2023, Hugging Face updated their Open LLM Leaderboard\\nto use the average of six different benchmarks to rank models:\\n1. ARC-C (Clark et al., 2018): Measuring the ability to solve complex,\\ngrade school-level science questions.\\n2. MMLU (Hendrycks et al., 2020): Measuring knowledge and reasoni',\n",
       "  'chapter': 4},\n",
       " {'start': 75500,\n",
       "  'text': '23): a graduate-level Q&A benchmark\\nMuSR (Sprague et al., 2023): a chain-of-thought, multistep reasoning benchmark\\nBBH (BIG-bench Hard) (Srivastava et al., 2023): another reasoning benchmark\\nIFEval (Zhou et al., 2023): an instruction-following benchmark\\nI have no doubt that these benchmarks will soo',\n",
       "  'chapter': 4},\n",
       " {'start': 69250,\n",
       "  'text': 'lets you run any of the\\napproximately 500 existing benchmarks and register new benchmarks to\\nevaluate OpenAI models. Their benchmarks evaluate a wide range of\\ncapabilities, from doing math and solving puzzles to identifying ASCII art\\nthat represents words.\\nBenchmark selection and aggregation\\nBenchma',\n",
       "  'chapter': 4},\n",
       " {'start': 80500,\n",
       "  'text': ' need to think about how important each benchmark\\nis to you and weigh their scores accordingly.\\nAs you evaluate models using public benchmarks, keep in mind that the\\ngoal of this process is to select a small subset of models to do more rigorous\\nexperiments using your own benchmarks and metrics. This',\n",
       "  'chapter': 4},\n",
       " {'start': 750,\n",
       "  'text': 'of\\nfoundation models to choose from, it can feel overwhelming to choose the\\nright model for your application. Thousands of benchmarks have been\\nintroduced to evaluate these models along different criteria. Can these\\nbenchmarks be trusted? How do you select what benchmarks to use? How\\nabout public le',\n",
       "  'chapter': 4},\n",
       " {'start': 86750,\n",
       "  'text': ' for your application. After using public\\nbenchmarks to narrow them to a set of promising models, you’ll need to run\\nyour own evaluation pipeline to find the best one for your application. How\\nto design a custom evaluation pipeline will be our next topic.\\nDesign Your Evaluation Pipeline\\nThe success ',\n",
       "  'chapter': 4},\n",
       " {'start': 79750,\n",
       "  'text': ' the\\nmodel you care about doesn’t have a publicly available score on your\\n25\\nbenchmark, you will need to run the evaluation yourself. Hopefully, an\\nevaluation harness can help you with that. Running benchmarks can be\\nexpensive. For example, Stanford spent approximately $80,000–$100,000\\n26\\nto evaluat',\n",
       "  'chapter': 4},\n",
       " {'start': 47500,\n",
       "  'text': 'mber of public benchmarks and why you can’t trust them. This will set\\nthe stage for the last section in the chapter. Because public benchmarks\\ncan’t be trusted, you need to design your own evaluation pipeline with\\nprompts and metrics you can trust.\\nModel Build Versus Buy\\nAn evergreen question for co',\n",
       "  'chapter': 4},\n",
       " {'start': 71500,\n",
       "  'text': ' are generally thoughtful about\\nhow they select benchmarks, their decision-making process isn’t always\\nclear to users. Different leaderboards often end up with different\\nbenchmarks, making it hard to compare and interpret their rankings. For\\nexample, in late 2023, Hugging Face updated their Open LLM',\n",
       "  'chapter': 4}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_question = \"what benchmarks to use for evaluating LLMs\"\n",
    "user_question_embedding = embedding_model.encode(user_question)\n",
    "vector_store.search(user_question_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b53466fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(user_question:str):\n",
    "    user_question_embedding = embedding_model.encode(user_question)\n",
    "    results = vector_store.search(user_question_embedding, num_results=10)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8376a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\" \n",
    "\n",
    "You are an AI Researcher with 10 years of experience as a senior AI Engineer. \n",
    "You are instructing a course based on the \"AI Engineering book\" published by Chip Huyen.\n",
    "Answer the QUESTION and base your response ONLY on the CONTEXT provided by this book.\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_template = \"\"\" \n",
    "<QUESTION>\n",
    "{user_question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1824115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def build_prompt(user_question, search_results):\n",
    "    search_json = json.dumps(search_results)\n",
    "    prompt = prompt_template.format(user_question=user_question, context=search_json).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996f361f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<QUESTION>\\nwhat benchmarks to use for evaluating LLMs\\n</QUESTION>\\n\\n<CONTEXT>\\n[{\"start\": 69000, \"text\": \"s\\\\nsaturate, necessitating the introduction of new benchmarks.\\\\nA tool that helps you evaluate a model on multiple benchmarks is an\\\\nevaluation harness. As of this writing, EleutherAI\\\\u2019s lm-evaluation-harness\\\\nsupports over 400 benchmarks. OpenAI\\\\u2019s evals lets you run any of the\\\\napproximately 500 existing\", \"chapter\": 4}, {\"start\": 71750, \"text\": \" in late 2023, Hugging Face updated their Open LLM Leaderboard\\\\nto use the average of six different benchmarks to rank models:\\\\n1. ARC-C (Clark et al., 2018): Measuring the ability to solve complex,\\\\ngrade school-level science questions.\\\\n2. MMLU (Hendrycks et al., 2020): Measuring knowledge and reasoni\", \"chapter\": 4}, {\"start\": 75500, \"text\": \"23): a graduate-level Q&A benchmark\\\\nMuSR (Sprague et al., 2023): a chain-of-thought, multistep reasoning benchmark\\\\nBBH (BIG-bench Hard) (Srivastava et al., 2023): another reasoning benchmark\\\\nIFEval (Zhou et al., 2023): an instruction-following benchmark\\\\nI have no doubt that these benchmarks will soo\", \"chapter\": 4}, {\"start\": 69250, \"text\": \"lets you run any of the\\\\napproximately 500 existing benchmarks and register new benchmarks to\\\\nevaluate OpenAI models. Their benchmarks evaluate a wide range of\\\\ncapabilities, from doing math and solving puzzles to identifying ASCII art\\\\nthat represents words.\\\\nBenchmark selection and aggregation\\\\nBenchma\", \"chapter\": 4}, {\"start\": 80500, \"text\": \" need to think about how important each benchmark\\\\nis to you and weigh their scores accordingly.\\\\nAs you evaluate models using public benchmarks, keep in mind that the\\\\ngoal of this process is to select a small subset of models to do more rigorous\\\\nexperiments using your own benchmarks and metrics. This\", \"chapter\": 4}, {\"start\": 750, \"text\": \"of\\\\nfoundation models to choose from, it can feel overwhelming to choose the\\\\nright model for your application. Thousands of benchmarks have been\\\\nintroduced to evaluate these models along different criteria. Can these\\\\nbenchmarks be trusted? How do you select what benchmarks to use? How\\\\nabout public le\", \"chapter\": 4}, {\"start\": 86750, \"text\": \" for your application. After using public\\\\nbenchmarks to narrow them to a set of promising models, you\\\\u2019ll need to run\\\\nyour own evaluation pipeline to find the best one for your application. How\\\\nto design a custom evaluation pipeline will be our next topic.\\\\nDesign Your Evaluation Pipeline\\\\nThe success \", \"chapter\": 4}, {\"start\": 79750, \"text\": \" the\\\\nmodel you care about doesn\\\\u2019t have a publicly available score on your\\\\n25\\\\nbenchmark, you will need to run the evaluation yourself. Hopefully, an\\\\nevaluation harness can help you with that. Running benchmarks can be\\\\nexpensive. For example, Stanford spent approximately $80,000\\\\u2013$100,000\\\\n26\\\\nto evaluat\", \"chapter\": 4}, {\"start\": 47500, \"text\": \"mber of public benchmarks and why you can\\\\u2019t trust them. This will set\\\\nthe stage for the last section in the chapter. Because public benchmarks\\\\ncan\\\\u2019t be trusted, you need to design your own evaluation pipeline with\\\\nprompts and metrics you can trust.\\\\nModel Build Versus Buy\\\\nAn evergreen question for co\", \"chapter\": 4}, {\"start\": 71500, \"text\": \" are generally thoughtful about\\\\nhow they select benchmarks, their decision-making process isn\\\\u2019t always\\\\nclear to users. Different leaderboards often end up with different\\\\nbenchmarks, making it hard to compare and interpret their rankings. For\\\\nexample, in late 2023, Hugging Face updated their Open LLM\", \"chapter\": 4}]\\n</CONTEXT>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_question = \"what benchmarks to use for evaluating LLMs\"\n",
    "search_results = search(user_question = user_question)\n",
    "\n",
    "user_prompt = build_prompt(user_question, search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93df1724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def ask_llm(user_prompt, instructions=None, model=\"gpt-4o-mini\"):\n",
    "    messages = []\n",
    "\n",
    "    if instructions:\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": instructions\n",
    "        })\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    })\n",
    "\n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5236e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(user_question):\n",
    "    search_results = search(user_question)\n",
    "    user_prompt = build_prompt(user_question, search_results)\n",
    "    response = ask_llm(user_prompt)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69146134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-notes (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
