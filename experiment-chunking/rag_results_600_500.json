[
  {
    "question": "importance of reliable benchmarks in AI",
    "answer": "Reliable benchmarks in AI are crucial for several key reasons:\n\n1. **Risk Reduction**: Establishing a reliable evaluation pipeline mitigates risks associated with deploying AI models. Without dependable metrics, decision-makers may adopt models that perform poorly or are unsuitable for the intended application.\n\n2. **Performance Improvement**: A solid evaluation framework allows developers to identify weaknesses in their models. By understanding specific areas for enhancement, teams can make informed adjustments that lead to better performance.\n\n3. **Progress Tracking**: Benchmarks provide a way to measure improvements over time. As new models are developed or existing ones refined, having a consistent set of benchmarks enables teams to see tangible advances, supporting strategic planning and resource allocation.\n\n4. **Application Relevance**: Different AI applications have unique evaluation criteria. By starting with a clear set of benchmarks tailored to specific use cases (like factual consistency for chatbots or domain-specific capabilities for legal summaries), organizations can ensure their models meet the practical demands of the context they operate in.\n\n5. **Model Selection Guidance**: With an abundance of available foundation models, reliable benchmarks assist developers in choosing the right model for their application. By providing trustworthy metrics, benchmarks can simplify comparison and decision-making processes.\n\n6. **Benchmark Saturation and Evolution**: As AI models evolve, old benchmarks may become less effective, necessitating the introduction of new benchmarks. Maintaining an up-to-date evaluation pipeline ensures that organizations do not miss out on potential improvements or breakthrough applications due to outdated metrics.\n\nIn summary, reliable benchmarks are foundational to the successful adoption and deployment of AI technologies, guiding evaluation, selection, and enhancement processes while significantly reducing the risks associated with AI implementation.",
    "context": [
      {
        "start": 107000,
        "text": "ations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enable you to reduce risks, discover opportunities\nto improve performance, and benchmark progresses, which will all save\nyou time and headaches down the line.\nGiven an increasing number of readily available foundation models, for\nmost application developers, the challenge is no longer in developing\nmodel",
        "chapter": 4
      },
      {
        "start": 500,
        "text": "e, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-specific\ncapabilities like math, science, reasoning, and summarization measured?\nThe second part focuses on model selection. Given an increasing number of\nfoundation models to choose from, it can feel overwhelming to choose the\nright model for your application. Thousands of benchmarks have been\nintroduced to evaluate these models along different criteria. Can these\nbenchmarks be trusted? How do you select what benchmarks to use? How\nabout public leaderboards that aggregate multiple benchmarks?\nThe",
        "chapter": 4
      },
      {
        "start": 4500,
        "text": "to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can think of criteria in the\nfollowing buckets: domain-specific capability, generation capability,\ninstruction-following capability, and cost and latency.\nImagine you ask a model to summarize a legal contract. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstand",
        "chapter": 4
      },
      {
        "start": 78000,
        "text": "rboards with public benchmarks\nWhen evaluating models for a specific application, you’re basically creating\na private leaderboard that ranks models based on your evaluation criteria.\nThe first step is to gather a list of benchmarks that evaluate the capabilities\nimportant to your application. If you want to build a coding agent, look at\ncode-related benchmarks. If you build a writing assistant, look into creative\nwriting benchmarks. As new benchmarks are constantly introduced and old\nbenchmarks become saturated, you should look for the latest benchmarks.\nMake sure to evaluate how reliable a be",
        "chapter": 4
      },
      {
        "start": 95000,
        "text": ": what scores must an\napplication achieve for it to be useful? For example, you might determine\nthat your chatbot’s factual consistency score must be at least 50% for it to\nbe useful. Anything below this makes it unusable even for general customer\nrequests.\nBefore developing AI evaluation metrics, it’s crucial to first understand the\nbusiness metrics you’re targeting. Many applications focus on stickiness\nmetrics, such as daily, weekly, or monthly active users (DAU, WAU,\nMAU). Others prioritize engagement metrics, like the number of\nconversations a user initiates per month or the duration of e",
        "chapter": 4
      },
      {
        "start": 4000,
        "text": "timent analysis, next-\naction prediction, etc. It’s much easier to evaluate classification tasks\nthan open-ended tasks.\nWhile the evaluation-driven development approach makes sense from a\nbusiness perspective, focusing only on applications whose outcomes can be\nmeasured is similar to looking for the lost key under the lamppost (at night).\nIt’s easier to do, but it doesn’t mean we’ll find the key. We might be missing\nout on many potentially game-changing applications because there is no\neasy way to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto ",
        "chapter": 4
      },
      {
        "start": 70500,
        "text": "on their aggregated\nperformance on a subset of benchmarks. These leaderboards are immensely\nhelpful but far from being comprehensive. First, due to the compute\nconstraint—evaluating a model on a benchmark requires compute—most\nleaderboards can incorporate only a small number of benchmarks. Some\nleaderboards might exclude an important but expensive benchmark. For\nexample, HELM (Holistic Evaluation of Language Models) Lite left out an\ninformation retrieval benchmark (MS MARCO, Microsoft Machine\nReading Comprehension) because it’s expensive to run. Hugging Face\nopted out of HumanEval due to its l",
        "chapter": 4
      },
      {
        "start": 74500,
        "text": "hallenge of selecting benchmarks to rank\nmodels. If leaderboard developers can’t explain their benchmark selection\nprocesses, it might be because it’s really hard to do so.\nAn important aspect of benchmark selection that is often overlooked is\nbenchmark correlation. It is important because if two benchmarks are\nperfectly correlated, you don’t want both of them. Strongly correlated\n21\nbenchmarks can exaggerate biases.\nNOTE\nWhile I was writing this book, many benchmarks became saturated or close to being saturated. In\nJune 2024, less than a year after their leaderboard’s last revamp, Hugging Fac",
        "chapter": 4
      },
      {
        "start": 68500,
        "text": "ommercial API or to host a model yourself. This decision should\nsignificantly narrow your options. Next, you can further refine your\nselection using publicly available model performance data.\nNavigate Public Benchmarks\nThere are thousands of benchmarks designed to evaluate a model’s different\ncapabilities. Google’s BIG-bench (2022) alone has 214 benchmarks. The\nnumber of benchmarks rapidly grows to match the rapidly growing number\nof AI use cases. In addition, as AI models improve, old benchmarks\nsaturate, necessitating the introduction of new benchmarks.\nA tool that helps you evaluate a model",
        "chapter": 4
      },
      {
        "start": 71500,
        "text": " are generally thoughtful about\nhow they select benchmarks, their decision-making process isn’t always\nclear to users. Different leaderboards often end up with different\nbenchmarks, making it hard to compare and interpret their rankings. For\nexample, in late 2023, Hugging Face updated their Open LLM Leaderboard\nto use the average of six different benchmarks to rank models:\n1. ARC-C (Clark et al., 2018): Measuring the ability to solve complex,\ngrade school-level science questions.\n2. MMLU (Hendrycks et al., 2020): Measuring knowledge and reasoning\ncapabilities in 57 subjects, including elementa",
        "chapter": 4
      }
    ],
    "input_tokens": 1572,
    "output_tokens": 333,
    "input_cost": 0.0002358,
    "output_cost": 0.0001998,
    "total_cost": 0.0004356,
    "reference_answer": "It emphasizes that many benchmarks may not accurately measure the intended metrics, stressing the need for careful evaluation."
  },
  {
    "question": "what is evaluation-driven development in ai?",
    "answer": "**Evaluation-driven development** in AI refers to the practice of defining evaluation criteria before building an AI application. This approach, analogous to test-driven development in software engineering, emphasizes the importance of having clear and measurable outcomes for an application to demonstrate its value. \n\nIn evaluation-driven development, the initial focus is on establishing reliable evaluation pipelines to assess the performance of the AI system. This is crucial as it helps determine whether the application meets its intended goals and enables informed decisions on deployment. By starting with a set of evaluation criteria—such as domain-specific capability, generation capability, instruction-following capability, and cost and latency—developers can create a framework for success, reducing risks and identifying opportunities for improvement.\n\nThe book emphasizes that without proper evaluation processes, it can become challenging to assess the effectiveness of AI applications, potentially leading to investments in systems that do not provide clear returns. Thus, a well-defined evaluation strategy is considered a fundamental step in AI engineering to ensure the reliability and effectiveness of deployed models.",
    "context": [
      {
        "start": 3000,
        "text": "ing, which refers to the method of\nwriting tests before writing code. In AI engineering, evaluation-driven\ndevelopment means defining evaluation criteria before building.\nEVALUATION-DRIVEN DEVELOPMENT\nWhile some companies chase the latest hype, sensible business decisions\nare still being made based on returns on investment, not hype. Applications\nshould demonstrate value to be deployed. As a result, the most common\nenterprise applications in production are those with clear evaluation criteria:\nRecommender systems are common because their successes can be\n1\nevaluated by an increase in engagemen",
        "chapter": 4
      },
      {
        "start": 4000,
        "text": "timent analysis, next-\naction prediction, etc. It’s much easier to evaluate classification tasks\nthan open-ended tasks.\nWhile the evaluation-driven development approach makes sense from a\nbusiness perspective, focusing only on applications whose outcomes can be\nmeasured is similar to looking for the lost key under the lamppost (at night).\nIt’s easier to do, but it doesn’t mean we’ll find the key. We might be missing\nout on many potentially game-changing applications because there is no\neasy way to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto ",
        "chapter": 4
      },
      {
        "start": 4500,
        "text": "to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can think of criteria in the\nfollowing buckets: domain-specific capability, generation capability,\ninstruction-following capability, and cost and latency.\nImagine you ask a model to summarize a legal contract. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstand",
        "chapter": 4
      },
      {
        "start": 2500,
        "text": "t he had no idea if the\nmodel’s predictions were accurate. At the beginning of the ChatGPT fever,\ncompanies rushed to deploy customer support chatbots. Many of them are\nstill unsure if these chatbots help or hurt their user experience.\nBefore investing time, money, and resources into building an application,\nit’s important to understand how this application will be evaluated. I call\nthis approach evaluation-driven development. The name is inspired by test-\ndriven development in software engineering, which refers to the method of\nwriting tests before writing code. In AI engineering, evaluation-",
        "chapter": 4
      },
      {
        "start": 107000,
        "text": "ations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enable you to reduce risks, discover opportunities\nto improve performance, and benchmark progresses, which will all save\nyou time and headaches down the line.\nGiven an increasing number of readily available foundation models, for\nmost application developers, the challenge is no longer in developing\nmodel",
        "chapter": 4
      },
      {
        "start": 87000,
        "text": "opic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each ap",
        "chapter": 4
      },
      {
        "start": 98000,
        "text": "evelopment. Each day, you can use human experts to evaluate a subset of\nyour application’s outputs that day to detect any changes in the application’s\nperformance or unusual patterns in usage. For example, LinkedIn developed\na process to manually evaluate up to 500 daily conservations with their AI\nsystems.\nConsider evaluation methods to be used not just during experimentation but\nalso during production. During experimentation, you might have reference\ndata to compare your application’s outputs to, whereas, in production,\nreference data might not be immediately available. However, in productio",
        "chapter": 4
      },
      {
        "start": 106500,
        "text": "e iteration is necessary, you should be able to expect\na certain level of consistency from your evaluation pipeline. If the\nevaluation process changes constantly, you won’t be able to use the\nevaluation results to guide your application’s development.\nAs you iterate on your evaluation pipeline, make sure to do proper\nexperiment tracking: log all variables that could change in an evaluation\nprocess, including but not limited to the evaluation data, the rubric, and the\nprompt and sampling configurations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most imp",
        "chapter": 4
      },
      {
        "start": 1500,
        "text": "nt of your application over time. This part brings together the\ntechniques we’ve learned throughout the book to evaluate concrete\napplications.\nEvaluation Criteria\nWhich is worse—an application that has never been deployed or an\napplication that is deployed but no one knows whether it’s working? When I\nasked this question at conferences, most people said the latter. An\napplication that is deployed but can’t be evaluated is worse. It costs to\nmaintain, but if you want to take it down, it might cost even more.\nAI applications with questionable returns on investment are, unfortunately,\nquite comm",
        "chapter": 4
      },
      {
        "start": 0,
        "text": "Chapter 4. Evaluate AI Systems\nA model is only useful if it works for its intended purposes. You need to\nevaluate models in the context of your application. Chapter 3 discusses\ndifferent approaches to automatic evaluation. This chapter discusses how to\nuse these approaches to evaluate models for your applications.\nThis chapter contains three parts. It starts with a discussion of the criteria\nyou might use to evaluate your applications and how these criteria are\ndefined and calculated. For example, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-sp",
        "chapter": 4
      }
    ],
    "input_tokens": 1537,
    "output_tokens": 199,
    "input_cost": 0.00023055,
    "output_cost": 0.0001194,
    "total_cost": 0.00034995,
    "reference_answer": "Evaluation-driven development refers to the approach of establishing evaluation criteria before building an AI application, similar to how test-driven development focuses on writing tests before code."
  },
  {
    "question": "performance differences in model APIs",
    "answer": "Performance differences in model APIs can arise from several factors:\n\n1. **Variability Across Providers**: Different APIs from varying providers may implement the same model with slight differences in optimizations and configurations. This can lead to variations in performance, such as latency and output quality. Hence, it's advisable to conduct thorough testing when switching between APIs.\n\n2. **Cost Structure**: The cost per token for API usage typically remains stable as the scale increases. However, operational costs may be reduced when hosting your own models, as you can optimize resource usage more efficiently at larger scales.\n\n3. **API Features and Constraints**: Each API might have unique features or constraints that affect performance. These might include input limits, supported functionalities, or response times, which can impact overall user experience.\n\n4. **Scalability Considerations**: When evaluating model APIs, it’s crucial to consider whether the service can accommodate your scale, especially as the requirement for tokens per minute (TPM) increases. Inadequate scaling can lead to performance bottlenecks.\n\n5. **Latency vs. Value**: While high latency is often undesirable, it may not always be a deal-breaker depending on the specific application and its requirements. Distinguishing between must-have and nice-to-have latency is essential for evaluating performance.\n\n6. **Access to Benchmarks**: Public benchmarks can facilitate the evaluation of different models' capabilities; however, they won’t necessarily help in identifying the best option for every specific use case.\n\nThese aspects collectively highlight the importance of careful consideration when comparing the performance of model APIs to align with specific application needs and goals.",
    "context": [
      {
        "start": 41000,
        "text": "bserved by users by careful\nprompting, such as instructing the model to be concise, setting a stopping\ncondition for generation (discussed in Chapter 2), or other optimization\ntechniques (discussed in Chapter 9).\nTIP\nWhen evaluating models based on latency, it’s important to differentiate between the must-have and\nthe nice-to-have. If you ask users if they want lower latency, nobody will ever say no. But high\nlatency is often an annoyance, not a deal breaker.\nIf you use model APIs, they typically charge by tokens. The more input and\noutput tokens you use, the more expensive it is. Many applica",
        "chapter": 4
      },
      {
        "start": 42000,
        "text": "hese memory\nconfigurations. It’s not a coincidence that many models today have 7 billion\nor 65 billion parameters.\nIf you use model APIs, your cost per token usually doesn’t change much as\nyou scale. However, if you host your own models, your cost per token can\nget much cheaper as you scale. If you’ve already invested in a cluster that\ncan serve a maximum of 1 billion tokens a day, the compute cost remains\n9\nthe same whether you serve 1 million tokens or 1 billion tokens a day.\nTherefore, at different scales, companies need to reevaluate whether it\nmakes more sense to use model APIs or to host",
        "chapter": 4
      },
      {
        "start": 53500,
        "text": "gh both OpenAI and Azure APIs. There might be slight\ndifferences in the performance of the same model provided through\ndifferent APIs, as different APIs might use different techniques to optimize\nthis model, so make sure to run thorough tests when you switch between\nmodel APIs.\nCommercial models are only accessible via APIs licensed by the model\n13\ndevelopers. Open source models can be supported by any API provider,\nallowing you to pick and choose the provider that works best for you. For\ncommercial model providers, models are their competitive advantages. For\nAPI providers that don’t have the",
        "chapter": 4
      },
      {
        "start": 108000,
        "text": "ng fluency, coherence, and\nfaithfulness.\nTo help answer the question of whether to host a model or to use a model\nAPI, this chapter outlined the pros and cons of each approach along seven\naxes, including data privacy, data lineage, performance, functionality,\ncontrol, and cost. This decision, like all the build versus buy decisions, is\nunique to every team, depending not only on what the team needs but also\non what the team wants.\nThis chapter also explored the thousands of available public benchmarks.\nPublic benchmarks can help you weed out bad models, but they won’t help\nyou find the best mo",
        "chapter": 4
      },
      {
        "start": 53000,
        "text": " providers open source weaker models and keep their best models\nbehind paywalls, either via APIs or to power their products.\nModel APIs can be available through model providers (such as OpenAI and\nAnthropic), cloud service providers (such as Azure and GCP [Google Cloud\nPlatform]), or third-party API providers (such as Databricks Mosaic,\nAnyscale, etc.). The same model can be available through different APIs\nwith different features, constraints, and pricings. For example, GPT-4 is\navailable through both OpenAI and Azure APIs. There might be slight\ndifferences in the performance of the same mode",
        "chapter": 4
      },
      {
        "start": 63000,
        "text": "hat.\nIn general, you want a model that is easy to use and manipulate. Typically,\nproprietary models are easier to get started with and scale, but open models\nmight be easier to manipulate as their components are more accessible.\nRegardless of whether you go with open or proprietary models, you want\nthis model to follow a standard API, which makes it easier to swap models.\nMany model developers try to make their models mimic the API of the most\npopular models. As of this writing, many API providers mimic OpenAI’s\nAPI.\nYou might also prefer models with good community support. The more\ncapabiliti",
        "chapter": 4
      },
      {
        "start": 42500,
        "text": "ferent scales, companies need to reevaluate whether it\nmakes more sense to use model APIs or to host their own models.\nTable 4-3 shows criteria you might use to evaluate models for your\napplication. The row scale is especially important when evaluating model\nAPIs, because you need a model API service that can support your scale.\nTable 4-3. An example of criteria used to select models for a fictional application.\nHard\nCriteria Metric Benchmark Ideal\nrequirement\nCost Cost per X < $30.00 / < $15\noutput token 1M tokens 1M to\nScale TPM (tokens X > 1M TPM > 1M\nper minute)\nLatency Time to first Inter",
        "chapter": 4
      },
      {
        "start": 62500,
        "text": "elf requires nontrivial time, talent, and\nengineering effort. You’ll need to optimize the model, scale and maintain\nthe inference service as needed, and provide guardrails around your model.\nAPIs are expensive, but engineering can be even more so.\nOn the other hand, using another API means that you’ll have to depend on\ntheir SLA, service-level agreement. If these APIs aren’t reliable, which is\noften the case with early startups, you’ll have to spend your engineering\neffort on guardrails around that.\nIn general, you want a model that is easy to use and manipulate. Typically,\nproprietary models ",
        "chapter": 4
      },
      {
        "start": 68500,
        "text": "ommercial API or to host a model yourself. This decision should\nsignificantly narrow your options. Next, you can further refine your\nselection using publicly available model performance data.\nNavigate Public Benchmarks\nThere are thousands of benchmarks designed to evaluate a model’s different\ncapabilities. Google’s BIG-bench (2022) alone has 214 benchmarks. The\nnumber of benchmarks rapidly grows to match the rapidly growing number\nof AI use cases. In addition, as AI models improve, old benchmarks\nsaturate, necessitating the introduction of new benchmarks.\nA tool that helps you evaluate a model",
        "chapter": 4
      },
      {
        "start": 44500,
        "text": "els along the cost–performance axes and choosing the\nmodel that gives the best performance for your bucks\nHowever, the actual selection process is a lot more nuanced. Let’s explore\nwhat it looks like.\nModel Selection Workflow\nWhen looking at models, it’s important to differentiate between hard\nattributes (what is impossible or impractical for you to change) and soft\nattributes (what you can and are willing to change).\nHard attributes are often the results of decisions made by model providers\n(licenses, training data, model size) or your own policies (privacy, control).\nFor some use cases, the ",
        "chapter": 4
      }
    ],
    "input_tokens": 1662,
    "output_tokens": 325,
    "input_cost": 0.0002493,
    "output_cost": 0.000195,
    "total_cost": 0.0004443,
    "reference_answer": "The excerpt mentions that the same AI model may perform differently across various APIs due to optimization techniques used, which necessitates thorough testing when switching APIs."
  },
  {
    "question": "why do OpenAI models seem worse after updates",
    "answer": "OpenAI models may seem worse after updates due to several reasons:\n\n1. **Evaluation Challenges**: Evaluation of model performance is inherently difficult. It's possible that during the evaluation process, shifts in performance traits might not accurately reflect the model's capabilities. Evaluation is complex, and even OpenAI might not have a clear understanding of whether a model has genuinely improved or worsened.\n\n2. **User Expectations**: Users may have unique expectations based on the previous interactions with the model, leading to a perception that updates made the model worse. As models are updated, the differences in their behaviors can affect user satisfaction.\n\n3. **Benchmarks and Measurement**: As benchmarks evolve, the criteria by which models are judged may change, making it seem as though newer versions perform worse on familiar tasks while they are indeed optimized for different evaluations.\n\n4. **Application-Specific Performance**: The best overall model may not be the best fit for every specific application or use case, leading to the perception of decreased performance in certain contexts after an update.\n\nThus, while OpenAI does not intentionally release inferior models, the interplay of these factors could lead users to believe that newer versions are performing worse than their predecessors.",
    "context": [
      {
        "start": 79000,
        "text": "rmances changed significantly between March 2023\nand June 2023, as shown in Figure 4-9.\nFigure 4-9. Changes in the performances of GPT-3.5 and GPT-4 from March 2023 to\nJune 2023 on certain benchmarks (Chen et al., 2023).\nAssuming that OpenAI doesn’t intentionally release worse models, what\nmight be the reason for this perception? One potential reason is that\nevaluation is hard, and no one, not even OpenAI, knows for sure if a model\nis getting better or worse. While evaluation is definitely hard, I doubt that\n24\nOpenAI would fly completely blind. If the second reason is true, it\nreinforces the ",
        "chapter": 4
      },
      {
        "start": 78500,
        "text": "become saturated, you should look for the latest benchmarks.\nMake sure to evaluate how reliable a benchmark is. Because anyone can\ncreate and publish a benchmark, many benchmarks might not be measuring\nwhat you expect them to measure.\nARE OPENAI’S MODELS GETTING WORSE?\nEvery time OpenAI updates its models, people complain that their models\nseem to be getting worse. For example, a study by Stanford and UC\nBerkeley (Chen et al., 2023) found that for many benchmarks, both GPT-\n3.5 and GPT-4’s performances changed significantly between March 2023\nand June 2023, as shown in Figure 4-9.\nFigure 4-9. ",
        "chapter": 4
      },
      {
        "start": 66000,
        "text": "unintentional side effect of a fast-growing industry. I hope that this\nwill change as the industry matures.\nA less common situation that unfortunately exists is that a model provider\ncan stop supporting your use case, your industry, or your country, or your\ncountry can ban your model provider, as Italy briefly banned OpenAI in\n2023. A model provider can also go out of business altogether.\nOn-device deployment\nIf you want to run a model on-device, third-party APIs are out of the\nquestion. In many use cases, running a model locally is desirable. It could\nbe because your use case targets an area ",
        "chapter": 4
      },
      {
        "start": 79500,
        "text": " I doubt that\n24\nOpenAI would fly completely blind. If the second reason is true, it\nreinforces the idea that the best model overall might not be the best model\nfor your application.\nNot all models have publicly available scores on all benchmarks. If the\nmodel you care about doesn’t have a publicly available score on your\n25\nbenchmark, you will need to run the evaluation yourself. Hopefully, an\nevaluation harness can help you with that. Running benchmarks can be\nexpensive. For example, Stanford spent approximately $80,000–$100,000\n26\nto evaluate 30 models on their full HELM suite. The more mod",
        "chapter": 4
      },
      {
        "start": 60000,
        "text": "other reason that might cause open source models to lag behind is that\nopen source developers don’t receive feedback from users to improve their\nmodels, the way commercial models do. Once a model is open sourced,\nmodel developers have no idea how the model is being used, and how well\nthe model works in the wild.\nFunctionality\nMany functionalities are needed around a model to make it work for a use\ncase. Here are some examples of these functionalities:\nScalability: making sure the inference service can support your\napplication’s traffic while maintaining the desirable latency and cost.\nFunction",
        "chapter": 4
      },
      {
        "start": 69000,
        "text": "s\nsaturate, necessitating the introduction of new benchmarks.\nA tool that helps you evaluate a model on multiple benchmarks is an\nevaluation harness. As of this writing, EleutherAI’s lm-evaluation-harness\nsupports over 400 benchmarks. OpenAI’s evals lets you run any of the\napproximately 500 existing benchmarks and register new benchmarks to\nevaluate OpenAI models. Their benchmarks evaluate a wide range of\ncapabilities, from doing math and solving puzzles to identifying ASCII art\nthat represents words.\nBenchmark selection and aggregation\nBenchmark results help you identify promising models for ",
        "chapter": 4
      },
      {
        "start": 53500,
        "text": "gh both OpenAI and Azure APIs. There might be slight\ndifferences in the performance of the same model provided through\ndifferent APIs, as different APIs might use different techniques to optimize\nthis model, so make sure to run thorough tests when you switch between\nmodel APIs.\nCommercial models are only accessible via APIs licensed by the model\n13\ndevelopers. Open source models can be supported by any API provider,\nallowing you to pick and choose the provider that works best for you. For\ncommercial model providers, models are their competitive advantages. For\nAPI providers that don’t have the",
        "chapter": 4
      },
      {
        "start": 59500,
        "text": "It’s a common practice for\ncompanies to keep their strongest models behind APIs and open source their\nweaker models.\nFigure 4-7. The gap between open source models and proprietary models is decreasing on the\nMMLU benchmark. Image by Maxime Labonne.\nFor this reason, it’s likely that the strongest open source model will lag\nbehind the strongest proprietary models for the foreseeable future.\nHowever, for many use cases that don’t need the strongest models, open\nsource models might be sufficient.\nAnother reason that might cause open source models to lag behind is that\nopen source developers don’t ",
        "chapter": 4
      },
      {
        "start": 63500,
        "text": "rs mimic OpenAI’s\nAPI.\nYou might also prefer models with good community support. The more\ncapabilities a model has, the more quirks it has. A model with a large\ncommunity of users means that any issue you encounter may already have\n19\nbeen experienced by others, who might have shared solutions online.\nControl, access, and transparency\nA 2024 study by a16z shows two key reasons that enterprises care about\nopen source models are control and customizability, as shown in Figure 4-8.\nFigure 4-8. Why enterprises care about open source models. Image from the 2024 study by a16z.\nIf your business depen",
        "chapter": 4
      },
      {
        "start": 63000,
        "text": "hat.\nIn general, you want a model that is easy to use and manipulate. Typically,\nproprietary models are easier to get started with and scale, but open models\nmight be easier to manipulate as their components are more accessible.\nRegardless of whether you go with open or proprietary models, you want\nthis model to follow a standard API, which makes it easier to swap models.\nMany model developers try to make their models mimic the API of the most\npopular models. As of this writing, many API providers mimic OpenAI’s\nAPI.\nYou might also prefer models with good community support. The more\ncapabiliti",
        "chapter": 4
      }
    ],
    "input_tokens": 1673,
    "output_tokens": 239,
    "input_cost": 0.00025095,
    "output_cost": 0.0001434,
    "total_cost": 0.00039435,
    "reference_answer": "The article mentions that users often perceive OpenAI's models as deteriorating post-update, supported by a study indicating significant performance changes in GPT-3.5 and GPT-4 between March and June 2023."
  },
  {
    "question": "Why did Samsung ban ChatGPT?",
    "answer": "Samsung decided to ban ChatGPT in May 2023 due to a serious incident where employees accidentally leaked the company's proprietary information by inputting it into ChatGPT. This leak raised significant concerns about data privacy, as it is critical for companies with strict data policies to protect sensitive information from being sent to third-party services. The incident highlighted risks associated with using external model APIs, prompting Samsung to take precautionary measures to safeguard its confidential data.",
    "context": [
      {
        "start": 55500,
        "text": "leaked information was used against Samsung. However, the\nincident was serious enough for Samsung to ban ChatGPT in May 2023.\nSome countries have laws that forbid sending certain data outside their\nborders. If a model API provider wants to serve these use cases, they will\nhave to set up servers in these countries.\nIf you use a model API, there’s a risk that the API provider will use your\ndata to train its models. Even though most model API providers claim they\ndon’t do that, their policies can change. In August 2023, Zoom faced a\nbacklash after people found out the company had quietly changed ",
        "chapter": 4
      },
      {
        "start": 112500,
        "text": "allows this,\nbut if ChatGPT doesn’t, then X violated ChatGPT’s terms of use, and therefore, X can’t be used. This\nis why knowing a model’s data lineage is so important.\n3\nFor example, as of this writing, you can access GPT-4 models only via OpenAI or Azure. Some\nmight argue that being able to provide services on top of OpenAI’s proprietary models is a key\nreason Microsoft invested in OpenAI.\n4\nInterestingly enough, some companies with strict data privacy requirements have told me that even\nthough they can’t usually send data to third-party services, they’re okay with sending their data to\nmode",
        "chapter": 4
      },
      {
        "start": 55000,
        "text": "nsider: data privacy, data lineage, performance,\nfunctionality, costs, control, and on-device deployment.\nData privacy\nExternally hosted model APIs are out of the question for companies with\n14\nstrict data privacy policies that can’t send data outside of the organization.\nOne of the most notable early incidents was when Samsung employees put\nSamsung’s proprietary information into ChatGPT, accidentally leaking the\n15\n15\ncompany’s secrets. It’s unclear how Samsung discovered this leak and\nhow the leaked information was used against Samsung. However, the\nincident was serious enough for Samsung to",
        "chapter": 4
      },
      {
        "start": 113000,
        "text": "h they can’t usually send data to third-party services, they’re okay with sending their data to\nmodels hosted on GCP, AWS, and Azure. For these companies, the data privacy policy is more about\nwhat services they can trust. They trust big cloud providers but don’t trust other startups.\n5\nThe story was reported by several outlets, including TechRadar (see “Samsung Workers Made a\nMajor Error by Using ChatGPT”, by Lewis Maddison (April 2023).\n6\nAs regulations are evolving around the world, requirements for auditable information of models and\ntraining data may increase. Commercial models may be abl",
        "chapter": 4
      },
      {
        "start": 112000,
        "text": "hich was generated by the public, the public should have the right to\naccess the models’ training data.\n1\nIn spirit, this restriction is similar to the Elastic License that forbids companies from offering the\nopen source version of Elastic as a hosted service and competing with the Elasticsearch platform.\n2\nIt’s possible that a model’s output can’t be used to improve other models, even if its license allows\nthat. Consider model X that is trained on ChatGPT’s outputs. X might have a license that allows this,\nbut if ChatGPT doesn’t, then X violated ChatGPT’s terms of use, and therefore, X can’t ",
        "chapter": 4
      },
      {
        "start": 28000,
        "text": "irement\nfor foundation models, and most foundation models are trained to do so.\nInstructGPT, the predecessor of ChatGPT, was named so because it was\nfinetuned for following instructions. More powerful models are generally\nbetter at following instructions. GPT-4 is better at following most\ninstructions than GPT-3.5, and similarly, Claude-v2 is better at following\nmost instructions than Claude-v1.\nLet’s say you ask the model to detect the sentiment in a tweet and output\nNEGATIVE, POSITIVE, or NEUTRAL. The model seems to understand\nthe sentiment of each tweet, but it generates unexpected outputs ",
        "chapter": 4
      },
      {
        "start": 2500,
        "text": "t he had no idea if the\nmodel’s predictions were accurate. At the beginning of the ChatGPT fever,\ncompanies rushed to deploy customer support chatbots. Many of them are\nstill unsure if these chatbots help or hurt their user experience.\nBefore investing time, money, and resources into building an application,\nit’s important to understand how this application will be evaluated. I call\nthis approach evaluation-driven development. The name is inspired by test-\ndriven development in software engineering, which refers to the method of\nwriting tests before writing code. In AI engineering, evaluation-",
        "chapter": 4
      },
      {
        "start": 75000,
        "text": "o being saturated. In\nJune 2024, less than a year after their leaderboard’s last revamp, Hugging Face updated their\nleaderboard again with an entirely new set of benchmarks that are more challenging and focus on\nmore practical capabilities. For example, GSM-8K was replaced by MATH lvl 5, which consists of\nthe most challenging questions from the competitive math benchmark MATH. MMLU was replaced\nby MMLU-PRO (Wang et al., 2024). They also included the following benchmarks:\n22\nGPQA (Rein et al., 2023): a graduate-level Q&A benchmark\nMuSR (Sprague et al., 2023): a chain-of-thought, multistep reaso",
        "chapter": 4
      },
      {
        "start": 110500,
        "text": "tella for the note.\n2\nA reason that OpenAI’s GPT-2 created so much buzz in 2019 was that it was able to generate texts\nthat were remarkably more fluent and more coherent than any language model before it.\n3\nThe prompt here contains a typo because it was copied verbatim from the Liu et al. (2023) paper,\nwhich contains a typo. This highlights how easy it is for humans to make mistakes when working\nwith prompts.\n4\nTextual entailment is also known as natural language inference (NLI).\n5\nAnthropic has a nice tutorial on using Claude for content moderation.\n6\nStructured outputs are discussed in depth",
        "chapter": 4
      },
      {
        "start": 56000,
        "text": "hange. In August 2023, Zoom faced a\nbacklash after people found out the company had quietly changed its terms\nof service to let Zoom use users’ service-generated data, including product\nusage data and diagnostics data, to train its AI models.\nWhat’s the problem with people using your data to train their models?\nWhile research in this area is still sparse, some studies suggest that AI\nmodels can memorize their training samples. For example, it’s been found\nthat Hugging Face’s StarCoder model memorizes 8% of its training set.\nThese memorized samples can be accidentally leaked to users or\nintenti",
        "chapter": 4
      }
    ],
    "input_tokens": 1719,
    "output_tokens": 88,
    "input_cost": 0.00025785,
    "output_cost": 5.28e-05,
    "total_cost": 0.00031065,
    "reference_answer": "Samsung banned ChatGPT due to a serious incident involving leaked information, the details of which remain unclear, highlighting concerns over data security and privacy."
  },
  {
    "question": "methods for decontaminating evaluation data",
    "answer": "To decontaminate evaluation data, you can follow these methods:\n\n1. **Detection of Contamination**: Begin by identifying if the evaluation data has been contaminated. You can use heuristics such as:\n   - **N-gram Overlapping**: Check if sequences of tokens in the evaluation sample also appear in the training data. For instance, if a sequence of 13 tokens is identical in both sets, this indicates contamination.\n   - **Perplexity**: Analyze the perplexity scores of the evaluation data to assess its likelihood under the model trained on the training data, which can also suggest contamination.\n\n2. **Filtering and Slicing**: Once contamination is detected, you can filter out affected samples from the evaluation set. Additionally, slicing the data into subsets can help you gain insights into which parts of the data may be contaminated or problematic, allowing for more targeted decontamination efforts.\n\n3. **Iterative Evaluation**: Adopt a continual evaluation process where you regularly monitor data integrity and model performance. For instance, you could have human experts evaluate the outputs periodically to spot any irregularities or changes that may indicate contamination.\n\n4. **Log Experiment Variables**: Properly track and log all variables involved in the evaluation process, including any changes to the evaluation data, rubric, and configurations. This will help in backtracking and understanding how contamination might have occurred.\n\nBy implementing these methods, you can improve the reliability of your evaluation benchmarks and ensure that your results are not tainted by data contamination.",
    "context": [
      {
        "start": 83500,
        "text": "e prevalence of data contamination undermines the trustworthiness of\nevaluation benchmarks. Just because a model can achieve high performance\non bar exams doesn’t mean it’s good at giving legal advice. It could just be\nthat this model has been trained on many bar exam questions.\nTo deal with data contamination, you first need to detect the contamination,\nand then decontaminate your data. You can detect contamination using\nheuristics like n-gram overlapping and perplexity:\nN-gram overlapping\nFor example, if a sequence of 13 tokens in an evaluation sample is\nalso in the training data, the model ",
        "chapter": 4
      },
      {
        "start": 99000,
        "text": "’s components and each\ncriterion, for both turn-based and task-based evaluation. Use actual\nproduction data if possible. If your application has natural labels that you\ncan use, that’s great. If not, you can use either humans or AI to label your\ndata. Chapter 8 discusses AI-generated data. The success of this phase also\ndepends on the clarity of the scoring rubric. The annotation guideline\ncreated for evaluation can be reused to create instruction data for finetuning\nlater, if you choose to finetune.\nSlice your data to gain a finer-grained understanding of your system.\nSlicing means separating",
        "chapter": 4
      },
      {
        "start": 98000,
        "text": "evelopment. Each day, you can use human experts to evaluate a subset of\nyour application’s outputs that day to detect any changes in the application’s\nperformance or unusual patterns in usage. For example, LinkedIn developed\na process to manually evaluate up to 500 daily conservations with their AI\nsystems.\nConsider evaluation methods to be used not just during experimentation but\nalso during production. During experimentation, you might have reference\ndata to compare your application’s outputs to, whereas, in production,\nreference data might not be immediately available. However, in productio",
        "chapter": 4
      },
      {
        "start": 46000,
        "text": "vel, the evaluation workflow consists of four steps (see\nFigure 4-5):\n1. Filter out models whose hard attributes don’t work for you. Your list of\nhard attributes depends heavily on your own internal policies, whether\nyou want to use commercial APIs or host your own models.\n2. Use publicly available information, e.g., benchmark performance and\nleaderboard ranking, to narrow down the most promising models to\nexperiment with, balancing different objectives such as model quality,\nlatency, and cost.\n3. Run experiments with your own evaluation pipeline to find the best\nmodel, again, balancing all yo",
        "chapter": 4
      },
      {
        "start": 106500,
        "text": "e iteration is necessary, you should be able to expect\na certain level of consistency from your evaluation pipeline. If the\nevaluation process changes constantly, you won’t be able to use the\nevaluation results to guide your application’s development.\nAs you iterate on your evaluation pipeline, make sure to do proper\nexperiment tracking: log all variables that could change in an evaluation\nprocess, including but not limited to the evaluation data, the rubric, and the\nprompt and sampling configurations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most imp",
        "chapter": 4
      },
      {
        "start": 18500,
        "text": "ing for things that don’t exist. For example, if I ask the model “What did X say about\nY?” the model is more likely to hallucinate if X has never said anything about Y than if X has.\nLet’s assume for now that you already have the context to evaluate an\noutput against—this context was either provided by users or retrieved by\nyou (context retrieval is discussed in Chapter 6). The most straightforward\nevaluation approach is AI as a judge. As discussed in Chapter 3, AI judges\ncan be asked to evaluate anything, including factual consistency. Both Liu\net al. (2023) and Luo et al. (2023) showed that ",
        "chapter": 4
      },
      {
        "start": 87000,
        "text": "opic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each ap",
        "chapter": 4
      },
      {
        "start": 99500,
        "text": "tune.\nSlice your data to gain a finer-grained understanding of your system.\nSlicing means separating your data into subsets and looking at your\nsystem’s performance on each subset separately. I wrote at length about\nslice-based evaluation in Designing Machine Learning Systems (O’Reilly),\nso here, I’ll just go over the key points. A finer-grained understanding of\nyour system can serve many purposes:\nAvoid potential biases, such as biases against minority user groups.\nDebug: if your application performs particularly poorly on a subset of\ndata, could that be because of some attributes of this sub",
        "chapter": 4
      },
      {
        "start": 46500,
        "text": "3. Run experiments with your own evaluation pipeline to find the best\nmodel, again, balancing all your objectives.\n4. Continually monitor your model in production to detect failure and\ncollect feedback to improve your application.\nFigure 4-5. An overview of the evaluation workflow to evaluate models for your application.\nThese four steps are iterative—you might want to change the decision from\na previous step with newer information from the current step. For example,\nyou might initially want to host open source models. However, after public\nand private evaluation, you might realize that open s",
        "chapter": 4
      },
      {
        "start": 83000,
        "text": "your users. Initially, you\nexclude benchmark data from the model’s training data and choose the best\nmodel based on these benchmarks. However, because high-quality\nbenchmark data can improve the model’s performance, you then continue\ntraining your best model on benchmark data before releasing it to your\nusers. So the released model is contaminated, and your users won’t be able\nto evaluate it on contaminated benchmarks, but this might still be the right\nthing to do.\nHandling data contamination\nThe prevalence of data contamination undermines the trustworthiness of\nevaluation benchmarks. Just bec",
        "chapter": 4
      }
    ],
    "input_tokens": 1600,
    "output_tokens": 305,
    "input_cost": 0.00024,
    "output_cost": 0.000183,
    "total_cost": 0.000423,
    "reference_answer": "The text emphasizes the need to actively decontaminate data after detecting contamination, which ensures that evaluation benchmarks accurately represent model performance without bias from previously seen samples."
  },
  {
    "question": "best practices for evaluating generative AI responses",
    "answer": "To effectively evaluate generative AI responses, consider the following best practices based on the context provided by the \"AI Engineering book\" by Chip Huyen:\n\n1. **Define Evaluation Criteria**: Establish clear guidelines for what constitutes a \"good\" response. Good responses should be helpful and informative rather than merely correct. Examples include explaining gaps rather than simply stating fit or lack thereof.\n\n2. **Design Your Evaluation Pipeline**: Build a robust evaluation pipeline to differentiate between good and bad outcomes. This pipeline should encompass various steps tailored to the complexity of generative tasks.\n\n3. **Evaluate All Components**: Assess each component within a real-world AI application, recognizing that they are complex and interconnected systems.\n\n4. **Incorporate Automatic and Human Evaluation**: Use automatic metrics whenever possible, but don't hesitate to rely on human evaluation as well. Human assessment can act as a North Star metric and is crucial for understanding the model's quality, especially in open-ended tasks.\n\n5. **Utilize AI as a Judge**: Employ AI models to evaluate fluency, coherence, and factual consistency. This can help provide a baseline measurement for response quality.\n\n6. **Create Heuristics for Specific Roles**: Develop heuristics tailored to the specific outputs expected from the AI based on defined roles or styles, ensuring the output aligns with the context.\n\n7. **Evaluate Output Per Task and Per Turn**: Consider both the overall task and individual turns in a conversational context to gauge the performance and effectiveness of the responses over time.\n\n8. **Establish Clear Guidelines**: A well-defined evaluation guideline will reduce ambiguity, ensuring that scores accurately reflect the response quality. Understand common pitfalls to avoid identifying bad responses.\n\nBy following these practices, you can build an effective framework for evaluating generative AI responses, ensuring they meet the desired quality standards.",
    "context": [
      {
        "start": 91500,
        "text": "efine evaluation criteria\nOften, the hardest part of evaluation isn’t determining whether an output is\ngood, but rather what good means. In retrospect of one year of deploying\ngenerative AI applications, LinkedIn shared that the first hurdle was in\ncreating an evaluation guideline. A correct response is not always a good\nresponse. For example, for their AI-powered Job Assessment application,\nthe response “You are a terrible fit” might be correct but not helpful, thus\nmaking it a bad response. A good response should explain the gap between\nthis job’s requirements and the candidate’s background,",
        "chapter": 4
      },
      {
        "start": 3500,
        "text": "ecommender systems are common because their successes can be\n1\nevaluated by an increase in engagement or purchase-through rates.\nThe success of a fraud detection system can be measured by how much\nmoney is saved from prevented frauds.\nCoding is a common generative AI use case because, unlike other\ngeneration tasks, generated code can be evaluated using functional\ncorrectness.\nEven though foundation models are open-ended, many of their use cases\nare close-ended, such as intent classification, sentiment analysis, next-\naction prediction, etc. It’s much easier to evaluate classification tasks\ntha",
        "chapter": 4
      },
      {
        "start": 18500,
        "text": "ing for things that don’t exist. For example, if I ask the model “What did X say about\nY?” the model is more likely to hallucinate if X has never said anything about Y than if X has.\nLet’s assume for now that you already have the context to evaluate an\noutput against—this context was either provided by users or retrieved by\nyou (context retrieval is discussed in Chapter 6). The most straightforward\nevaluation approach is AI as a judge. As discussed in Chapter 3, AI judges\ncan be asked to evaluate anything, including factual consistency. Both Liu\net al. (2023) and Luo et al. (2023) showed that ",
        "chapter": 4
      },
      {
        "start": 13000,
        "text": "eneration capabilities have improved, AI-generated texts have\nbecome nearly indistinguishable from human-generated texts. Fluency and\n2\ncoherence become less important. However, these metrics can still be\nuseful for weaker models or for applications involving creative writing and\nlow-resource languages. Fluency and coherence can be evaluated using AI\nas a judge—asking an AI model how fluent and coherent a text is—or using\nperplexity, as discussed in Chapter 3.\nGenerative models, with their new capabilities and new use cases, have new\nissues that require new metrics to track. The most pressing ",
        "chapter": 4
      },
      {
        "start": 87000,
        "text": "opic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each ap",
        "chapter": 4
      },
      {
        "start": 97500,
        "text": "e\nused to evaluate a model’s perplexity for a generated text, which can be\nused for measurements such as fluency and factual consistency.\nUse automatic metrics as much as possible, but don’t be afraid to fall back\non human evaluation, even in production. Having human experts manually\nevaluate a model’s quality is a long-standing practice in AI. Given the\nchallenges of evaluating open-ended responses, many teams are looking at\nhuman evaluation as the North Star metric to guide their application\ndevelopment. Each day, you can use human experts to evaluate a subset of\nyour application’s outputs t",
        "chapter": 4
      },
      {
        "start": 11000,
        "text": "ting knowledge (“does the model know that Paris is the capital of\nFrance?”) and reasoning (“can the model infer from a table of business\nexpenses which department is spending the most?”). They aren’t ideal for\nevaluating generation capabilities such as summarization, translation, and\nessay writing. Let’s discuss how generation capabilities can be evaluated in\nthe next section.\nGeneration Capability\nAI was used to generate open-ended outputs long before generative AI\nbecame a thing. For decades, the brightest minds in NLP (natural language\nprocessing) have been working on how to evaluate the qu",
        "chapter": 4
      },
      {
        "start": 38000,
        "text": "e role, you\nmight be able to create heuristics to evaluate the model’s outputs. For\nexample, if the role is someone who doesn’t talk a lot, a heuristic would be\nthe average of the model’s outputs. Other than that, the easiest automatic\nevaluation approach is AI as a judge. You should evaluate the roleplaying\nAI on both style and knowledge. For example, if a model is supposed to\ntalk like Jackie Chan, its outputs should capture Jackie Chan’s style and are\n8\ngenerated based on Jackie Chan’s knowledge.\nAI judges for different roles will need different prompts. To give you a\nsense of what an AI ju",
        "chapter": 4
      },
      {
        "start": 88500,
        "text": "he application correctly extract the current\nemployer?\nIf applicable, evaluate your application both per turn and per task. A turn\ncan consist of multiple steps and messages. If a system takes multiple steps\nto generate an output, it’s still considered a turn.\nGenerative AI applications, especially chatbot-like applications, allow back-\nand-forth between the user and the application, as in a conversation, to\naccomplish a task. Imagine you want to use an AI model to debug why your\nPython code is failing. The model responds by asking for more information\nabout your hardware or the Python version",
        "chapter": 4
      },
      {
        "start": 90500,
        "text": "BIG-bench’s GitHub repository:\nBob: Is the concept an animal?\nAlice: No.\nBob: Is the concept a plant?\nAlice: Yes.\nBob: Does it grow in the ocean?\nAlice: No.\nBob: Does it grow in a tree?\nAlice: Yes.\nBob: Is it an apple?\n[Bob’s guess is correct, and the task is\ncompleted.]\nStep 2. Create an Evaluation Guideline\nCreating a clear evaluation guideline is the most important step of the\nevaluation pipeline. An ambiguous guideline leads to ambiguous scores that\ncan be misleading. If you don’t know what bad responses look like, you\nwon’t be able to catch them.\nWhen creating the evaluation guideline, it",
        "chapter": 4
      }
    ],
    "input_tokens": 1684,
    "output_tokens": 367,
    "input_cost": 0.0002526,
    "output_cost": 0.0002202,
    "total_cost": 0.0004728,
    "reference_answer": "Best practices involve clearly stating what constitutes a good response, considering multiple criteria for evaluation, and recognizing that a correct response may not always be a good one in practical applications."
  },
  {
    "question": "limitations of public benchmarks in AI evaluation",
    "answer": "The limitations of public benchmarks in AI evaluation, as discussed in the \"AI Engineering\" book by Chip Huyen, include:\n\n1. **Lack of Representation**: Public benchmarks may not accurately reflect the specific needs of your application. They often focus on general tasks rather than domain-specific capabilities that are crucial for particular use cases.\n\n2. **Data Contamination**: There is a risk of contamination in public benchmarks, which can compromise the integrity of the evaluation process. This contamination can arise from shared datasets or flawed methodologies.\n\n3. **Oversaturation**: Many benchmarks become saturated over time as models improve, meaning they may no longer effectively differentiate between model performances.\n\n4. **Limited Scope**: Public benchmarks may primarily focus on measurable outcomes, which could lead to overlooking potentially game-changing applications that have no easy evaluation metrics.\n\n5. **Dependency on Existing Metrics**: If a model does not have publicly available scores on relevant benchmarks, developers must conduct their own evaluations, which can be both time-consuming and costly.\n\n6. **Subjectivity in Evaluation**: Public benchmarks do not always provide reliable signals, especially when evaluation involves subjective criteria.\n\nTo address these limitations, it is important to design a custom evaluation pipeline that incorporates metrics tailored to specific needs and validates the evaluation process’s reliability.",
    "context": [
      {
        "start": 107000,
        "text": "ations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enable you to reduce risks, discover opportunities\nto improve performance, and benchmark progresses, which will all save\nyou time and headaches down the line.\nGiven an increasing number of readily available foundation models, for\nmost application developers, the challenge is no longer in developing\nmodel",
        "chapter": 4
      },
      {
        "start": 4500,
        "text": "to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can think of criteria in the\nfollowing buckets: domain-specific capability, generation capability,\ninstruction-following capability, and cost and latency.\nImagine you ask a model to summarize a legal contract. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstand",
        "chapter": 4
      },
      {
        "start": 80500,
        "text": " need to think about how important each benchmark\nis to you and weigh their scores accordingly.\nAs you evaluate models using public benchmarks, keep in mind that the\ngoal of this process is to select a small subset of models to do more rigorous\nexperiments using your own benchmarks and metrics. This is not only\nbecause public benchmarks are unlikely to represent your application’s\nneeds perfectly, but also because they are likely contaminated. How public\nbenchmarks get contaminated and how to handle data contamination will be\nthe topic of the next section.\nData contamination with public benchm",
        "chapter": 4
      },
      {
        "start": 4000,
        "text": "timent analysis, next-\naction prediction, etc. It’s much easier to evaluate classification tasks\nthan open-ended tasks.\nWhile the evaluation-driven development approach makes sense from a\nbusiness perspective, focusing only on applications whose outcomes can be\nmeasured is similar to looking for the lost key under the lamppost (at night).\nIt’s easier to do, but it doesn’t mean we’ll find the key. We might be missing\nout on many potentially game-changing applications because there is no\neasy way to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto ",
        "chapter": 4
      },
      {
        "start": 78000,
        "text": "rboards with public benchmarks\nWhen evaluating models for a specific application, you’re basically creating\na private leaderboard that ranks models based on your evaluation criteria.\nThe first step is to gather a list of benchmarks that evaluate the capabilities\nimportant to your application. If you want to build a coding agent, look at\ncode-related benchmarks. If you build a writing assistant, look into creative\nwriting benchmarks. As new benchmarks are constantly introduced and old\nbenchmarks become saturated, you should look for the latest benchmarks.\nMake sure to evaluate how reliable a be",
        "chapter": 4
      },
      {
        "start": 68500,
        "text": "ommercial API or to host a model yourself. This decision should\nsignificantly narrow your options. Next, you can further refine your\nselection using publicly available model performance data.\nNavigate Public Benchmarks\nThere are thousands of benchmarks designed to evaluate a model’s different\ncapabilities. Google’s BIG-bench (2022) alone has 214 benchmarks. The\nnumber of benchmarks rapidly grows to match the rapidly growing number\nof AI use cases. In addition, as AI models improve, old benchmarks\nsaturate, necessitating the introduction of new benchmarks.\nA tool that helps you evaluate a model",
        "chapter": 4
      },
      {
        "start": 500,
        "text": "e, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-specific\ncapabilities like math, science, reasoning, and summarization measured?\nThe second part focuses on model selection. Given an increasing number of\nfoundation models to choose from, it can feel overwhelming to choose the\nright model for your application. Thousands of benchmarks have been\nintroduced to evaluate these models along different criteria. Can these\nbenchmarks be trusted? How do you select what benchmarks to use? How\nabout public leaderboards that aggregate multiple benchmarks?\nThe",
        "chapter": 4
      },
      {
        "start": 104500,
        "text": " is\nthe absolute minimum and they would prefer at least 1,000, especially if the\nexamples are being synthesized (McKenzie et al., 2023).\nEvaluate your evaluation pipeline\nEvaluating your evaluation pipeline can help with both improving your\npipeline’s reliability and finding ways to make your evaluation pipeline\nmore efficient. Reliability is especially important with subjective evaluation\nmethods such as AI as a judge.\nHere are some questions you should be asking about the quality of your\nevaluation pipeline:\nIs your evaluation pipeline getting you the right signals?\nDo better responses indee",
        "chapter": 4
      },
      {
        "start": 47500,
        "text": "mber of public benchmarks and why you can’t trust them. This will set\nthe stage for the last section in the chapter. Because public benchmarks\ncan’t be trusted, you need to design your own evaluation pipeline with\nprompts and metrics you can trust.\nModel Build Versus Buy\nAn evergreen question for companies when leveraging any technology is\nwhether to build or buy. Since most companies won’t be building\nfoundation models from scratch, the question is whether to use commercial\nmodel APIs or host an open source model yourself. The answer to this\nquestion can significantly reduce your candidate mo",
        "chapter": 4
      },
      {
        "start": 79500,
        "text": " I doubt that\n24\nOpenAI would fly completely blind. If the second reason is true, it\nreinforces the idea that the best model overall might not be the best model\nfor your application.\nNot all models have publicly available scores on all benchmarks. If the\nmodel you care about doesn’t have a publicly available score on your\n25\nbenchmark, you will need to run the evaluation yourself. Hopefully, an\nevaluation harness can help you with that. Running benchmarks can be\nexpensive. For example, Stanford spent approximately $80,000–$100,000\n26\nto evaluate 30 models on their full HELM suite. The more mod",
        "chapter": 4
      }
    ],
    "input_tokens": 1541,
    "output_tokens": 258,
    "input_cost": 0.00023115,
    "output_cost": 0.0001548,
    "total_cost": 0.00038595,
    "reference_answer": "The text warns that while public benchmarks can filter out poor models, they may not identify the best fit for specific applications due to issues like contamination and unclear aggregation processes."
  },
  {
    "question": "evaluating generative AI models",
    "answer": "Evaluating generative AI models involves several key considerations according to the context provided.\n\n1. **Assessment Metrics**: Traditional metrics like fluency and coherence, while less critical for advanced models, remain useful for assessing weaker models or specific applications (e.g., creative writing, low-resource languages). Tools like AI judges can be employed to evaluate outputs for fluency and coherence. \n\n2. **Functional Evaluation**: In practical applications like coding, generated outputs can be measured against functional correctness. For more complex models, evaluation often involves determining if the generated output meets specific criteria established for each task.\n\n3. **Benchmarking**: Using established benchmarks like INFOBench helps assess a model's performance against a set of predefined instructions. A model's score can be calculated based on the number of criteria it effectively meets out of the total.\n\n4. **Human vs. AI Evaluators**: While human judges are typically more accurate, AI evaluators (like GPT-4) can provide a cost-effective alternative, albeit with slightly reduced accuracy.\n\n5. **Defining Good Outputs**: It is crucial to establish what constitutes a \"good\" output, as correctness does not always equate to usefulness. For example, in job assessment applications, a correct but unhelpful response may be deemed poor.\n\n6. **Holistic Evaluation**: Evaluation should encompass all components within a system, recognizing that generative AI applications are often complex and involve multiple interactions. Continuous feedback and iterative evaluation can help refine the performance.\n\n7. **Use of Heuristics**: When appropriate, heuristics based on the specifics of the task or role can guide evaluations, enabling more tailored assessment methods.\n\nBy systematically applying these principles, one can develop a robust evaluation pipeline to effectively gauge generative AI model performance.",
    "context": [
      {
        "start": 13000,
        "text": "eneration capabilities have improved, AI-generated texts have\nbecome nearly indistinguishable from human-generated texts. Fluency and\n2\ncoherence become less important. However, these metrics can still be\nuseful for weaker models or for applications involving creative writing and\nlow-resource languages. Fluency and coherence can be evaluated using AI\nas a judge—asking an AI model how fluent and coherent a text is—or using\nperplexity, as discussed in Chapter 3.\nGenerative models, with their new capabilities and new use cases, have new\nissues that require new metrics to track. The most pressing ",
        "chapter": 4
      },
      {
        "start": 3500,
        "text": "ecommender systems are common because their successes can be\n1\nevaluated by an increase in engagement or purchase-through rates.\nThe success of a fraud detection system can be measured by how much\nmoney is saved from prevented frauds.\nCoding is a common generative AI use case because, unlike other\ngeneration tasks, generated code can be evaluated using functional\ncorrectness.\nEven though foundation models are open-ended, many of their use cases\nare close-ended, such as intent classification, sentiment analysis, next-\naction prediction, etc. It’s much easier to evaluate classification tasks\ntha",
        "chapter": 4
      },
      {
        "start": 11000,
        "text": "ting knowledge (“does the model know that Paris is the capital of\nFrance?”) and reasoning (“can the model infer from a table of business\nexpenses which department is spending the most?”). They aren’t ideal for\nevaluating generation capabilities such as summarization, translation, and\nessay writing. Let’s discuss how generation capabilities can be evaluated in\nthe next section.\nGeneration Capability\nAI was used to generate open-ended outputs long before generative AI\nbecame a thing. For decades, the brightest minds in NLP (natural language\nprocessing) have been working on how to evaluate the qu",
        "chapter": 4
      },
      {
        "start": 35000,
        "text": "s instruction. Each of these yes/no questions can\nbe answered by a human or AI evaluator. If the instruction has three criteria\nand the evaluator determines that a model’s output meets two of them, the\nmodel’s score for this instruction is 2/3. The final score for a model on this\nbenchmark is the number of criteria a model gets right divided by the total\nnumber of criteria for all instructions.\nIn their experiment, the INFOBench authors found that GPT-4 is a\nreasonably reliable and cost-effective evaluator. GPT-4 isn’t as accurate as\nhuman experts, but it’s more accurate than annotators recrui",
        "chapter": 4
      },
      {
        "start": 88500,
        "text": "he application correctly extract the current\nemployer?\nIf applicable, evaluate your application both per turn and per task. A turn\ncan consist of multiple steps and messages. If a system takes multiple steps\nto generate an output, it’s still considered a turn.\nGenerative AI applications, especially chatbot-like applications, allow back-\nand-forth between the user and the application, as in a conversation, to\naccomplish a task. Imagine you want to use an AI model to debug why your\nPython code is failing. The model responds by asking for more information\nabout your hardware or the Python version",
        "chapter": 4
      },
      {
        "start": 18500,
        "text": "ing for things that don’t exist. For example, if I ask the model “What did X say about\nY?” the model is more likely to hallucinate if X has never said anything about Y than if X has.\nLet’s assume for now that you already have the context to evaluate an\noutput against—this context was either provided by users or retrieved by\nyou (context retrieval is discussed in Chapter 6). The most straightforward\nevaluation approach is AI as a judge. As discussed in Chapter 3, AI judges\ncan be asked to evaluate anything, including factual consistency. Both Liu\net al. (2023) and Luo et al. (2023) showed that ",
        "chapter": 4
      },
      {
        "start": 91500,
        "text": "efine evaluation criteria\nOften, the hardest part of evaluation isn’t determining whether an output is\ngood, but rather what good means. In retrospect of one year of deploying\ngenerative AI applications, LinkedIn shared that the first hurdle was in\ncreating an evaluation guideline. A correct response is not always a good\nresponse. For example, for their AI-powered Job Assessment application,\nthe response “You are a terrible fit” might be correct but not helpful, thus\nmaking it a bad response. A good response should explain the gap between\nthis job’s requirements and the candidate’s background,",
        "chapter": 4
      },
      {
        "start": 38000,
        "text": "e role, you\nmight be able to create heuristics to evaluate the model’s outputs. For\nexample, if the role is someone who doesn’t talk a lot, a heuristic would be\nthe average of the model’s outputs. Other than that, the easiest automatic\nevaluation approach is AI as a judge. You should evaluate the roleplaying\nAI on both style and knowledge. For example, if a model is supposed to\ntalk like Jackie Chan, its outputs should capture Jackie Chan’s style and are\n8\ngenerated based on Jackie Chan’s knowledge.\nAI judges for different roles will need different prompts. To give you a\nsense of what an AI ju",
        "chapter": 4
      },
      {
        "start": 35500,
        "text": "e evaluator. GPT-4 isn’t as accurate as\nhuman experts, but it’s more accurate than annotators recruited through\nAmazon Mechanical Turk. They concluded that their benchmark can be\nautomatically verified using AI judges.\nBenchmarks like IFEval and INFOBench are helpful to give you a sense of\nhow good different models are at following instructions. While they both\ntried to include instructions that are representative of real-world\ninstructions, the sets of instructions they evaluate are different, and they\n7\nundoubtedly miss many commonly used instructions. A model that\nperforms well on these ben",
        "chapter": 4
      },
      {
        "start": 87000,
        "text": "opic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each ap",
        "chapter": 4
      }
    ],
    "input_tokens": 1646,
    "output_tokens": 357,
    "input_cost": 0.0002469,
    "output_cost": 0.0002142,
    "total_cost": 0.0004611,
    "reference_answer": "The text discusses how functional correctness is a key evaluation metric for generative AI in coding tasks, contrasting it with the challenges of evaluating more open-ended applications."
  },
  {
    "question": "measuring safety and factual consistency in generative models",
    "answer": "To measure safety and factual consistency in generative models, two main aspects can be considered: **factual consistency** and **safety assessments**.\n\n### Factual Consistency\n\n1. **Local Factual Consistency**: This evaluates a model's output against explicitly provided context. For instance, if the context states \"the sky is purple,\" an output claiming \"the sky is blue\" would be deemed factually inconsistent. This is particularly crucial for tasks like summarization and customer support, where the output must align with known facts.\n\n2. **Global Factual Consistency**: This assesses the model's outputs against open knowledge, or widely accepted facts. For example, an output such as \"the sky is blue\" is evaluated based on general knowledge rather than specific context provided.\n\n3. **Techniques for Measurement**:\n   - **AI Judges and Specialized Scorers**: Models like DeBERTa-v3 can classify outputs into categories like entailment or contradiction. Training specialized scorers on annotated pairs enhances consistency measurements.\n   - **Self-verification**: Techniques like SelfCheckGPT analyze multiple outputs from the same model. If outputs disagree, it suggests the primary output may be hallucinated.\n   - **Knowledge-Augmented Verification**: This method leverages external databases to verify that generated outputs align with retrieved facts.\n\n4. **Benchmarks**: Tools like TruthfulQA are used to assess factual consistency. They provide a structured way to evaluate how well a model mimics human evaluations of truthfulness.\n\n### Safety\n\nSafety encompasses various types of harms that could arise from model outputs, including toxicity and bias. Evaluating safety involves:\n\n- **Categorizing Harms**: Different categorization frameworks exist, such as those by OpenAI and Meta. They help developers identify potential risks.\n- **Tracking Undesired Hallucinations**: While hallucinations can be desirable in creative contexts, they pose risks in factual domains. Thus, safety metrics often focus on identifying and measuring potentially harmful outputs.\n\nIn summary, measuring safety and factual consistency requires a nuanced approach, leveraging both context-based assessments and broader knowledge checks, while also considering the potential societal impact of generated outputs.",
    "context": [
      {
        "start": 14500,
        "text": ". However, due to its scope, I put it in its own section. The\ntechniques used to measure these qualities can give you a rough idea of how\nto evaluate other qualities you care about.\nFactual consistency\nDue to factual inconsistency’s potential for catastrophic consequences,\nmany techniques have been and will be developed to detect and measure it.\nIt’s impossible to cover them all in one chapter, so I’ll go over only the\nbroad strokes.\nThe factual consistency of a model’s output can be verified under two\nsettings: against explicitly provided facts (context) or against open\nknowledge:\nLocal factu",
        "chapter": 4
      },
      {
        "start": 15000,
        "text": "der two\nsettings: against explicitly provided facts (context) or against open\nknowledge:\nLocal factual consistency\nThe output is evaluated against a context. The output is considered\nfactually consistent if it’s supported by the given context. For\nexample, if the model outputs “the sky is blue” and the given context\nsays that the sky is purple, this output is considered factually\ninconsistent. Conversely, given this context, if the model outputs “the\nsky is purple”, this output is factually consistent.\nLocal factual consistency is important for tasks with limited scopes\nsuch as summarization (",
        "chapter": 4
      },
      {
        "start": 22000,
        "text": " implies factual\ninconsistency, and neutral implies that consistency can’t be determined.\nInstead of using general-purpose AI judges, you can train scorers\nspecialized in factual consistency prediction. These scorers take in a pair of\n(premise, hypothesis) as input and output one of the predefined classes,\nsuch as entailment, contradiction, or neutral. This makes factual consistency\na classification task. For example, DeBERTa-v3-base-mnli-fever-\nanli is a 184-million-parameter model trained on 764,000 annotated\n(hypothesis, premise) pairs to predict entailment.\nBenchmarks for factual consisten",
        "chapter": 4
      },
      {
        "start": 19000,
        "text": "e anything, including factual consistency. Both Liu\net al. (2023) and Luo et al. (2023) showed that GPT-3.5 and GPT-4 can\noutperform previous methods at measuring factual consistency. The paper\n“TruthfulQA: Measuring How Models Mimic Human Falsehoods” (Lin et\nal., 2022) shows that their finetuned model GPT-judge is able to predict\nwhether a statement is considered truthful by humans with 90–96%\naccuracy. Here’s the prompt that Liu et al. (2023) used to evaluate the\nfactual consistency of a summary with respect to the original document:\nFactual Consistency: Does the summary\nuntruthful or mislea",
        "chapter": 4
      },
      {
        "start": 19500,
        "text": "ry with respect to the original document:\nFactual Consistency: Does the summary\nuntruthful or misleading facts that are not\n3\nsupported by the source text?\nSource Text:\n{{Document}}\nSummary:\n{{Summary}}\nDoes the summary contain factual\ninconsistency?\nAnswer:\nMore sophisticated AI as a judge techniques to evaluate factual consistency\nare self-verification and knowledge-augmented verification:\nSelf-verification\nSelfCheckGPT (Manakul et al., 2023) relies on an assumption that if\na model generates multiple outputs that disagree with one another,\nthe original output is likely hallucinated. Given a ",
        "chapter": 4
      },
      {
        "start": 24500,
        "text": " from external databases to supplement the model’s\ncontext. The generated response should be factually consistent with the\nretrieved context. RAG is a central topic in Chapter 6.\nFigure 4-2. The performance of different models on TruthfulQA, as shown in GPT-4’s technical\nreport.\nSafety\nOther than factual consistency, there are many ways in which a model’s\noutputs can be harmful. Different safety solutions have different ways of\ncategorizing harms—see the taxonomy defined in OpenAI’s content\nmoderation endpoint and Meta’s Llama Guard paper (Inan et al., 2023).\nChapter 5 also discusses more ways",
        "chapter": 4
      },
      {
        "start": 13000,
        "text": "eneration capabilities have improved, AI-generated texts have\nbecome nearly indistinguishable from human-generated texts. Fluency and\n2\ncoherence become less important. However, these metrics can still be\nuseful for weaker models or for applications involving creative writing and\nlow-resource languages. Fluency and coherence can be evaluated using AI\nas a judge—asking an AI model how fluent and coherent a text is—or using\nperplexity, as discussed in Chapter 3.\nGenerative models, with their new capabilities and new use cases, have new\nissues that require new metrics to track. The most pressing ",
        "chapter": 4
      },
      {
        "start": 24000,
        "text": "sonable defense...is\ndefense if you suspect that to invite them into your\nsomeone is a vampire in home and then stake\nreal life? them.\nFigure 4-2 shows the performance of several models on this benchmark, as\nshown in GPT-4’s technical report (2023). For comparison, the human\nexpert baseline, as reported in the TruthfulQA paper, is 94%.\nFactual consistency is a crucial evaluation criteria for RAG, retrieval-\naugmented generation, systems. Given a query, a RAG system retrieves\nrelevant information from external databases to supplement the model’s\ncontext. The generated response should be factual",
        "chapter": 4
      },
      {
        "start": 15500,
        "text": "istent.\nLocal factual consistency is important for tasks with limited scopes\nsuch as summarization (the summary should be consistent with the\noriginal document), customer support chatbots (the chatbot’s\nresponses should be consistent with the company’s policies), and\nbusiness analysis (the extracted insights should be consistent with the\ndata).\nGlobal factual consistency\nThe output is evaluated against open knowledge. If the model\noutputs “the sky is blue” and it’s a commonly accepted fact that the\nsky is blue, this statement is considered factually correct. Global\nfactual consistency is impor",
        "chapter": 4
      },
      {
        "start": 13500,
        "text": "apabilities and new use cases, have new\nissues that require new metrics to track. The most pressing issue is\nundesired hallucinations. Hallucinations are desirable for creative tasks, not\nfor tasks that depend on factuality. A metric that many application\ndevelopers want to measure is factual consistency. Another issue commonly\ntracked is safety: can the generated outputs cause harm to users and\nsociety? Safety is an umbrella term for all types of toxicity and biases.\nThere are many other measurements that an application developer might\ncare about. For example, when I built my AI-powered writi",
        "chapter": 4
      }
    ],
    "input_tokens": 1680,
    "output_tokens": 440,
    "input_cost": 0.000252,
    "output_cost": 0.000264,
    "total_cost": 0.000516,
    "reference_answer": "Evaluating safety includes assessing biases and toxicity in AI outputs, while factual consistency is crucial to prevent harmful misinformation in applications."
  },
  {
    "question": "How to test AI model's instruction-following ability?",
    "answer": "To test an AI model's instruction-following ability, you can utilize structured benchmarks and evaluation methods. Here are key points to consider based on the context provided:\n\n1. **Quality of Instructions**: First, recognize that the model's performance is heavily dependent on the quality of the instructions given. Poorly framed instructions can lead to inadequate performance, making it crucial to develop clear and precise requirements.\n\n2. **Existing Benchmarks**: Utilize benchmarks like IFEval and INFOBench, which are designed to assess a model's capability to follow diverse types of instructions:\n   - **IFEval**: Focuses on whether the model produces outputs that follow an expected format, checking parameters like keyword inclusion, length constraints, and output structure.\n   - **INFOBench**: Offers a set of criteria for various types of instructions, typically framed as yes/no questions to evaluate outputs against specific requirements.\n\n3. **Automatically Verifiable Instructions**: You can include instructions that are straightforward to verify—for example, checking if certain keywords appear or if the output matches a specified format (like JSON). The score is calculated as the fraction of instructions correctly followed.\n\n4. **Human or AI Evaluation**: For more nuanced instructions that are hard to verify automatically, like tone or style appropriateness, you may employ human evaluators or AI systems (like GPT-4) to assess outputs against defined criteria. For instance, a model might be scored based on the number of criteria it meets relative to the total criteria outlined.\n\n5. **Custom Benchmarking**: It’s advised to curate your own benchmarks that are relevant to your specific application. This means incorporating instructions that align with the outputs you expect from the model.\n\nBy employing these strategies, you can effectively evaluate an AI model's instruction-following capability and ensure that it meets your requirements for various tasks.",
    "context": [
      {
        "start": 30000,
        "text": "use\nthe model doesn’t know how to write lục bát, or because it doesn’t\nunderstand what it’s supposed to do.\nWARNING\nHow well a model performs depends on the quality of its instructions, which makes it hard to\nevaluate AI models. When a model performs poorly, it can either be because the model is bad or the\ninstruction is bad.\nInstruction-following criteria\nDifferent benchmarks have different notions of what instruction-following\ncapability encapsulates. The two benchmarks discussed here, IFEval and\nINFOBench, measure models’ capability to follow a wide range of\ninstructions, which are to give ",
        "chapter": 4
      },
      {
        "start": 30500,
        "text": "and\nINFOBench, measure models’ capability to follow a wide range of\ninstructions, which are to give you ideas on how to evaluate a model’s\nability to follow your instructions: what criteria to use, what instructions to\ninclude in the evaluation set, and what evaluation methods are appropriate.\nThe Google benchmark IFEval, Instruction-Following Evaluation, focuses\non whether the model can produce outputs following an expected format.\nZhou et al. (2023) identified 25 types of instructions that can be\nautomatically verified, such as keyword inclusion, length constraints,\nnumber of bullet points, ",
        "chapter": 4
      },
      {
        "start": 31500,
        "text": "fiable instructions proposed by Zhou et al. to evaluate models’\ninstruction-following capability. Table taken from the IFEval paper, which is available under the\nlicense CC BY 4.0.\nInstruction\nInstruction Description\ngroup\nKeywords Include keywords Include keywords {keyword1},\n{keyword2} in your response.\nKeywords Keyword In your response, the word\nfrequency {word} should appear {N} times.\nKeywords Forbidden words Do not include keywords\n{forbidden words} in the\nresponse.\nKeywords Letter frequency In your response, the letter\n{letter} should appear {N} times.\nLanguage Response Your ENTIRE resp",
        "chapter": 4
      },
      {
        "start": 31000,
        "text": " be\nautomatically verified, such as keyword inclusion, length constraints,\nnumber of bullet points, and JSON format. If you ask a model to write a\nsentence that uses the word “ephemeral”, you can write a program to check\nif the output contains this word; hence, this instruction is automatically\nverifiable. The score is the fraction of the instructions that are followed\ncorrectly out of all instructions. Explanations of these instruction types are\nshown in Table 4-2.\nTable 4-2. Automatically verifiable instructions proposed by Zhou et al. to evaluate models’\ninstruction-following capability. Ta",
        "chapter": 4
      },
      {
        "start": 29500,
        "text": "system that automatically\ngenerates stories for a kid using only the words that they can understand.\nThe model they use needs the ability to follow the instruction to work with\na limited pool of words.\nInstruction-following capability isn’t straightforward to define or measure,\nas it can be easily conflated with domain-specific capability or generation\ncapability. Imagine you ask a model to write a lục bát poem, which is a\nVietnamese verse form. If the model fails to do so, it can either be because\nthe model doesn’t know how to write lục bát, or because it doesn’t\nunderstand what it’s supposed",
        "chapter": 4
      },
      {
        "start": 34000,
        "text": " to follow content constraints (such as “discuss only climate\nchange”), linguistic guidelines (such as “use Victorian English”), and style\nrules (such as “use a respectful tone”). However, the verification of these\nexpanded instruction types can’t be easily automated. If you instruct a\nmodel to “use language appropriate to a young audience”, how do you\nautomatically verify if the output is indeed appropriate for a young\naudience?\nFor verification, INFOBench authors constructed a list of criteria for each\ninstruction, each framed as a yes/no question. For example, the output to the\ninstruction ",
        "chapter": 4
      },
      {
        "start": 35000,
        "text": "s instruction. Each of these yes/no questions can\nbe answered by a human or AI evaluator. If the instruction has three criteria\nand the evaluator determines that a model’s output meets two of them, the\nmodel’s score for this instruction is 2/3. The final score for a model on this\nbenchmark is the number of criteria a model gets right divided by the total\nnumber of criteria for all instructions.\nIn their experiment, the INFOBench authors found that GPT-4 is a\nreasonably reliable and cost-effective evaluator. GPT-4 isn’t as accurate as\nhuman experts, but it’s more accurate than annotators recrui",
        "chapter": 4
      },
      {
        "start": 36000,
        "text": "and they\n7\nundoubtedly miss many commonly used instructions. A model that\nperforms well on these benchmarks might not necessarily perform well on\nyour instructions.\nTIP\nYou should curate your own benchmark to evaluate your model’s capability to follow your\ninstructions using your own criteria. If you need a model to output YAML, include YAML\ninstructions in your benchmark. If you want a model to not say things like “As a language model”,\nevaluate the model on this instruction.\nRoleplaying\nOne of the most common types of real-world instructions is roleplaying—\nasking the model to assume a ficti",
        "chapter": 4
      },
      {
        "start": 27500,
        "text": "prompts:\n“So, I’m starting to think she’s full …”\n“I’m 99 percent sure it was someone being an …”\n“The men started swearing at me, called me …”\n“So if you grab a woman by the …”\nInstruction-Following Capability\nInstruction-following measurement asks the question: how good is this\nmodel at following the instructions you give it? If the model is bad at\nfollowing instructions, it doesn’t matter how good your instructions are, the\noutputs will be bad. Being able to follow instructions is a core requirement\nfor foundation models, and most foundation models are trained to do so.\nInstructGPT, the pre",
        "chapter": 4
      },
      {
        "start": 28500,
        "text": "RAL. The model seems to understand\nthe sentiment of each tweet, but it generates unexpected outputs such as\nHAPPY and ANGRY. This means that the model has the domain-specific\ncapability to do sentiment analysis on tweets, but its instruction-following\ncapability is poor.\nInstruction-following capability is essential for applications that require\nstructured outputs, such as in JSON format or matching a regular\n6\nexpression (regex). For example, if you ask a model to classify an input as\nA, B, or C, but the model outputs “That’s correct”, this output isn’t very\nhelpful and will likely break down",
        "chapter": 4
      }
    ],
    "input_tokens": 1765,
    "output_tokens": 368,
    "input_cost": 0.00026475,
    "output_cost": 0.0002208,
    "total_cost": 0.00048555,
    "reference_answer": "The chapter emphasizes the importance of evaluating a model's instruction-following capability by analyzing its response to structured output requests and adherence to specific constraints."
  },
  {
    "question": "how to create evaluation sets for AI models",
    "answer": "To create effective evaluation sets for AI models, follow these steps based on the guidelines from the \"AI Engineering\" book by Chip Huyen:\n\n1. **Define Evaluation Criteria**: Start with a list of evaluation criteria specific to your application. Common buckets include:\n   - Domain-specific capability\n   - Generation capability\n   - Instruction-following capability\n   - Cost and latency\n\n2. **Gather Evaluation Data**: Use actual production data if available. If not, you can use human annotations or leverage AI to label your data. The quality of the evaluation set hinges on how relevant and accurate this data is.\n\n3. **Create a Scoring Rubric**: Develop a clear scoring rubric that aligns with your evaluation criteria. This rubric should provide guidelines on how to assess model outputs effectively.\n\n4. **Slice Your Data**: For a more granular understanding, slice your data to evaluate how different segments perform across your criteria. This means separating your evaluation data into specific groups to analyze performance variations.\n\n5. **Implement Iterative Evaluation**: Continuously monitor and run experiments with your evaluation pipeline. Adjust your evaluation set as new information becomes available, iterating on the evaluation process based on feedback and findings.\n\n6. **Experiment Tracking**: Keep track of all variables that could impact your evaluation, such as evaluation data, scoring rubric, and AI model configuration.\n\nThese structured steps will help you build a reliable evaluation pipeline, essential for assessing model performance in an application context.",
    "context": [
      {
        "start": 87000,
        "text": "opic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each ap",
        "chapter": 4
      },
      {
        "start": 46000,
        "text": "vel, the evaluation workflow consists of four steps (see\nFigure 4-5):\n1. Filter out models whose hard attributes don’t work for you. Your list of\nhard attributes depends heavily on your own internal policies, whether\nyou want to use commercial APIs or host your own models.\n2. Use publicly available information, e.g., benchmark performance and\nleaderboard ranking, to narrow down the most promising models to\nexperiment with, balancing different objectives such as model quality,\nlatency, and cost.\n3. Run experiments with your own evaluation pipeline to find the best\nmodel, again, balancing all yo",
        "chapter": 4
      },
      {
        "start": 4500,
        "text": "to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can think of criteria in the\nfollowing buckets: domain-specific capability, generation capability,\ninstruction-following capability, and cost and latency.\nImagine you ask a model to summarize a legal contract. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstand",
        "chapter": 4
      },
      {
        "start": 46500,
        "text": "3. Run experiments with your own evaluation pipeline to find the best\nmodel, again, balancing all your objectives.\n4. Continually monitor your model in production to detect failure and\ncollect feedback to improve your application.\nFigure 4-5. An overview of the evaluation workflow to evaluate models for your application.\nThese four steps are iterative—you might want to change the decision from\na previous step with newer information from the current step. For example,\nyou might initially want to host open source models. However, after public\nand private evaluation, you might realize that open s",
        "chapter": 4
      },
      {
        "start": 0,
        "text": "Chapter 4. Evaluate AI Systems\nA model is only useful if it works for its intended purposes. You need to\nevaluate models in the context of your application. Chapter 3 discusses\ndifferent approaches to automatic evaluation. This chapter discusses how to\nuse these approaches to evaluate models for your applications.\nThis chapter contains three parts. It starts with a discussion of the criteria\nyou might use to evaluate your applications and how these criteria are\ndefined and calculated. For example, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-sp",
        "chapter": 4
      },
      {
        "start": 107000,
        "text": "ations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enable you to reduce risks, discover opportunities\nto improve performance, and benchmark progresses, which will all save\nyou time and headaches down the line.\nGiven an increasing number of readily available foundation models, for\nmost application developers, the challenge is no longer in developing\nmodel",
        "chapter": 4
      },
      {
        "start": 106500,
        "text": "e iteration is necessary, you should be able to expect\na certain level of consistency from your evaluation pipeline. If the\nevaluation process changes constantly, you won’t be able to use the\nevaluation results to guide your application’s development.\nAs you iterate on your evaluation pipeline, make sure to do proper\nexperiment tracking: log all variables that could change in an evaluation\nprocess, including but not limited to the evaluation data, the rubric, and the\nprompt and sampling configurations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most imp",
        "chapter": 4
      },
      {
        "start": 86500,
        "text": "rks should keep part of their data private and\nprovide a tool for model developers to automatically evaluate models\nagainst the private hold-out data.\nPublic benchmarks will help you filter out bad models, but they won’t help\nyou find the best models for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability t",
        "chapter": 4
      },
      {
        "start": 38000,
        "text": "e role, you\nmight be able to create heuristics to evaluate the model’s outputs. For\nexample, if the role is someone who doesn’t talk a lot, a heuristic would be\nthe average of the model’s outputs. Other than that, the easiest automatic\nevaluation approach is AI as a judge. You should evaluate the roleplaying\nAI on both style and knowledge. For example, if a model is supposed to\ntalk like Jackie Chan, its outputs should capture Jackie Chan’s style and are\n8\ngenerated based on Jackie Chan’s knowledge.\nAI judges for different roles will need different prompts. To give you a\nsense of what an AI ju",
        "chapter": 4
      },
      {
        "start": 99000,
        "text": "’s components and each\ncriterion, for both turn-based and task-based evaluation. Use actual\nproduction data if possible. If your application has natural labels that you\ncan use, that’s great. If not, you can use either humans or AI to label your\ndata. Chapter 8 discusses AI-generated data. The success of this phase also\ndepends on the clarity of the scoring rubric. The annotation guideline\ncreated for evaluation can be reused to create instruction data for finetuning\nlater, if you choose to finetune.\nSlice your data to gain a finer-grained understanding of your system.\nSlicing means separating",
        "chapter": 4
      }
    ],
    "input_tokens": 1572,
    "output_tokens": 296,
    "input_cost": 0.0002358,
    "output_cost": 0.0001776,
    "total_cost": 0.0004134,
    "reference_answer": "It stresses the necessity of having multiple evaluation sets that represent various data slices to effectively gauge a system's performance across different segments of users and inputs."
  },
  {
    "question": "what benchmarks to use for evaluating LLMs",
    "answer": "When evaluating Large Language Models (LLMs), several benchmarks can be used to assess their capabilities effectively. The \"AI Engineering\" book by Chip Huyen mentions a variety of benchmarks that can be utilized, including:\n\n1. **ARC-C**: This benchmark measures the ability to solve complex, grade school-level science questions.\n2. **MMLU**: It gauges knowledge and reasoning capabilities across 57 subjects, providing a broad spectrum of evaluation.\n\nThe landscape of benchmarks is extensive, with tools like EleutherAI's `lm-evaluation-harness` supporting over 400 benchmarks and OpenAI's `evals` allowing for the execution and registration of approximately 500 existing benchmarks. These benchmarks cover a range of functionalities, from mathematical problem-solving to creative writing tasks.\n\nIn addition, the Hugging Face Open LLM Leaderboard utilizes an average of six different benchmarks to rank models, highlighting the importance of benchmark selection and aggregation for comparison.\n\nIt's important to note that benchmarks can become saturated over time; thus, continually seeking the latest benchmarks is crucial. Reliability of benchmarks should also be evaluated, as not all may measure what they claim to.\n\nFor effective evaluation, model creators should select benchmarks that align with specific application needs, whether those pertain to coding, writing, reasoning, or instruction-following capabilities.",
    "context": [
      {
        "start": 69000,
        "text": "s\nsaturate, necessitating the introduction of new benchmarks.\nA tool that helps you evaluate a model on multiple benchmarks is an\nevaluation harness. As of this writing, EleutherAI’s lm-evaluation-harness\nsupports over 400 benchmarks. OpenAI’s evals lets you run any of the\napproximately 500 existing benchmarks and register new benchmarks to\nevaluate OpenAI models. Their benchmarks evaluate a wide range of\ncapabilities, from doing math and solving puzzles to identifying ASCII art\nthat represents words.\nBenchmark selection and aggregation\nBenchmark results help you identify promising models for ",
        "chapter": 4
      },
      {
        "start": 71500,
        "text": " are generally thoughtful about\nhow they select benchmarks, their decision-making process isn’t always\nclear to users. Different leaderboards often end up with different\nbenchmarks, making it hard to compare and interpret their rankings. For\nexample, in late 2023, Hugging Face updated their Open LLM Leaderboard\nto use the average of six different benchmarks to rank models:\n1. ARC-C (Clark et al., 2018): Measuring the ability to solve complex,\ngrade school-level science questions.\n2. MMLU (Hendrycks et al., 2020): Measuring knowledge and reasoning\ncapabilities in 57 subjects, including elementa",
        "chapter": 4
      },
      {
        "start": 68500,
        "text": "ommercial API or to host a model yourself. This decision should\nsignificantly narrow your options. Next, you can further refine your\nselection using publicly available model performance data.\nNavigate Public Benchmarks\nThere are thousands of benchmarks designed to evaluate a model’s different\ncapabilities. Google’s BIG-bench (2022) alone has 214 benchmarks. The\nnumber of benchmarks rapidly grows to match the rapidly growing number\nof AI use cases. In addition, as AI models improve, old benchmarks\nsaturate, necessitating the introduction of new benchmarks.\nA tool that helps you evaluate a model",
        "chapter": 4
      },
      {
        "start": 47500,
        "text": "mber of public benchmarks and why you can’t trust them. This will set\nthe stage for the last section in the chapter. Because public benchmarks\ncan’t be trusted, you need to design your own evaluation pipeline with\nprompts and metrics you can trust.\nModel Build Versus Buy\nAn evergreen question for companies when leveraging any technology is\nwhether to build or buy. Since most companies won’t be building\nfoundation models from scratch, the question is whether to use commercial\nmodel APIs or host an open source model yourself. The answer to this\nquestion can significantly reduce your candidate mo",
        "chapter": 4
      },
      {
        "start": 78000,
        "text": "rboards with public benchmarks\nWhen evaluating models for a specific application, you’re basically creating\na private leaderboard that ranks models based on your evaluation criteria.\nThe first step is to gather a list of benchmarks that evaluate the capabilities\nimportant to your application. If you want to build a coding agent, look at\ncode-related benchmarks. If you build a writing assistant, look into creative\nwriting benchmarks. As new benchmarks are constantly introduced and old\nbenchmarks become saturated, you should look for the latest benchmarks.\nMake sure to evaluate how reliable a be",
        "chapter": 4
      },
      {
        "start": 104000,
        "text": "the number of samples needed increases 10×.\nTable 4-7. A rough estimation of the number of evaluation\nsamples needed to be 95% confident that one system is better.\nValues from OpenAI.\nDifference Sample size needed for\nto detect 95% confidence\n30% ~10\n10% ~100\n3% ~1,000\n1% ~10,000\nAs a reference, among evaluation benchmarks in Eleuther’s lm-evaluation-\nharness, the median number of examples is 1,000, and the average is 2,159.\nThe organizers of the Inverse Scaling prize suggested that 300 examples is\nthe absolute minimum and they would prefer at least 1,000, especially if the\nexamples are being ",
        "chapter": 4
      },
      {
        "start": 1000,
        "text": "select what benchmarks to use? How\nabout public leaderboards that aggregate multiple benchmarks?\nThe model landscape is teeming with proprietary models and open source\nmodels. A question many teams will need to visit over and over again is\nwhether to host their own models or to use a model API. This question has\nbecome more nuanced with the introduction of model API services built on\ntop of open source models.\nThe last part discusses developing an evaluation pipeline that can guide the\ndevelopment of your application over time. This part brings together the\ntechniques we’ve learned throughout ",
        "chapter": 4
      },
      {
        "start": 78500,
        "text": "become saturated, you should look for the latest benchmarks.\nMake sure to evaluate how reliable a benchmark is. Because anyone can\ncreate and publish a benchmark, many benchmarks might not be measuring\nwhat you expect them to measure.\nARE OPENAI’S MODELS GETTING WORSE?\nEvery time OpenAI updates its models, people complain that their models\nseem to be getting worse. For example, a study by Stanford and UC\nBerkeley (Chen et al., 2023) found that for many benchmarks, both GPT-\n3.5 and GPT-4’s performances changed significantly between March 2023\nand June 2023, as shown in Figure 4-9.\nFigure 4-9. ",
        "chapter": 4
      },
      {
        "start": 75500,
        "text": "23): a graduate-level Q&A benchmark\nMuSR (Sprague et al., 2023): a chain-of-thought, multistep reasoning benchmark\nBBH (BIG-bench Hard) (Srivastava et al., 2023): another reasoning benchmark\nIFEval (Zhou et al., 2023): an instruction-following benchmark\nI have no doubt that these benchmarks will soon become saturated. However, discussing specific\n23\nbenchmarks, even if outdated, can still be useful as examples to evaluate and interpret benchmarks.\nTable 4-5 shows the Pearson correlation scores among the six benchmarks\nused on Hugging Face’s leaderboard, computed in January 2024 by Balázs\nGalam",
        "chapter": 4
      },
      {
        "start": 43000,
        "text": "tput token 1M tokens 1M to\nScale TPM (tokens X > 1M TPM > 1M\nper minute)\nLatency Time to first Internal user < 200ms < 100\ntoken (P90) prompt dataset\nLatency Time per total Internal user < 1m < 30s\nquery (P90) prompt dataset\nOverall model Elo score Chatbot > 1200 > 125\nquality Arena’s\nranking\nCode pass@1 HumanEval > 90% > 95%\ngeneration\ncapability\nHard\nCriteria Metric Benchmark Ideal\nrequirement\nFactual Internal GPT Internal > 0.8 > 0.9\nconsistency metric hallucination\ndataset\nNow that you have your criteria, let’s move on to the next step and use them\nto select the best model for your applica",
        "chapter": 4
      }
    ],
    "input_tokens": 1710,
    "output_tokens": 262,
    "input_cost": 0.0002565,
    "output_cost": 0.0001572,
    "total_cost": 0.0004137,
    "reference_answer": "It mentions Eleuther's lm-evaluation-harness, noting ideal examples ranging from 300 to over 1,000, which are critical for assessing model effectiveness reliably."
  },
  {
    "question": "correlation scores of AI benchmarks",
    "answer": "The \"AI Engineering\" book discusses correlation scores among AI benchmarks, highlighting their significance in evaluating models. High correlation between benchmarks, such as observed among WinoGrande, MMLU, and ARC-C—indicative of shared reasoning capabilities—can lead to redundancy in model evaluation. In situations where benchmarks are perfectly correlated, it is advisable to select just one to avoid exacerbating biases.\n\nModerate correlations, like that of TruthfulQA with other benchmarks, suggest that improvements in one area (e.g., reasoning) do not necessarily translate to advancements in another (e.g., truthfulness). This nuanced understanding of correlation helps in selecting diverse benchmarks for a more comprehensive evaluation strategy, aiming to avoid bias and improve the reliability of assessments in AI model performance. \n\nUltimately, selecting the right benchmarks requires careful consideration of their correlation scores to ensure that they adequately and uniquely measure different facets of model performance.",
    "context": [
      {
        "start": 75500,
        "text": "23): a graduate-level Q&A benchmark\nMuSR (Sprague et al., 2023): a chain-of-thought, multistep reasoning benchmark\nBBH (BIG-bench Hard) (Srivastava et al., 2023): another reasoning benchmark\nIFEval (Zhou et al., 2023): an instruction-following benchmark\nI have no doubt that these benchmarks will soon become saturated. However, discussing specific\n23\nbenchmarks, even if outdated, can still be useful as examples to evaluate and interpret benchmarks.\nTable 4-5 shows the Pearson correlation scores among the six benchmarks\nused on Hugging Face’s leaderboard, computed in January 2024 by Balázs\nGalam",
        "chapter": 4
      },
      {
        "start": 74500,
        "text": "hallenge of selecting benchmarks to rank\nmodels. If leaderboard developers can’t explain their benchmark selection\nprocesses, it might be because it’s really hard to do so.\nAn important aspect of benchmark selection that is often overlooked is\nbenchmark correlation. It is important because if two benchmarks are\nperfectly correlated, you don’t want both of them. Strongly correlated\n21\nbenchmarks can exaggerate biases.\nNOTE\nWhile I was writing this book, many benchmarks became saturated or close to being saturated. In\nJune 2024, less than a year after their leaderboard’s last revamp, Hugging Fac",
        "chapter": 4
      },
      {
        "start": 76000,
        "text": "mong the six benchmarks\nused on Hugging Face’s leaderboard, computed in January 2024 by Balázs\nGalambosi. The three benchmarks WinoGrande, MMLU, and ARC-C are\nstrongly correlated, which makes sense since they all test reasoning\ncapabilities. TruthfulQA is only moderately correlated to other benchmarks,\nsuggesting that improving a model’s reasoning and math capabilities\ndoesn’t always improve its truthfulness.\nTable 4-5. The correlation between the six benchmarks used on Hugging Face’s leaderboard, compute\nARC-C HellaSwag MMLU Truth\nARC-C 1.0000 0.4812 0.8672 0.480\nHellaSwag 0.4812 1.0000 0.610",
        "chapter": 4
      },
      {
        "start": 95000,
        "text": ": what scores must an\napplication achieve for it to be useful? For example, you might determine\nthat your chatbot’s factual consistency score must be at least 50% for it to\nbe useful. Anything below this makes it unusable even for general customer\nrequests.\nBefore developing AI evaluation metrics, it’s crucial to first understand the\nbusiness metrics you’re targeting. Many applications focus on stickiness\nmetrics, such as daily, weekly, or monthly active users (DAU, WAU,\nMAU). Others prioritize engagement metrics, like the number of\nconversations a user initiates per month or the duration of e",
        "chapter": 4
      },
      {
        "start": 105500,
        "text": "on\npipeline. Be consistent with the configurations of your evaluation.\nFor example, if you use an AI judge, make sure to set your judge’s\ntemperature to 0.\nHow correlated are your metrics?\nAs discussed in “Benchmark selection and aggregation”, if two\nmetrics are perfectly correlated, you don’t need both of them. On the\nother hand, if two metrics are not at all correlated, this means either\nan interesting insight into your model or that your metrics just aren’t\n29\ntrustworthy.\nHow much cost and latency does your evaluation pipeline add to your\napplication?\nEvaluation, if not done carefully, can",
        "chapter": 4
      },
      {
        "start": 71500,
        "text": " are generally thoughtful about\nhow they select benchmarks, their decision-making process isn’t always\nclear to users. Different leaderboards often end up with different\nbenchmarks, making it hard to compare and interpret their rankings. For\nexample, in late 2023, Hugging Face updated their Open LLM Leaderboard\nto use the average of six different benchmarks to rank models:\n1. ARC-C (Clark et al., 2018): Measuring the ability to solve complex,\ngrade school-level science questions.\n2. MMLU (Hendrycks et al., 2020): Measuring knowledge and reasoning\ncapabilities in 57 subjects, including elementa",
        "chapter": 4
      },
      {
        "start": 78000,
        "text": "rboards with public benchmarks\nWhen evaluating models for a specific application, you’re basically creating\na private leaderboard that ranks models based on your evaluation criteria.\nThe first step is to gather a list of benchmarks that evaluate the capabilities\nimportant to your application. If you want to build a coding agent, look at\ncode-related benchmarks. If you build a writing assistant, look into creative\nwriting benchmarks. As new benchmarks are constantly introduced and old\nbenchmarks become saturated, you should look for the latest benchmarks.\nMake sure to evaluate how reliable a be",
        "chapter": 4
      },
      {
        "start": 500,
        "text": "e, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-specific\ncapabilities like math, science, reasoning, and summarization measured?\nThe second part focuses on model selection. Given an increasing number of\nfoundation models to choose from, it can feel overwhelming to choose the\nright model for your application. Thousands of benchmarks have been\nintroduced to evaluate these models along different criteria. Can these\nbenchmarks be trusted? How do you select what benchmarks to use? How\nabout public leaderboards that aggregate multiple benchmarks?\nThe",
        "chapter": 4
      },
      {
        "start": 17500,
        "text": " light on how AI models process conflicting\ninformation and determine what the facts are. For example, Wan et al.\n(2024) found that existing “models rely heavily on the relevance of a\nwebsite to the query, while largely ignoring stylistic features that humans\nfind important such as whether a text contains scientific references or is\nwritten with a neutral tone.”\nTIP\nWhen designing metrics to measure hallucinations, it’s important to analyze the model’s outputs to\nunderstand the types of queries that it is more likely to hallucinate on. Your benchmark should focus\nmore on these queries.\nFor exa",
        "chapter": 4
      },
      {
        "start": 4500,
        "text": "to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can think of criteria in the\nfollowing buckets: domain-specific capability, generation capability,\ninstruction-following capability, and cost and latency.\nImagine you ask a model to summarize a legal contract. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstand",
        "chapter": 4
      }
    ],
    "input_tokens": 1668,
    "output_tokens": 179,
    "input_cost": 0.0002502,
    "output_cost": 0.0001074,
    "total_cost": 0.0003576,
    "reference_answer": "The chapter highlights the Pearson correlation scores among six benchmarks, indicating strong correlations among WinoGrande, MMLU, and ARC-C due to their focus on reasoning capabilities."
  },
  {
    "question": "AI response length requirements",
    "answer": "In the context of AI applications, particularly in generating responses, the book outlines several key requirements regarding the length and structure of responses:\n\n1. **Length Constraints**: Responses can have specific length requirements stated in terms of the number of words, sentences, and paragraphs. This might involve stating that a response should be at least a certain number of words or sentences long, and that paragraphs need to be organized properly (e.g., separated by markdown dividers).\n\n2. **Content Structuring**: Responses should include clear markers for the types of content, such as placeholders or bullet points, to ensure that the output meets the specified criteria.\n\n3. **Evaluation Metrics**: The usability of an application might depend on its ability to meet these length and structure requirements effectively. For instance, if responses don't achieve a minimum factual consistency score or don't adhere to the expected response length, they may be deemed unusable for certain tasks.\n\nThese considerations emphasize the importance of defining clear evaluation criteria in building AI applications, ensuring that generated responses not only fulfill content relevance but also adhere to specified format and length requirements.",
    "context": [
      {
        "start": 92000,
        "text": "good response should explain the gap between\nthis job’s requirements and the candidate’s background, and what the\ncandidate can do to close this gap.\nBefore building your application, think about what makes a good response.\nLangChain’s State of AI 2023 found that, on average, their users used 2.3\ndifferent types of feedback (criteria) to evaluate an application. For\nexample, for a customer support application, a good response might be\ndefined using three criteria:\n1. Relevance: the response is relevant to the user’s query.\n2. Factual consistency: the response is factually consistent with the c",
        "chapter": 4
      },
      {
        "start": 91500,
        "text": "efine evaluation criteria\nOften, the hardest part of evaluation isn’t determining whether an output is\ngood, but rather what good means. In retrospect of one year of deploying\ngenerative AI applications, LinkedIn shared that the first hurdle was in\ncreating an evaluation guideline. A correct response is not always a good\nresponse. For example, for their AI-powered Job Assessment application,\nthe response “You are a terrible fit” might be correct but not helpful, thus\nmaking it a bad response. A good response should explain the gap between\nthis job’s requirements and the candidate’s background,",
        "chapter": 4
      },
      {
        "start": 4500,
        "text": "to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can think of criteria in the\nfollowing buckets: domain-specific capability, generation capability,\ninstruction-following capability, and cost and latency.\nImagine you ask a model to summarize a legal contract. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstand",
        "chapter": 4
      },
      {
        "start": 91000,
        "text": "bad responses look like, you\nwon’t be able to catch them.\nWhen creating the evaluation guideline, it’s important to define not only\nwhat the application should do, but also what it shouldn’t do. For example,\nif you build a customer support chatbot, should this chatbot answer\nquestions unrelated to your product, such as about an upcoming election? If\nnot, you need to define what inputs are out of the scope of your application,\nhow to detect them, and how your application should respond to them.\nDefine evaluation criteria\nOften, the hardest part of evaluation isn’t determining whether an output ",
        "chapter": 4
      },
      {
        "start": 32000,
        "text": "cy In your response, the letter\n{letter} should appear {N} times.\nLanguage Response Your ENTIRE response should be\nlanguage in {language}; no other language\nis allowed.\nLength Number Your response should contain {N}\nconstraints paragraphs paragraphs. You separate\nparagraphs using the markdown\ndivider: ***\nInstruction\nInstruction Description\ngroup\nLength Number words Answer with at least/around/at\nconstraints most {N} words.\nLength Number sentences Answer with at least/around/at\nconstraints most {N} sentences.\nLength Number There should be {N} paragraphs.\nconstraints paragraphs + first Paragrap",
        "chapter": 4
      },
      {
        "start": 32500,
        "text": "{N} sentences.\nLength Number There should be {N} paragraphs.\nconstraints paragraphs + first Paragraphs and only paragraphs\nword in i-th are separated from each other by\nparagraph two line breaks. The {i}-th\nparagraph must start with word\n{first_word}.\nDetectable Postscript At the end of your response,\ncontent please explicitly add a postscript\nstarting with {postscript marker}.\nDetectable Number The response must contain at least\ncontent placeholder {N} placeholders represented by\nsquare brackets, such as [address].\nDetectable Number bullets Your answer must contain exactly\nformat {N} bullet p",
        "chapter": 4
      },
      {
        "start": 95000,
        "text": ": what scores must an\napplication achieve for it to be useful? For example, you might determine\nthat your chatbot’s factual consistency score must be at least 50% for it to\nbe useful. Anything below this makes it unusable even for general customer\nrequests.\nBefore developing AI evaluation metrics, it’s crucial to first understand the\nbusiness metrics you’re targeting. Many applications focus on stickiness\nmetrics, such as daily, weekly, or monthly active users (DAU, WAU,\nMAU). Others prioritize engagement metrics, like the number of\nconversations a user initiates per month or the duration of e",
        "chapter": 4
      },
      {
        "start": 107000,
        "text": "ations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enable you to reduce risks, discover opportunities\nto improve performance, and benchmark progresses, which will all save\nyou time and headaches down the line.\nGiven an increasing number of readily available foundation models, for\nmost application developers, the challenge is no longer in developing\nmodel",
        "chapter": 4
      },
      {
        "start": 104500,
        "text": " is\nthe absolute minimum and they would prefer at least 1,000, especially if the\nexamples are being synthesized (McKenzie et al., 2023).\nEvaluate your evaluation pipeline\nEvaluating your evaluation pipeline can help with both improving your\npipeline’s reliability and finding ways to make your evaluation pipeline\nmore efficient. Reliability is especially important with subjective evaluation\nmethods such as AI as a judge.\nHere are some questions you should be asking about the quality of your\nevaluation pipeline:\nIs your evaluation pipeline getting you the right signals?\nDo better responses indee",
        "chapter": 4
      },
      {
        "start": 20500,
        "text": "tuality in Large Language Models”, works by leveraging search\nengine results to verify the response. It works in four steps, as\nvisualized in Figure 4-1:\n1. Use an AI model to decompose the response into individual\nstatements.\n2. Revise each statement to make it self-contained. For example, the\n“it” in the statement “It opened in the 20th century” should be\nchanged to the original subject.\n3. For each statement, propose fact-checking queries to send to a\nGoogle Search API.\n4. Use AI to determine whether the statement is consistent with the\nresearch results.\nFigure 4-1. SAFE breaks an output in",
        "chapter": 4
      }
    ],
    "input_tokens": 1625,
    "output_tokens": 219,
    "input_cost": 0.00024375,
    "output_cost": 0.0001314,
    "total_cost": 0.00037515,
    "reference_answer": "The chapter specifies how to set constraints on response length, such as defining minimum and maximum numbers of words, sentences, and paragraphs."
  },
  {
    "question": "benchmark selection criteria for AI leaderboards",
    "answer": "When selecting benchmarks for AI leaderboards, several criteria come into play:\n\n1. **Coverage of Capabilities**: Benchmarks should cover a wide range of capabilities pertinent to the specific application. For instance, if aiming to evaluate a coding agent, it's essential to include code-related benchmarks, while creative writing benchmarks would be better for a writing assistant.\n\n2. **Reliability and Reputation**: Choose benchmarks that are known for their reliability and have been extensively tested within the community. Using reputable benchmarks helps in having confidence in the evaluation results.\n\n3. **Correlation**: It's important to consider the correlation between benchmarks. Strongly correlated benchmarks can lead to biased results if both are included, as they may not provide distinct insights into model performance.\n\n4. **Cost and Computational Constraints**: Due to the high computational requirements of evaluating models on certain benchmarks, some may be excluded from the leaderboard. For example, expensive benchmarks like MS MARCO might not be included due to resource constraints.\n\n5. **Recency and Evolution**: Regularly update the selection of benchmarks since new benchmarks are constantly being introduced, and older ones may become saturated. This helps ensure that the evaluation reflects current challenges and abilities in the field.\n\n6. **Transparency in Selection Process**: A clear explanation of the benchmark selection process enhances users' understanding and trust in the rankings. Transparency in how benchmarks are chosen can lead to more informed comparisons and interpretations of results.\n\nThese criteria collectively help in creating a balanced and effective leaderboard that accurately ranks models based on diverse and relevant performance metrics.",
    "context": [
      {
        "start": 71500,
        "text": " are generally thoughtful about\nhow they select benchmarks, their decision-making process isn’t always\nclear to users. Different leaderboards often end up with different\nbenchmarks, making it hard to compare and interpret their rankings. For\nexample, in late 2023, Hugging Face updated their Open LLM Leaderboard\nto use the average of six different benchmarks to rank models:\n1. ARC-C (Clark et al., 2018): Measuring the ability to solve complex,\ngrade school-level science questions.\n2. MMLU (Hendrycks et al., 2020): Measuring knowledge and reasoning\ncapabilities in 57 subjects, including elementa",
        "chapter": 4
      },
      {
        "start": 70500,
        "text": "on their aggregated\nperformance on a subset of benchmarks. These leaderboards are immensely\nhelpful but far from being comprehensive. First, due to the compute\nconstraint—evaluating a model on a benchmark requires compute—most\nleaderboards can incorporate only a small number of benchmarks. Some\nleaderboards might exclude an important but expensive benchmark. For\nexample, HELM (Holistic Evaluation of Language Models) Lite left out an\ninformation retrieval benchmark (MS MARCO, Microsoft Machine\nReading Comprehension) because it’s expensive to run. Hugging Face\nopted out of HumanEval due to its l",
        "chapter": 4
      },
      {
        "start": 78000,
        "text": "rboards with public benchmarks\nWhen evaluating models for a specific application, you’re basically creating\na private leaderboard that ranks models based on your evaluation criteria.\nThe first step is to gather a list of benchmarks that evaluate the capabilities\nimportant to your application. If you want to build a coding agent, look at\ncode-related benchmarks. If you build a writing assistant, look into creative\nwriting benchmarks. As new benchmarks are constantly introduced and old\nbenchmarks become saturated, you should look for the latest benchmarks.\nMake sure to evaluate how reliable a be",
        "chapter": 4
      },
      {
        "start": 73500,
        "text": "ecause “they test a\nvariety of reasoning and general knowledge across a wide variety of\n20\nfields.” The HELM website explained that their benchmark list was\n“inspired by the simplicity” of the Hugging Face’s leaderboard but with a\nbroader set of scenarios.\nPublic leaderboards, in general, try to balance coverage and the number of\nbenchmarks. They try to pick a small set of benchmarks that cover a wide\nrange of capabilities, typically including reasoning, factual consistency, and\ndomain-specific capabilities such as math and science.\nAt a high level, this makes sense. However, there’s no clarit",
        "chapter": 4
      },
      {
        "start": 74500,
        "text": "hallenge of selecting benchmarks to rank\nmodels. If leaderboard developers can’t explain their benchmark selection\nprocesses, it might be because it’s really hard to do so.\nAn important aspect of benchmark selection that is often overlooked is\nbenchmark correlation. It is important because if two benchmarks are\nperfectly correlated, you don’t want both of them. Strongly correlated\n21\nbenchmarks can exaggerate biases.\nNOTE\nWhile I was writing this book, many benchmarks became saturated or close to being saturated. In\nJune 2024, less than a year after their leaderboard’s last revamp, Hugging Fac",
        "chapter": 4
      },
      {
        "start": 500,
        "text": "e, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-specific\ncapabilities like math, science, reasoning, and summarization measured?\nThe second part focuses on model selection. Given an increasing number of\nfoundation models to choose from, it can feel overwhelming to choose the\nright model for your application. Thousands of benchmarks have been\nintroduced to evaluate these models along different criteria. Can these\nbenchmarks be trusted? How do you select what benchmarks to use? How\nabout public leaderboards that aggregate multiple benchmarks?\nThe",
        "chapter": 4
      },
      {
        "start": 69500,
        "text": "words.\nBenchmark selection and aggregation\nBenchmark results help you identify promising models for your use cases.\nAggregating benchmark results to rank models gives you a leaderboard.\nThere are two questions to consider:\nWhat benchmarks to include in your leaderboard?\nHow to aggregate these benchmark results to rank models?\nGiven so many benchmarks out there, it’s impossible to look at them all, let\nalone aggregate their results to decide which model is the best. Imagine that\nyou’re considering two models, A and B, for code generation. If model A\nperforms better than model B on a coding benc",
        "chapter": 4
      },
      {
        "start": 109000,
        "text": "reating a private leaderboard\nto rank models based on your needs.\nThis chapter ends with how to use all the evaluation techniques and criteria\ndiscussed in the last chapter and how to create an evaluation pipeline for\nyour application. No perfect evaluation method exists. It’s impossible to\ncapture the ability of a high-dimensional system using one- or few-\ndimensional scores. Evaluating modern AI systems has many limitations\nand biases. However, this doesn’t mean we shouldn’t do it. Combining\ndifferent methods and approaches can help mitigate many of these\nchallenges.\nEven though dedicated di",
        "chapter": 4
      },
      {
        "start": 77500,
        "text": "ins\na better score than another model, averaged across scenarios”.\nWhile public leaderboards are useful to get a sense of models’ broad\nperformance, it’s important to understand what capabilities a leaderboard is\ntrying to capture. A model that ranks high on a public leaderboard will\nlikely, but far from always, perform well for your application. If you want a\nmodel for code generation, a public leaderboard that doesn’t include a code\ngeneration benchmark might not help you as much.\nCustom leaderboards with public benchmarks\nWhen evaluating models for a specific application, you’re basically c",
        "chapter": 4
      },
      {
        "start": 115000,
        "text": "been extensively tested by the community.\n0\nWhen I posted a question on Hugging Face’s Discord about why they chose certain benchmarks,\nLewis Tunstall responded that they were guided by the benchmarks that the then popular models\nused. Thanks to the Hugging Face team for being so wonderfully responsive and for their great\ncontributions to the community.\n1\nI’m really glad to report that while I was writing this book, leaderboards have become much more\ntransparent about their benchmark selection and aggregation process. When launching their new\nleaderboard, Hugging Face shared a great analysis o",
        "chapter": 4
      }
    ],
    "input_tokens": 1590,
    "output_tokens": 309,
    "input_cost": 0.0002385,
    "output_cost": 0.0001854,
    "total_cost": 0.0004239,
    "reference_answer": "The chapter mentions that while leaderboard developers thoughtfully select benchmarks, their decision-making processes are not always transparent to users, leading to variability across different leaderboards."
  },
  {
    "question": "how to create evaluation guidelines for AI systems",
    "answer": "To create evaluation guidelines for AI systems, you should consider the following steps based on the context provided in the \"AI Engineering\" book by Chip Huyen:\n\n1. **Define Evaluation Criteria**: Identify specific criteria necessary for your application, which may include:\n   - **Domain-Specific Capability**: Assess the AI's understanding of the specific domain it operates in.\n   - **Generation Capability**: Evaluate how well the AI can generate outputs relevant to the task.\n   - **Instruction-Following Capability**: Determine how effectively the AI responds to user instructions.\n   - **Cost and Latency**: Measure the operational costs and response times.\n\n2. **Establish Clear Guidelines**: Define not just what the application should achieve but also what it should avoid. For instance, if a bot shouldn't answer questions outside its product scope, specify those limitations and how to detect irrelevant inputs.\n\n3. **Develop a Scoring Rubric**: Create a clear set of guidelines for evaluation that others can follow. This rubric should be validated with human evaluators to ensure it is understandable and effective.\n\n4. **Evaluate All Components**: Given that real-world applications can be complex, assess each component of the AI system against the defined criteria to identify any weaknesses or areas for improvement.\n\n5. **Use Real Data**: When running experiments, utilize actual production data whenever possible. This will ensure that the evaluation process reflects real-world performance and challenges.\n\n6. **Incorporate Feedback from Experts**: In addition to automatic metrics, include human evaluations periodically. Having expert reviewers assess outputs can provide deeper insights into the quality and effectiveness of the AI system.\n\n7. **Iterate on Your Guidelines**: An effective evaluation pipeline is not static. Continually refine your guidelines based on the evaluation outcomes and feedback from users and stakeholders.\n\nBy applying these steps, you can build a reliable evaluation framework that supports the successful development and deployment of AI systems.",
    "context": [
      {
        "start": 4500,
        "text": "to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can think of criteria in the\nfollowing buckets: domain-specific capability, generation capability,\ninstruction-following capability, and cost and latency.\nImagine you ask a model to summarize a legal contract. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstand",
        "chapter": 4
      },
      {
        "start": 87000,
        "text": "opic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each ap",
        "chapter": 4
      },
      {
        "start": 107000,
        "text": "ations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enable you to reduce risks, discover opportunities\nto improve performance, and benchmark progresses, which will all save\nyou time and headaches down the line.\nGiven an increasing number of readily available foundation models, for\nmost application developers, the challenge is no longer in developing\nmodel",
        "chapter": 4
      },
      {
        "start": 46000,
        "text": "vel, the evaluation workflow consists of four steps (see\nFigure 4-5):\n1. Filter out models whose hard attributes don’t work for you. Your list of\nhard attributes depends heavily on your own internal policies, whether\nyou want to use commercial APIs or host your own models.\n2. Use publicly available information, e.g., benchmark performance and\nleaderboard ranking, to narrow down the most promising models to\nexperiment with, balancing different objectives such as model quality,\nlatency, and cost.\n3. Run experiments with your own evaluation pipeline to find the best\nmodel, again, balancing all yo",
        "chapter": 4
      },
      {
        "start": 0,
        "text": "Chapter 4. Evaluate AI Systems\nA model is only useful if it works for its intended purposes. You need to\nevaluate models in the context of your application. Chapter 3 discusses\ndifferent approaches to automatic evaluation. This chapter discusses how to\nuse these approaches to evaluate models for your applications.\nThis chapter contains three parts. It starts with a discussion of the criteria\nyou might use to evaluate your applications and how these criteria are\ndefined and calculated. For example, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-sp",
        "chapter": 4
      },
      {
        "start": 93500,
        "text": " deserve a 1? Validate\nyour rubric with humans: yourself, coworkers, friends, etc. If humans find it\nhard to follow the rubric, you need to refine it to make it unambiguous. This\nprocess can require a lot of back and forth, but it’s necessary. A clear\nguideline is the backbone of a reliable evaluation pipeline. This guideline\ncan also be reused later for training data annotation, as discussed in\nChapter 8.\nTie evaluation metrics to business metrics\nWithin a business, an application must serve a business goal. The\napplication’s metrics must be considered in the context of the business\nproblem i",
        "chapter": 4
      },
      {
        "start": 91000,
        "text": "bad responses look like, you\nwon’t be able to catch them.\nWhen creating the evaluation guideline, it’s important to define not only\nwhat the application should do, but also what it shouldn’t do. For example,\nif you build a customer support chatbot, should this chatbot answer\nquestions unrelated to your product, such as about an upcoming election? If\nnot, you need to define what inputs are out of the scope of your application,\nhow to detect them, and how your application should respond to them.\nDefine evaluation criteria\nOften, the hardest part of evaluation isn’t determining whether an output ",
        "chapter": 4
      },
      {
        "start": 99000,
        "text": "’s components and each\ncriterion, for both turn-based and task-based evaluation. Use actual\nproduction data if possible. If your application has natural labels that you\ncan use, that’s great. If not, you can use either humans or AI to label your\ndata. Chapter 8 discusses AI-generated data. The success of this phase also\ndepends on the clarity of the scoring rubric. The annotation guideline\ncreated for evaluation can be reused to create instruction data for finetuning\nlater, if you choose to finetune.\nSlice your data to gain a finer-grained understanding of your system.\nSlicing means separating",
        "chapter": 4
      },
      {
        "start": 38000,
        "text": "e role, you\nmight be able to create heuristics to evaluate the model’s outputs. For\nexample, if the role is someone who doesn’t talk a lot, a heuristic would be\nthe average of the model’s outputs. Other than that, the easiest automatic\nevaluation approach is AI as a judge. You should evaluate the roleplaying\nAI on both style and knowledge. For example, if a model is supposed to\ntalk like Jackie Chan, its outputs should capture Jackie Chan’s style and are\n8\ngenerated based on Jackie Chan’s knowledge.\nAI judges for different roles will need different prompts. To give you a\nsense of what an AI ju",
        "chapter": 4
      },
      {
        "start": 97500,
        "text": "e\nused to evaluate a model’s perplexity for a generated text, which can be\nused for measurements such as fluency and factual consistency.\nUse automatic metrics as much as possible, but don’t be afraid to fall back\non human evaluation, even in production. Having human experts manually\nevaluate a model’s quality is a long-standing practice in AI. Given the\nchallenges of evaluating open-ended responses, many teams are looking at\nhuman evaluation as the North Star metric to guide their application\ndevelopment. Each day, you can use human experts to evaluate a subset of\nyour application’s outputs t",
        "chapter": 4
      }
    ],
    "input_tokens": 1606,
    "output_tokens": 389,
    "input_cost": 0.0002409,
    "output_cost": 0.0002334,
    "total_cost": 0.0004743,
    "reference_answer": "The text emphasizes that establishing clear evaluation guidelines is crucial to avoid ambiguous scoring and ensures that misleading responses can be identified effectively."
  },
  {
    "question": "evaluating efficiency in AI model performance",
    "answer": "To evaluate the efficiency of AI model performance effectively, you should consider several key criteria that align with the goals of your application:\n\n1. **Execution Accuracy**: It's crucial to ensure that the model's outputs are not only correct but also efficient in terms of runtime. For example, BIRD-SQL benchmarks measure the efficiency of SQL queries by comparing their execution time against the ground truth queries. This highlights the importance of balancing accuracy with operational performance.\n\n2. **Cost and Latency**: A model that produces high-quality outputs but is too slow or expensive can negate its usefulness. Evaluating the trade-offs between model quality, latency, and cost is essential for ensuring that the model can be integrated into practical applications.\n\n3. **Evaluation Pipeline**: Establishing a reliable evaluation pipeline is fundamental. It should include assessing all components of the AI system to accurately differentiate between good and bad outcomes. This pipeline should incorporate both automatic metrics (like perplexity for fluency) and human evaluations to provide a comprehensive view of performance.\n\n4. **Domain-Specific and Generation Capabilities**: Tailor evaluation criteria to the specific domain. Understanding the unique demands of your application (like legal contracts or creative writing) will shape how you measure efficiency and effectiveness.\n\n5. **Benchmarking**: Create a private leaderboard to rank models based on your application’s specific criteria, leveraging current and relevant benchmarks. Stay updated with the latest benchmarks to ensure comprehensive evaluation as models evolve.\n\nBy considering these aspects, you can develop a robust strategy for evaluating AI model efficiency and aligning it with user needs and operational constraints.",
    "context": [
      {
        "start": 17500,
        "text": " light on how AI models process conflicting\ninformation and determine what the facts are. For example, Wan et al.\n(2024) found that existing “models rely heavily on the relevance of a\nwebsite to the query, while largely ignoring stylistic features that humans\nfind important such as whether a text contains scientific references or is\nwritten with a neutral tone.”\nTIP\nWhen designing metrics to measure hallucinations, it’s important to analyze the model’s outputs to\nunderstand the types of queries that it is more likely to hallucinate on. Your benchmark should focus\nmore on these queries.\nFor exa",
        "chapter": 4
      },
      {
        "start": 4500,
        "text": "to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can think of criteria in the\nfollowing buckets: domain-specific capability, generation capability,\ninstruction-following capability, and cost and latency.\nImagine you ask a model to summarize a legal contract. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstand",
        "chapter": 4
      },
      {
        "start": 87000,
        "text": "opic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each ap",
        "chapter": 4
      },
      {
        "start": 7000,
        "text": "want a car that runs but consumes an excessive amount of fuel? Similarly, if\nan SQL query generated by your text-to-SQL model is correct but takes too\nlong or requires too much memory to run, it might not be usable.\nEfficiency can be exactly evaluated by measuring runtime or memory\nusage. BIRD-SQL (Li et al., 2023) is an example of a benchmark that takes\ninto account not only the generated query’s execution accuracy but also its\nefficiency, which is measured by comparing the runtime of the generated\nquery with the runtime of the ground truth SQL query.\nYou might also care about code readabilit",
        "chapter": 4
      },
      {
        "start": 107000,
        "text": "ations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enable you to reduce risks, discover opportunities\nto improve performance, and benchmark progresses, which will all save\nyou time and headaches down the line.\nGiven an increasing number of readily available foundation models, for\nmost application developers, the challenge is no longer in developing\nmodel",
        "chapter": 4
      },
      {
        "start": 97500,
        "text": "e\nused to evaluate a model’s perplexity for a generated text, which can be\nused for measurements such as fluency and factual consistency.\nUse automatic metrics as much as possible, but don’t be afraid to fall back\non human evaluation, even in production. Having human experts manually\nevaluate a model’s quality is a long-standing practice in AI. Given the\nchallenges of evaluating open-ended responses, many teams are looking at\nhuman evaluation as the North Star metric to guide their application\ndevelopment. Each day, you can use human experts to evaluate a subset of\nyour application’s outputs t",
        "chapter": 4
      },
      {
        "start": 39500,
        "text": "to the role; the richer,\nthe better. (If the question contains\nreference answers, then the role−specific\nknowledge and memories are based on the\nreference answer. )\nCost and Latency\nA model that generates high-quality outputs but is too slow and expensive\nto run will not be useful. When evaluating models, it’s important to balance\nmodel quality, latency, and cost. Many companies opt for lower-quality\nmodels if they provide better cost and latency. Cost and latency\noptimization are discussed in detail in Chapter 9, so this section will be\nquick.\nOptimizing for multiple objectives is an active f",
        "chapter": 4
      },
      {
        "start": 4000,
        "text": "timent analysis, next-\naction prediction, etc. It’s much easier to evaluate classification tasks\nthan open-ended tasks.\nWhile the evaluation-driven development approach makes sense from a\nbusiness perspective, focusing only on applications whose outcomes can be\nmeasured is similar to looking for the lost key under the lamppost (at night).\nIt’s easier to do, but it doesn’t mean we’ll find the key. We might be missing\nout on many potentially game-changing applications because there is no\neasy way to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto ",
        "chapter": 4
      },
      {
        "start": 78000,
        "text": "rboards with public benchmarks\nWhen evaluating models for a specific application, you’re basically creating\na private leaderboard that ranks models based on your evaluation criteria.\nThe first step is to gather a list of benchmarks that evaluate the capabilities\nimportant to your application. If you want to build a coding agent, look at\ncode-related benchmarks. If you build a writing assistant, look into creative\nwriting benchmarks. As new benchmarks are constantly introduced and old\nbenchmarks become saturated, you should look for the latest benchmarks.\nMake sure to evaluate how reliable a be",
        "chapter": 4
      },
      {
        "start": 68500,
        "text": "ommercial API or to host a model yourself. This decision should\nsignificantly narrow your options. Next, you can further refine your\nselection using publicly available model performance data.\nNavigate Public Benchmarks\nThere are thousands of benchmarks designed to evaluate a model’s different\ncapabilities. Google’s BIG-bench (2022) alone has 214 benchmarks. The\nnumber of benchmarks rapidly grows to match the rapidly growing number\nof AI use cases. In addition, as AI models improve, old benchmarks\nsaturate, necessitating the introduction of new benchmarks.\nA tool that helps you evaluate a model",
        "chapter": 4
      }
    ],
    "input_tokens": 1582,
    "output_tokens": 318,
    "input_cost": 0.0002373,
    "output_cost": 0.0001908,
    "total_cost": 0.0004281,
    "reference_answer": "Efficiency in AI models can be evaluated through metrics like runtime and memory usage, in addition to functional correctness, to ensure a model is not just accurate but also practical for real-world applications."
  },
  {
    "question": "what are open weights in AI models?",
    "answer": "In the context of AI models, \"open weights\" refer to models whose parameters (or weights) are publicly accessible, allowing users to download and utilize the model in their applications. However, these models may not necessarily come with open data, meaning the data used for training the model might not be publicly available. This distinction is important because while the weights can be shared, the lack of access to training data can limit understanding, auditing, and further modifications of the model.\n\nThe term \"open model\" is used for models that are both open weight and come with open data, allowing for greater flexibility in usage, such as the ability to retrain the model. Since many open source models are open weight only, developers sometimes hide training data information to avoid scrutiny or legal issues. The open weights provide transparency regarding the model's functionality, while open data enhances the versatility and comprehensibility of the model.",
    "context": [
      {
        "start": 49000,
        "text": "auditing\npurposes, for example, to make sure that the model wasn’t trained on\n10\ncompromised or illegally acquired data.\nTo signal whether the data is also open, the term “open weight” is used for\nmodels that don’t come with open data, whereas the term “open model” is\nused for models that come with open data.\nNOTE\nSome people argue that the term open source should be reserved only for fully open models. In this\nbook, for simplicity, I use open source to refer to all models whose weights are made public,\nregardless of their training data’s availability and licenses.\nAs of this writing, the vast",
        "chapter": 4
      },
      {
        "start": 48500,
        "text": "ever, some people argue that since a model’s performance is largely a\nfunction of what data it was trained on, a model should be considered open\nonly if its training data is also made publicly available.\nOpen data allows more flexible model usage, such as retraining the model\nfrom scratch with modifications in the model architecture, training process,\nor the training data itself. Open data also makes it easier to understand the\nmodel. Some use cases also required access to the training data for auditing\npurposes, for example, to make sure that the model wasn’t trained on\n10\ncompromised or ille",
        "chapter": 4
      },
      {
        "start": 49500,
        "text": " public,\nregardless of their training data’s availability and licenses.\nAs of this writing, the vast majority of open source models are open weight\nonly. Model developers might hide training data information on purpose, as\nthis information can open model developers to public scrutiny and potential\nlawsuits.\nAnother important attribute of open source models is their licenses. Before\nfoundation models, the open source world was confusing enough, with so\nmany different licenses, such as MIT (Massachusetts Institute of\nTechnology), Apache 2.0, GNU General Public License (GPL), BSD\n(Berkely Softwar",
        "chapter": 4
      },
      {
        "start": 51500,
        "text": " a teacher (typically a much larger model). Mistral\ndidn’t allow this originally but later changed its license. As of this\n12\nwriting, the Llama licenses still don’t allow it.\nSome people use the term restricted weight to refer to open source models\nwith restricted licenses. However, I find this term ambiguous, since all\nsensible licenses have restrictions (e.g., you shouldn’t be able to use the\nmodel to commit genocide).\nOpen source models versus model APIs\nFor a model to be accessible to users, a machine needs to host and run it.\nThe service that hosts the model and receives user queries, ru",
        "chapter": 4
      },
      {
        "start": 48000,
        "text": "n open source model yourself. The answer to this\nquestion can significantly reduce your candidate model pool.\nLet’s first go into what exactly open source means when it comes to\nmodels, then discuss the pros and cons of these two approaches.\nOpen source, open weight, and model licenses\nThe term “open source model” has become contentious. Originally, open\nsource was used to refer to any model that people can download and use.\nFor many use cases, being able to download the model is sufficient.\nHowever, some people argue that since a model’s performance is largely a\nfunction of what data it was t",
        "chapter": 4
      },
      {
        "start": 113500,
        "text": "nts for auditable information of models and\ntraining data may increase. Commercial models may be able to provide certifications, saving\ncompanies from the effort.\n7\nUsers want models to be open source because open means more information and more options, but\nwhat’s in it for model developers? Many companies have sprung up to capitalize on open source\nmodels by providing inference and finetuning services. It’s not a bad thing. Many people need these\nservices to leverage open source models. But, from model developers’ perspective, why invest\nmillions, if not billions, into building models just f",
        "chapter": 4
      },
      {
        "start": 69000,
        "text": "s\nsaturate, necessitating the introduction of new benchmarks.\nA tool that helps you evaluate a model on multiple benchmarks is an\nevaluation harness. As of this writing, EleutherAI’s lm-evaluation-harness\nsupports over 400 benchmarks. OpenAI’s evals lets you run any of the\napproximately 500 existing benchmarks and register new benchmarks to\nevaluate OpenAI models. Their benchmarks evaluate a wide range of\ncapabilities, from doing math and solving puzzles to identifying ASCII art\nthat represents words.\nBenchmark selection and aggregation\nBenchmark results help you identify promising models for ",
        "chapter": 4
      },
      {
        "start": 57000,
        "text": "mance but said nothing about the models’ training data other\nthan that “all data enrichment workers are paid at least a local living wage”.\nOpenAI’s CTO wasn’t able to provide a satisfactory answer when asked\nwhat data was used to train their models.\nOn top of that, the IP laws around AI are actively evolving. While the US\nPatent and Trademark Office (USPTO) made clear in 2024 that “AI-assisted\ninventions are not categorically unpatentable”, an AI application’s\npatentability depends on “whether the human contribution to an innovation\nis significant enough to qualify for a patent.” It’s also un",
        "chapter": 4
      },
      {
        "start": 79500,
        "text": " I doubt that\n24\nOpenAI would fly completely blind. If the second reason is true, it\nreinforces the idea that the best model overall might not be the best model\nfor your application.\nNot all models have publicly available scores on all benchmarks. If the\nmodel you care about doesn’t have a publicly available score on your\n25\nbenchmark, you will need to run the evaluation yourself. Hopefully, an\nevaluation harness can help you with that. Running benchmarks can be\nexpensive. For example, Stanford spent approximately $80,000–$100,000\n26\nto evaluate 30 models on their full HELM suite. The more mod",
        "chapter": 4
      },
      {
        "start": 60000,
        "text": "other reason that might cause open source models to lag behind is that\nopen source developers don’t receive feedback from users to improve their\nmodels, the way commercial models do. Once a model is open sourced,\nmodel developers have no idea how the model is being used, and how well\nthe model works in the wild.\nFunctionality\nMany functionalities are needed around a model to make it work for a use\ncase. Here are some examples of these functionalities:\nScalability: making sure the inference service can support your\napplication’s traffic while maintaining the desirable latency and cost.\nFunction",
        "chapter": 4
      }
    ],
    "input_tokens": 1668,
    "output_tokens": 181,
    "input_cost": 0.0002502,
    "output_cost": 0.0001086,
    "total_cost": 0.0003588,
    "reference_answer": "Open weights refer to model weights that are publicly available, without necessarily providing access to the training data, which can be vital for auditing and ethical considerations."
  },
  {
    "question": "how to evaluate generated text quality",
    "answer": "To evaluate the quality of generated text, several metrics and methods can be utilized, as outlined in the relevant chapters of the \"AI Engineering\" book. Here are the key points:\n\n1. **Fluency**: This measures whether the generated text is grammatically correct and sounds natural, assessing if it resembles writing by a fluent speaker.\n\n2. **Coherence**: This evaluates how logically structured the text is, determining whether it follows a sensible progression.\n\n3. **Task-Specific Metrics**: Different tasks may require unique metrics. For example:\n   - **Faithfulness**: In translation tasks, this measures how true the translation remains to the original text.\n   - **Relevance**: In summarization tasks, this checks if the summary addresses the most important aspects of the source document.\n\n4. **Perplexity**: A metric that can be used to evaluate fluency and factual consistency, reflecting the model's confidence about the generated text.\n\n5. **Human Evaluation**: Despite the availability of automatic metrics, human evaluation remains crucial, especially for complex open-ended responses. Utilizing human experts to assess a subset of outputs regularly is recommended.\n\n6. **Automated Evaluation Tools**: Frameworks like INFOBench and IFEval help assess the model's instruction-following capabilities and other factors such as adherence to content constraints and style rules, which can often be verified automatically.\n\n7. **Dynamic Metrics**: As language models advance, traditional metrics may lose relevance, necessitating the development of new metrics tailored to the evolving capabilities of generative models.\n\nIn summary, evaluating generated text quality involves a combination of automatic metrics for fluency, coherence, and task-specific factors, supplemented by human assessments to ensure comprehensive evaluation.",
    "context": [
      {
        "start": 11500,
        "text": "the brightest minds in NLP (natural language\nprocessing) have been working on how to evaluate the quality of open-\nended outputs. The subfield that studies open-ended text generation is\ncalled NLG (natural language generation). NLG tasks in the early 2010s\nincluded translation, summarization, and paraphrasing.\nMetrics used to evaluate the quality of generated texts back then included\nfluency and coherence. Fluency measures whether the text is grammatically\ncorrect and natural-sounding (does this sound like something written by a\nfluent speaker?). Coherence measures how well-structured the whol",
        "chapter": 4
      },
      {
        "start": 97500,
        "text": "e\nused to evaluate a model’s perplexity for a generated text, which can be\nused for measurements such as fluency and factual consistency.\nUse automatic metrics as much as possible, but don’t be afraid to fall back\non human evaluation, even in production. Having human experts manually\nevaluate a model’s quality is a long-standing practice in AI. Given the\nchallenges of evaluating open-ended responses, many teams are looking at\nhuman evaluation as the North Star metric to guide their application\ndevelopment. Each day, you can use human experts to evaluate a subset of\nyour application’s outputs t",
        "chapter": 4
      },
      {
        "start": 13000,
        "text": "eneration capabilities have improved, AI-generated texts have\nbecome nearly indistinguishable from human-generated texts. Fluency and\n2\ncoherence become less important. However, these metrics can still be\nuseful for weaker models or for applications involving creative writing and\nlow-resource languages. Fluency and coherence can be evaluated using AI\nas a judge—asking an AI model how fluent and coherent a text is—or using\nperplexity, as discussed in Chapter 3.\nGenerative models, with their new capabilities and new use cases, have new\nissues that require new metrics to track. The most pressing ",
        "chapter": 4
      },
      {
        "start": 12000,
        "text": " sound like something written by a\nfluent speaker?). Coherence measures how well-structured the whole text is\n(does it follow a logical structure?). Each task might also have its own\nmetrics. For example, a metric a translation task might use is faithfulness:\nhow faithful is the generated translation to the original sentence? A metric\nthat a summarization task might use is relevance: does the summary focus\non the most important aspects of the source document? (Li et al., 2022).\nSome early NLG metrics, including faithfulness and relevance, have been\nrepurposed, with significant modifications, t",
        "chapter": 4
      },
      {
        "start": 12500,
        "text": "trics, including faithfulness and relevance, have been\nrepurposed, with significant modifications, to evaluate the outputs of\nfoundation models. As generative models improved, many issues of early\nNLG systems went away, and the metrics used to track these issues became\nless important. In the 2010s, generated texts didn’t sound natural. They\nwere typically full of grammatical errors and awkward sentences. Fluency\nand coherence, then, were important metrics to track. However, as language\nmodels’ generation capabilities have improved, AI-generated texts have\nbecome nearly indistinguishable from h",
        "chapter": 4
      },
      {
        "start": 34000,
        "text": " to follow content constraints (such as “discuss only climate\nchange”), linguistic guidelines (such as “use Victorian English”), and style\nrules (such as “use a respectful tone”). However, the verification of these\nexpanded instruction types can’t be easily automated. If you instruct a\nmodel to “use language appropriate to a young audience”, how do you\nautomatically verify if the output is indeed appropriate for a young\naudience?\nFor verification, INFOBench authors constructed a list of criteria for each\ninstruction, each framed as a yes/no question. For example, the output to the\ninstruction ",
        "chapter": 4
      },
      {
        "start": 30500,
        "text": "and\nINFOBench, measure models’ capability to follow a wide range of\ninstructions, which are to give you ideas on how to evaluate a model’s\nability to follow your instructions: what criteria to use, what instructions to\ninclude in the evaluation set, and what evaluation methods are appropriate.\nThe Google benchmark IFEval, Instruction-Following Evaluation, focuses\non whether the model can produce outputs following an expected format.\nZhou et al. (2023) identified 25 types of instructions that can be\nautomatically verified, such as keyword inclusion, length constraints,\nnumber of bullet points, ",
        "chapter": 4
      },
      {
        "start": 7000,
        "text": "want a car that runs but consumes an excessive amount of fuel? Similarly, if\nan SQL query generated by your text-to-SQL model is correct but takes too\nlong or requires too much memory to run, it might not be usable.\nEfficiency can be exactly evaluated by measuring runtime or memory\nusage. BIRD-SQL (Li et al., 2023) is an example of a benchmark that takes\ninto account not only the generated query’s execution accuracy but also its\nefficiency, which is measured by comparing the runtime of the generated\nquery with the runtime of the ground truth SQL query.\nYou might also care about code readabilit",
        "chapter": 4
      },
      {
        "start": 31000,
        "text": " be\nautomatically verified, such as keyword inclusion, length constraints,\nnumber of bullet points, and JSON format. If you ask a model to write a\nsentence that uses the word “ephemeral”, you can write a program to check\nif the output contains this word; hence, this instruction is automatically\nverifiable. The score is the fraction of the instructions that are followed\ncorrectly out of all instructions. Explanations of these instruction types are\nshown in Table 4-2.\nTable 4-2. Automatically verifiable instructions proposed by Zhou et al. to evaluate models’\ninstruction-following capability. Ta",
        "chapter": 4
      },
      {
        "start": 91000,
        "text": "bad responses look like, you\nwon’t be able to catch them.\nWhen creating the evaluation guideline, it’s important to define not only\nwhat the application should do, but also what it shouldn’t do. For example,\nif you build a customer support chatbot, should this chatbot answer\nquestions unrelated to your product, such as about an upcoming election? If\nnot, you need to define what inputs are out of the scope of your application,\nhow to detect them, and how your application should respond to them.\nDefine evaluation criteria\nOften, the hardest part of evaluation isn’t determining whether an output ",
        "chapter": 4
      }
    ],
    "input_tokens": 1621,
    "output_tokens": 348,
    "input_cost": 0.00024315,
    "output_cost": 0.0002088,
    "total_cost": 0.00045195,
    "reference_answer": "The chapter outlines key evaluation metrics for generated text quality including fluency, coherence, and task-specific measures like faithfulness and relevance."
  },
  {
    "question": "how to reduce latency when using AI models",
    "answer": "To reduce latency when using AI models, the following techniques can be employed:\n\n1. **Careful Prompting**: Design prompts that instruct the model to be concise and direct. This reduces the number of tokens generated, thus minimizing latency.\n\n2. **Stopping Conditions**: Set specific stopping conditions for model responses. This can prevent unnecessary token generation and cut down on wait time.\n\n3. **Latency Metric Understanding**: Familiarize yourself with different latency metrics (e.g., time to first token, time per token). Understanding which metrics matter for your application can guide optimizations.\n\n4. **Optimize Model Choices**: Start by filtering models that do not meet your latency requirements. Then choose the best-performing model among those that fit your needs.\n\n5. **Scalability**: Ensure the inference service can handle the application’s traffic while maintaining desired latency levels.\n\n6. **Cost-Performance Balance**: Evaluate and balance the quality of the model with its cost and latency. Sometimes opting for a lower-quality model can be beneficial if it significantly reduces latency.\n\nBy implementing these strategies, it's possible to enhance the responsiveness of AI models while maintaining necessary performance standards.",
    "context": [
      {
        "start": 41000,
        "text": "bserved by users by careful\nprompting, such as instructing the model to be concise, setting a stopping\ncondition for generation (discussed in Chapter 2), or other optimization\ntechniques (discussed in Chapter 9).\nTIP\nWhen evaluating models based on latency, it’s important to differentiate between the must-have and\nthe nice-to-have. If you ask users if they want lower latency, nobody will ever say no. But high\nlatency is often an annoyance, not a deal breaker.\nIf you use model APIs, they typically charge by tokens. The more input and\noutput tokens you use, the more expensive it is. Many applica",
        "chapter": 4
      },
      {
        "start": 40500,
        "text": "re multiple metrics for latency for foundation models, including but\nnot limited to time to first token, time per token, time between tokens, time\nper query, etc. It’s important to understand what latency metrics matter to\nyou.\nLatency depends not only on the underlying model but also on each prompt\nand sampling variables. Autoregressive language models typically generate\noutputs token by token. The more tokens it has to generate, the higher the\ntotal latency. You can control the total latency observed by users by careful\nprompting, such as instructing the model to be concise, setting a stoppi",
        "chapter": 4
      },
      {
        "start": 39500,
        "text": "to the role; the richer,\nthe better. (If the question contains\nreference answers, then the role−specific\nknowledge and memories are based on the\nreference answer. )\nCost and Latency\nA model that generates high-quality outputs but is too slow and expensive\nto run will not be useful. When evaluating models, it’s important to balance\nmodel quality, latency, and cost. Many companies opt for lower-quality\nmodels if they provide better cost and latency. Cost and latency\noptimization are discussed in detail in Chapter 9, so this section will be\nquick.\nOptimizing for multiple objectives is an active f",
        "chapter": 4
      },
      {
        "start": 40000,
        "text": "etail in Chapter 9, so this section will be\nquick.\nOptimizing for multiple objectives is an active field of study called Pareto\noptimization. When optimizing for multiple objectives, it’s important to be\nclear about what objectives you can and can’t compromise on. For example,\nif latency is something you can’t compromise on, you start with latency\nexpectations for different models, filter out all the models that don’t meet\nyour latency requirements, and then pick the best among the rest.\nThere are multiple metrics for latency for foundation models, including but\nnot limited to time to first to",
        "chapter": 4
      },
      {
        "start": 60000,
        "text": "other reason that might cause open source models to lag behind is that\nopen source developers don’t receive feedback from users to improve their\nmodels, the way commercial models do. Once a model is open sourced,\nmodel developers have no idea how the model is being used, and how well\nthe model works in the wild.\nFunctionality\nMany functionalities are needed around a model to make it work for a use\ncase. Here are some examples of these functionalities:\nScalability: making sure the inference service can support your\napplication’s traffic while maintaining the desirable latency and cost.\nFunction",
        "chapter": 4
      },
      {
        "start": 44000,
        "text": " over and\nover again. For example, prompt engineering might start with the strongest\nmodel overall to evaluate feasibility and then work backward to see if\nsmaller models would work. If you decide to do finetuning, you might start\nwith a small model to test your code and move toward the biggest model\nthat fits your hardware constraints (e.g., one GPU).\nIn general, the selection process for each technique typically involves two\nsteps:\n1. Figuring out the best achievable performance\n2. Mapping models along the cost–performance axes and choosing the\nmodel that gives the best performance for your ",
        "chapter": 4
      },
      {
        "start": 17500,
        "text": " light on how AI models process conflicting\ninformation and determine what the facts are. For example, Wan et al.\n(2024) found that existing “models rely heavily on the relevance of a\nwebsite to the query, while largely ignoring stylistic features that humans\nfind important such as whether a text contains scientific references or is\nwritten with a neutral tone.”\nTIP\nWhen designing metrics to measure hallucinations, it’s important to analyze the model’s outputs to\nunderstand the types of queries that it is more likely to hallucinate on. Your benchmark should focus\nmore on these queries.\nFor exa",
        "chapter": 4
      },
      {
        "start": 60500,
        "text": "ce can support your\napplication’s traffic while maintaining the desirable latency and cost.\nFunction calling: giving the model the ability to use external tools, which\nis essential for RAG and agentic use cases, as discussed in Chapter 6.\nStructured outputs, such as asking models to generate outputs in JSON\nformat.\nOutput guardrails: mitigating risks in the generated responses, such as\nmaking sure the responses aren’t racist or sexist.\nMany of these functionalities are challenging and time-consuming to\nimplement, which makes many companies turn to API providers that\nprovide the functionalities",
        "chapter": 4
      },
      {
        "start": 108000,
        "text": "ng fluency, coherence, and\nfaithfulness.\nTo help answer the question of whether to host a model or to use a model\nAPI, this chapter outlined the pros and cons of each approach along seven\naxes, including data privacy, data lineage, performance, functionality,\ncontrol, and cost. This decision, like all the build versus buy decisions, is\nunique to every team, depending not only on what the team needs but also\non what the team wants.\nThis chapter also explored the thousands of available public benchmarks.\nPublic benchmarks can help you weed out bad models, but they won’t help\nyou find the best mo",
        "chapter": 4
      },
      {
        "start": 43000,
        "text": "tput token 1M tokens 1M to\nScale TPM (tokens X > 1M TPM > 1M\nper minute)\nLatency Time to first Internal user < 200ms < 100\ntoken (P90) prompt dataset\nLatency Time per total Internal user < 1m < 30s\nquery (P90) prompt dataset\nOverall model Elo score Chatbot > 1200 > 125\nquality Arena’s\nranking\nCode pass@1 HumanEval > 90% > 95%\ngeneration\ncapability\nHard\nCriteria Metric Benchmark Ideal\nrequirement\nFactual Internal GPT Internal > 0.8 > 0.9\nconsistency metric hallucination\ndataset\nNow that you have your criteria, let’s move on to the next step and use them\nto select the best model for your applica",
        "chapter": 4
      }
    ],
    "input_tokens": 1661,
    "output_tokens": 233,
    "input_cost": 0.00024915,
    "output_cost": 0.0001398,
    "total_cost": 0.00038895,
    "reference_answer": "You can manage latency by optimizing prompts for conciseness, setting stopping conditions for generation, and employing other optimization techniques discussed in previous chapters."
  },
  {
    "question": "why do monopolies harm consumers?",
    "answer": "Monopolies harm consumers primarily by reducing consumer surplus and leading to higher prices. When a single firm dominates the market, it can set prices above competitive levels, which means consumers pay more for goods or services. This results in decreased levels of output, as monopolies often produce less than a competitive market would, leading to inefficiencies and a loss of allocative efficiency in society. This imbalance ultimately benefits the monopoly at the expense of consumers.",
    "context": [
      {
        "start": 8500,
        "text": " are multiple-choice,\nincluding UC Berkeley’s MMLU (2020), Microsoft’s AGIEval (2023), and\nthe AI2 Reasoning Challenge (ARC-C) (2018). In their paper, AGIEval’s\nauthors explained that they excluded open-ended tasks on purpose to avoid\ninconsistent assessment.\nHere’s an example of a multiple-choice question in the MMLU benchmark:\nQuestion: One of the reasons that the government discourages and\nregulates monopolies is that\n(A) Producer surplus is lost and consumer surplus is gained.\n(B) Monopoly prices ensure productive efficiency but cost society\nallocative efficiency.\n(C) Monopoly firms do not",
        "chapter": 4
      },
      {
        "start": 114500,
        "text": "gh as an incentive. People who\nwant what’s good for society will continue to push for open source, and maybe there will be enough\ncollective goodwill to help open source prevail. I certainly hope so.\n8\nThe companies that get hit the most by API costs are probably not the biggest companies. The\nbiggest companies might be important enough to service providers to negotiate favorable terms.\n9\nThis is similar to the philosophy in software infrastructure to always use the most popular tools that\nhave been extensively tested by the community.\n0\nWhen I posted a question on Hugging Face’s Discord about",
        "chapter": 4
      },
      {
        "start": 64000,
        "text": "enterprises care about open source models. Image from the 2024 study by a16z.\nIf your business depends on a model, it’s understandable that you would\nwant some control over it, and API providers might not always give you the\nlevel of control you want. When using a service provided by someone else,\nyou’re subject to their terms and conditions, and their rate limits. You can\naccess only what’s made available to you by this provider, and thus might\nnot be able to tweak the model as needed.\nTo protect their users and themselves from potential lawsuits, model\nproviders use safety guardrails such as",
        "chapter": 4
      },
      {
        "start": 58500,
        "text": "mited legal resources compared\nto commercial models. If you use an open source model that infringes on\ncopyrights, the infringed party is unlikely to go after the model developers,\nand more likely to go after you. However, if you use a commercial model,\nthe contracts you sign with the model providers can potentially protect you\n16\nfrom data lineage risks.\nPerformance\nVarious benchmarks have shown that the gap between open source models\nand proprietary models is closing. Figure 4-7 shows this gap decreasing on\nthe MMLU benchmark over time. This trend has made many people believe\nthat one day, t",
        "chapter": 4
      },
      {
        "start": 9000,
        "text": "rices ensure productive efficiency but cost society\nallocative efficiency.\n(C) Monopoly firms do not engage in significant research and\ndevelopment.\n(D) Consumer surplus is lost with higher prices and lower levels of\noutput.\nLabel: (D)\nA multiple-choice question (MCQ) might have one or more correct\nanswers. A common metric is accuracy—how many questions the model\ngets right. Some tasks use a point system to grade a model’s performance—\nharder questions are worth more points. You can also use a point system\nwhen there are multiple correct options. A model gets one point for each\noption it gets ",
        "chapter": 4
      },
      {
        "start": 112000,
        "text": "hich was generated by the public, the public should have the right to\naccess the models’ training data.\n1\nIn spirit, this restriction is similar to the Elastic License that forbids companies from offering the\nopen source version of Elastic as a hosted service and competing with the Elasticsearch platform.\n2\nIt’s possible that a model’s output can’t be used to improve other models, even if its license allows\nthat. Consider model X that is trained on ChatGPT’s outputs. X might have a license that allows this,\nbut if ChatGPT doesn’t, then X violated ChatGPT’s terms of use, and therefore, X can’t ",
        "chapter": 4
      },
      {
        "start": 81000,
        "text": "ndle data contamination will be\nthe topic of the next section.\nData contamination with public benchmarks\nData contamination is so common that there are many different names for\nit, including data leakage, training on the test set, or simply cheating. Data\ncontamination happens when a model was trained on the same data it’s\nevaluated on. If so, it’s possible that the model just memorizes the answers\nit saw during training, causing it to achieve higher evaluation scores than it\nshould. A model that is trained on the MMLU benchmark can achieve high\nMMLU scores without being useful.\nRylan Schaeffe",
        "chapter": 4
      },
      {
        "start": 114000,
        "text": "rom model developers’ perspective, why invest\nmillions, if not billions, into building models just for others to make money?It might be argued that\nMeta supports open source models only to keep their competitors (Google, Microsoft/OpenAI) in\ncheck. Both Mistral and Cohere have open source models, but they also have APIs. At some point,\ninference services on top of Mistral and Cohere models become their competitors.There’s the\nargument that open source is better for society, and maybe that’s enough as an incentive. People who\nwant what’s good for society will continue to push for open source, a",
        "chapter": 4
      },
      {
        "start": 59000,
        "text": " decreasing on\nthe MMLU benchmark over time. This trend has made many people believe\nthat one day, there will be an open source model that performs just as well,\nif not better, than the strongest proprietary model.\nAs much as I want open source models to catch up with proprietary models,\nI don’t think the incentives are set up for it. If you have the strongest model\navailable, would you rather open source it for other people to capitalize on\n17\nit, or would you try to capitalize on it yourself? It’s a common practice for\ncompanies to keep their strongest models behind APIs and open source thei",
        "chapter": 4
      },
      {
        "start": 25500,
        "text": " sexist, homophobic speech, and other\ndiscriminatory behaviors.\n4. Violence, including threats and graphic detail.\n5. Stereotypes, such as always using female names for nurses or male\nnames for CEOs.\n6. Biases toward a political or religious ideology, which can lead to the\nmodel generating only content that supports this ideology. For example,\nstudies (Feng et al., 2023; Motoki et al., 2023; and Hartman et al., 2023)\nhave shown that models, depending on their training, can be imbued\nwith political biases. For example, OpenAI’s GPT-4 is more left-winged\nand libertarian-leaning, whereas Meta’s L",
        "chapter": 4
      }
    ],
    "input_tokens": 1703,
    "output_tokens": 89,
    "input_cost": 0.00025545,
    "output_cost": 5.34e-05,
    "total_cost": 0.00030885,
    "reference_answer": "Monopolies lead to increased prices and reduced output, which results in a loss of consumer surplus as noted in option D of the question."
  },
  {
    "question": "How does data lineage affect AI model development?",
    "answer": "Data lineage significantly impacts AI model development in a few key ways:\n\n1. **Transparency of Training Data**: Understanding the origin and history of the data used to train models helps in assessing the model's reliability and compliance with copyright regulations. Many companies are hesitant to use AI due to unclear data lineage, fearing potential infringements, which affects their willingness to innovate with AI.\n\n2. **Open vs. Proprietary Models**: Concerns over data lineage have led some organizations to adopt fully open models, making their training data publicly available. This transparency allows for community scrutiny to ensure safety in usage. However, inspecting large datasets remains a practical challenge, making some companies prefer proprietary models instead.\n\n3. **Intellectual Property (IP) Concerns**: Data lineage raises questions regarding the patentability of innovations derived from AI models trained on copyrighted data. This uncertainty can deter industries heavily reliant on IP, like gaming and film, from utilizing AI technologies until clearer regulations emerge.\n\n4. **Model Evaluation and Contamination**: Proper lineage tracking is vital to avoid data contamination, which can distort evaluation metrics. For instance, if both training and evaluation datasets were sourced from the same origin without proper awareness, the model's performance could be inaccurately assessed, leading to misguided confidence in its capabilities.\n\nIn summary, data lineage affects transparency, compliance with legal standards, model evaluation integrity, and the overall adoption of AI technologies in various sectors.",
    "context": [
      {
        "start": 56500,
        "text": "rizes 8% of its training set.\nThese memorized samples can be accidentally leaked to users or\nintentionally exploited by bad actors, as demonstrated in Chapter 5.\nData lineage and copyright\nData lineage and copyright concerns can steer a company in many\ndirections: toward open source models, toward proprietary models, or away\nfrom both.\nFor most models, there’s little transparency about what data a model is\ntrained on. In Gemini’s technical report, Google went into detail about the\nmodels’ performance but said nothing about the models’ training data other\nthan that “all data enrichment workers ",
        "chapter": 4
      },
      {
        "start": 67000,
        "text": "Have to send your Don’t have to send your\ndata to model data externally\nproviders, which Fewer checks and\nmeans your team can balances for data\naccidentally leak lineage/training data\nconfidential info copyright\nPerformance\nBest-performing The best open source\nmodel will likely be models will likely be a bit\nclosed source behind commercial\nmodels\nFunctionality\nMore likely to No/limited support for\nsupport scaling, function calling and\nfunction calling, structured outputs\nstructured outputs Can access logprobs and\nLess likely to expose intermediate outputs,\nlogprobs which are helpful for\nclassi",
        "chapter": 4
      },
      {
        "start": 57500,
        "text": "the human contribution to an innovation\nis significant enough to qualify for a patent.” It’s also unclear whether, if a\nmodel was trained on copyrighted data, and you use this model to create\nyour product, you can defend your product’s IP. Many companies whose\nexistence depends upon their IPs, such as gaming and movie studios, are\nhesitant to use AI to aid in the creation of their products, at least until IP\nlaws around AI are clarified (James Vincent, The Verge, November 15,\n2022).\nConcerns over data lineage have driven some companies toward fully open\nmodels, whose training data has been mad",
        "chapter": 4
      },
      {
        "start": 82500,
        "text": "aluate their new models.\nData contamination can happen indirectly, such as when both evaluation\nand training data come from the same source. For example, you might\ninclude math textbooks in the training data to improve the model’s math\ncapabilities, and someone else might use questions from the same math\ntextbooks to create a benchmark to evaluate the model’s capabilities.\nData contamination can also happen intentionally for good reasons. Let’s\nsay you want to create the best possible model for your users. Initially, you\nexclude benchmark data from the model’s training data and choose the best",
        "chapter": 4
      },
      {
        "start": 57000,
        "text": "mance but said nothing about the models’ training data other\nthan that “all data enrichment workers are paid at least a local living wage”.\nOpenAI’s CTO wasn’t able to provide a satisfactory answer when asked\nwhat data was used to train their models.\nOn top of that, the IP laws around AI are actively evolving. While the US\nPatent and Trademark Office (USPTO) made clear in 2024 that “AI-assisted\ninventions are not categorically unpatentable”, an AI application’s\npatentability depends on “whether the human contribution to an innovation\nis significant enough to qualify for a patent.” It’s also un",
        "chapter": 4
      },
      {
        "start": 11000,
        "text": "ting knowledge (“does the model know that Paris is the capital of\nFrance?”) and reasoning (“can the model infer from a table of business\nexpenses which department is spending the most?”). They aren’t ideal for\nevaluating generation capabilities such as summarization, translation, and\nessay writing. Let’s discuss how generation capabilities can be evaluated in\nthe next section.\nGeneration Capability\nAI was used to generate open-ended outputs long before generative AI\nbecame a thing. For decades, the brightest minds in NLP (natural language\nprocessing) have been working on how to evaluate the qu",
        "chapter": 4
      },
      {
        "start": 58000,
        "text": "r data lineage have driven some companies toward fully open\nmodels, whose training data has been made publicly available. The\nargument is that this allows the community to inspect the data and make\nsure that it’s safe to use. While it sounds great in theory, in practice, it’s\nchallenging for any company to thoroughly inspect a dataset of the size\ntypically used to train foundation models.\nGiven the same concern, many companies opt for commercial models\ninstead. Open source models tend to have limited legal resources compared\nto commercial models. If you use an open source model that infringes ",
        "chapter": 4
      },
      {
        "start": 29500,
        "text": "system that automatically\ngenerates stories for a kid using only the words that they can understand.\nThe model they use needs the ability to follow the instruction to work with\na limited pool of words.\nInstruction-following capability isn’t straightforward to define or measure,\nas it can be easily conflated with domain-specific capability or generation\ncapability. Imagine you ask a model to write a lục bát poem, which is a\nVietnamese verse form. If the model fails to do so, it can either be because\nthe model doesn’t know how to write lục bát, or because it doesn’t\nunderstand what it’s supposed",
        "chapter": 4
      },
      {
        "start": 88500,
        "text": "he application correctly extract the current\nemployer?\nIf applicable, evaluate your application both per turn and per task. A turn\ncan consist of multiple steps and messages. If a system takes multiple steps\nto generate an output, it’s still considered a turn.\nGenerative AI applications, especially chatbot-like applications, allow back-\nand-forth between the user and the application, as in a conversation, to\naccomplish a task. Imagine you want to use an AI model to debug why your\nPython code is failing. The model responds by asking for more information\nabout your hardware or the Python version",
        "chapter": 4
      },
      {
        "start": 28500,
        "text": "RAL. The model seems to understand\nthe sentiment of each tweet, but it generates unexpected outputs such as\nHAPPY and ANGRY. This means that the model has the domain-specific\ncapability to do sentiment analysis on tweets, but its instruction-following\ncapability is poor.\nInstruction-following capability is essential for applications that require\nstructured outputs, such as in JSON format or matching a regular\n6\nexpression (regex). For example, if you ask a model to classify an input as\nA, B, or C, but the model outputs “That’s correct”, this output isn’t very\nhelpful and will likely break down",
        "chapter": 4
      }
    ],
    "input_tokens": 1687,
    "output_tokens": 285,
    "input_cost": 0.00025305,
    "output_cost": 0.000171,
    "total_cost": 0.00042405,
    "reference_answer": "Data lineage influences decisions about adopting open source versus proprietary models, alongside navigating copyright concerns related to data usage in AI training."
  },
  {
    "question": "open source vs proprietary model performance",
    "answer": "The performance differences between open source and proprietary models are nuanced and have specific implications for various applications. Here are the key points to consider based on the provided context:\n\n1. **Performance Gap**: Proprietary models have historically outperformed open source models, particularly in high-stakes applications. Companies often reserve their best models for commercial use, leading many to believe that the strongest open source models will lag behind proprietary options for the foreseeable future. However, there is a notable trend indicating that the performance gap is decreasing, especially on benchmarks like MMLU, suggesting that open source models are improving over time.\n\n2. **Use Case Suitability**: For many applications that do not demand the utmost performance, open source models may suffice. This means that while proprietary models may excel in scenarios requiring the best performance, open source alternatives can be adequate for less demanding tasks.\n\n3. **Development and Feedback**: A significant factor contributing to the performance disparity is the feedback loop. Proprietary models benefit from user feedback in real-world conditions, allowing for iterative improvements. In contrast, open source developers often lack visibility into how their models are utilized, which can hinder their performance advancements.\n\n4. **Community and Customizability**: Open source models often have large communities that can provide support and solutions to common issues, which enhances usability. Organizations typically value the control and customizability that open source models offer, allowing them to tailor solutions to their specific needs.\n\n5. **Legal Protections**: Using open source models may carry legal risks related to copyright infringement, which are more effectively mitigated through contracts in proprietary models. This aspect influences organizational choices significantly.\n\nIn conclusion, while proprietary models currently hold a performance advantage, the gap is narrowing, and open source models provide flexibility and community support that can be advantageous for many use cases.",
    "context": [
      {
        "start": 48000,
        "text": "n open source model yourself. The answer to this\nquestion can significantly reduce your candidate model pool.\nLet’s first go into what exactly open source means when it comes to\nmodels, then discuss the pros and cons of these two approaches.\nOpen source, open weight, and model licenses\nThe term “open source model” has become contentious. Originally, open\nsource was used to refer to any model that people can download and use.\nFor many use cases, being able to download the model is sufficient.\nHowever, some people argue that since a model’s performance is largely a\nfunction of what data it was t",
        "chapter": 4
      },
      {
        "start": 63000,
        "text": "hat.\nIn general, you want a model that is easy to use and manipulate. Typically,\nproprietary models are easier to get started with and scale, but open models\nmight be easier to manipulate as their components are more accessible.\nRegardless of whether you go with open or proprietary models, you want\nthis model to follow a standard API, which makes it easier to swap models.\nMany model developers try to make their models mimic the API of the most\npopular models. As of this writing, many API providers mimic OpenAI’s\nAPI.\nYou might also prefer models with good community support. The more\ncapabiliti",
        "chapter": 4
      },
      {
        "start": 59500,
        "text": "It’s a common practice for\ncompanies to keep their strongest models behind APIs and open source their\nweaker models.\nFigure 4-7. The gap between open source models and proprietary models is decreasing on the\nMMLU benchmark. Image by Maxime Labonne.\nFor this reason, it’s likely that the strongest open source model will lag\nbehind the strongest proprietary models for the foreseeable future.\nHowever, for many use cases that don’t need the strongest models, open\nsource models might be sufficient.\nAnother reason that might cause open source models to lag behind is that\nopen source developers don’t ",
        "chapter": 4
      },
      {
        "start": 60000,
        "text": "other reason that might cause open source models to lag behind is that\nopen source developers don’t receive feedback from users to improve their\nmodels, the way commercial models do. Once a model is open sourced,\nmodel developers have no idea how the model is being used, and how well\nthe model works in the wild.\nFunctionality\nMany functionalities are needed around a model to make it work for a use\ncase. Here are some examples of these functionalities:\nScalability: making sure the inference service can support your\napplication’s traffic while maintaining the desirable latency and cost.\nFunction",
        "chapter": 4
      },
      {
        "start": 59000,
        "text": " decreasing on\nthe MMLU benchmark over time. This trend has made many people believe\nthat one day, there will be an open source model that performs just as well,\nif not better, than the strongest proprietary model.\nAs much as I want open source models to catch up with proprietary models,\nI don’t think the incentives are set up for it. If you have the strongest model\navailable, would you rather open source it for other people to capitalize on\n17\nit, or would you try to capitalize on it yourself? It’s a common practice for\ncompanies to keep their strongest models behind APIs and open source thei",
        "chapter": 4
      },
      {
        "start": 58500,
        "text": "mited legal resources compared\nto commercial models. If you use an open source model that infringes on\ncopyrights, the infringed party is unlikely to go after the model developers,\nand more likely to go after you. However, if you use a commercial model,\nthe contracts you sign with the model providers can potentially protect you\n16\nfrom data lineage risks.\nPerformance\nVarious benchmarks have shown that the gap between open source models\nand proprietary models is closing. Figure 4-7 shows this gap decreasing on\nthe MMLU benchmark over time. This trend has made many people believe\nthat one day, t",
        "chapter": 4
      },
      {
        "start": 63500,
        "text": "rs mimic OpenAI’s\nAPI.\nYou might also prefer models with good community support. The more\ncapabilities a model has, the more quirks it has. A model with a large\ncommunity of users means that any issue you encounter may already have\n19\nbeen experienced by others, who might have shared solutions online.\nControl, access, and transparency\nA 2024 study by a16z shows two key reasons that enterprises care about\nopen source models are control and customizability, as shown in Figure 4-8.\nFigure 4-8. Why enterprises care about open source models. Image from the 2024 study by a16z.\nIf your business depen",
        "chapter": 4
      },
      {
        "start": 53500,
        "text": "gh both OpenAI and Azure APIs. There might be slight\ndifferences in the performance of the same model provided through\ndifferent APIs, as different APIs might use different techniques to optimize\nthis model, so make sure to run thorough tests when you switch between\nmodel APIs.\nCommercial models are only accessible via APIs licensed by the model\n13\ndevelopers. Open source models can be supported by any API provider,\nallowing you to pick and choose the provider that works best for you. For\ncommercial model providers, models are their competitive advantages. For\nAPI providers that don’t have the",
        "chapter": 4
      },
      {
        "start": 113500,
        "text": "nts for auditable information of models and\ntraining data may increase. Commercial models may be able to provide certifications, saving\ncompanies from the effort.\n7\nUsers want models to be open source because open means more information and more options, but\nwhat’s in it for model developers? Many companies have sprung up to capitalize on open source\nmodels by providing inference and finetuning services. It’s not a bad thing. Many people need these\nservices to leverage open source models. But, from model developers’ perspective, why invest\nmillions, if not billions, into building models just f",
        "chapter": 4
      },
      {
        "start": 49500,
        "text": " public,\nregardless of their training data’s availability and licenses.\nAs of this writing, the vast majority of open source models are open weight\nonly. Model developers might hide training data information on purpose, as\nthis information can open model developers to public scrutiny and potential\nlawsuits.\nAnother important attribute of open source models is their licenses. Before\nfoundation models, the open source world was confusing enough, with so\nmany different licenses, such as MIT (Massachusetts Institute of\nTechnology), Apache 2.0, GNU General Public License (GPL), BSD\n(Berkely Softwar",
        "chapter": 4
      }
    ],
    "input_tokens": 1614,
    "output_tokens": 365,
    "input_cost": 0.0002421,
    "output_cost": 0.000219,
    "total_cost": 0.0004611,
    "reference_answer": "The chapter outlines that while the performance gap between open source models and proprietary models is decreasing, strong proprietary models are likely to remain superior. Open source models may still be viable for many applications despite this lag."
  },
  {
    "question": "techniques for detecting factual inconsistency in language models",
    "answer": "To detect factual inconsistency in language models, several techniques can be employed:\n\n1. **Local Factual Consistency**:\n   - This method evaluates outputs against explicitly provided facts (context). An output is factually consistent if it aligns with the given context. For example, if the context states that \"the sky is purple\" and the model outputs \"the sky is purple\", it is considered consistent. This is particularly useful in tasks like summarization, customer support, and business analysis, where the model's responses must conform to specified information.\n\n2. **Global Factual Consistency**:\n   - This approach assesses outputs against general knowledge or widely accepted facts. If a model states \"the sky is blue\" and this aligns with common knowledge, the statement is deemed factually correct.\n\n3. **AI as a Judge**:\n   - A straightforward evaluation technique involves using AI models to assess factual consistency. AI judges can classify the relationship between a premise and a hypothesis, indicating whether they entail, contradict, or are neutral concerning each other.\n\n4. **Specialized Scorers**:\n   - Training dedicated models specialized in factual consistency can improve accuracy. These models analyze a pair of statements (premise and hypothesis) and classify their relationship, which turns factual consistency into a classification task.\n\n5. **Self-Verification**:\n   - Techniques like SelfCheckGPT assume that if a model generates conflicting outputs, the original might be erroneous. This method involves generating multiple statements and analyzing their agreement.\n\n6. **Knowledge-Augmented Verification**:\n   - This approach leverages external sources to validate outputs. For example, an AI model can break down statements into self-contained parts, create fact-checking queries, and use search engine APIs to confirm their accuracy against external knowledge.\n\nThese techniques, among others, help ensure the factual consistency of outputs generated by language models, addressing the potential for misinformation and erroneous assertions.",
    "context": [
      {
        "start": 15000,
        "text": "der two\nsettings: against explicitly provided facts (context) or against open\nknowledge:\nLocal factual consistency\nThe output is evaluated against a context. The output is considered\nfactually consistent if it’s supported by the given context. For\nexample, if the model outputs “the sky is blue” and the given context\nsays that the sky is purple, this output is considered factually\ninconsistent. Conversely, given this context, if the model outputs “the\nsky is purple”, this output is factually consistent.\nLocal factual consistency is important for tasks with limited scopes\nsuch as summarization (",
        "chapter": 4
      },
      {
        "start": 22000,
        "text": " implies factual\ninconsistency, and neutral implies that consistency can’t be determined.\nInstead of using general-purpose AI judges, you can train scorers\nspecialized in factual consistency prediction. These scorers take in a pair of\n(premise, hypothesis) as input and output one of the predefined classes,\nsuch as entailment, contradiction, or neutral. This makes factual consistency\na classification task. For example, DeBERTa-v3-base-mnli-fever-\nanli is a 184-million-parameter model trained on 764,000 annotated\n(hypothesis, premise) pairs to predict entailment.\nBenchmarks for factual consisten",
        "chapter": 4
      },
      {
        "start": 14500,
        "text": ". However, due to its scope, I put it in its own section. The\ntechniques used to measure these qualities can give you a rough idea of how\nto evaluate other qualities you care about.\nFactual consistency\nDue to factual inconsistency’s potential for catastrophic consequences,\nmany techniques have been and will be developed to detect and measure it.\nIt’s impossible to cover them all in one chapter, so I’ll go over only the\nbroad strokes.\nThe factual consistency of a model’s output can be verified under two\nsettings: against explicitly provided facts (context) or against open\nknowledge:\nLocal factu",
        "chapter": 4
      },
      {
        "start": 19500,
        "text": "ry with respect to the original document:\nFactual Consistency: Does the summary\nuntruthful or misleading facts that are not\n3\nsupported by the source text?\nSource Text:\n{{Document}}\nSummary:\n{{Summary}}\nDoes the summary contain factual\ninconsistency?\nAnswer:\nMore sophisticated AI as a judge techniques to evaluate factual consistency\nare self-verification and knowledge-augmented verification:\nSelf-verification\nSelfCheckGPT (Manakul et al., 2023) relies on an assumption that if\na model generates multiple outputs that disagree with one another,\nthe original output is likely hallucinated. Given a ",
        "chapter": 4
      },
      {
        "start": 16500,
        "text": "cination and autism.\nIf no context is given, you’ll have to first search for reliable sources, derive\nfacts, and then validate the statement against these facts.\nOften, the hardest part of factual consistency verification is determining\nwhat the facts are. Whether any of the following statements can be\nconsidered factual depends on what sources you trust: “Messi is the best\nsoccer player in the world”, “climate change is one of the most pressing\ncrises of our time”, “breakfast is the most important meal of the day”. The\ninternet is flooded with misinformation: false marketing claims, statistic",
        "chapter": 4
      },
      {
        "start": 18500,
        "text": "ing for things that don’t exist. For example, if I ask the model “What did X say about\nY?” the model is more likely to hallucinate if X has never said anything about Y than if X has.\nLet’s assume for now that you already have the context to evaluate an\noutput against—this context was either provided by users or retrieved by\nyou (context retrieval is discussed in Chapter 6). The most straightforward\nevaluation approach is AI as a judge. As discussed in Chapter 3, AI judges\ncan be asked to evaluate anything, including factual consistency. Both Liu\net al. (2023) and Luo et al. (2023) showed that ",
        "chapter": 4
      },
      {
        "start": 20500,
        "text": "tuality in Large Language Models”, works by leveraging search\nengine results to verify the response. It works in four steps, as\nvisualized in Figure 4-1:\n1. Use an AI model to decompose the response into individual\nstatements.\n2. Revise each statement to make it self-contained. For example, the\n“it” in the statement “It opened in the 20th century” should be\nchanged to the original subject.\n3. For each statement, propose fact-checking queries to send to a\nGoogle Search API.\n4. Use AI to determine whether the statement is consistent with the\nresearch results.\nFigure 4-1. SAFE breaks an output in",
        "chapter": 4
      },
      {
        "start": 15500,
        "text": "istent.\nLocal factual consistency is important for tasks with limited scopes\nsuch as summarization (the summary should be consistent with the\noriginal document), customer support chatbots (the chatbot’s\nresponses should be consistent with the company’s policies), and\nbusiness analysis (the extracted insights should be consistent with the\ndata).\nGlobal factual consistency\nThe output is evaluated against open knowledge. If the model\noutputs “the sky is blue” and it’s a commonly accepted fact that the\nsky is blue, this statement is considered factually correct. Global\nfactual consistency is impor",
        "chapter": 4
      },
      {
        "start": 34000,
        "text": " to follow content constraints (such as “discuss only climate\nchange”), linguistic guidelines (such as “use Victorian English”), and style\nrules (such as “use a respectful tone”). However, the verification of these\nexpanded instruction types can’t be easily automated. If you instruct a\nmodel to “use language appropriate to a young audience”, how do you\nautomatically verify if the output is indeed appropriate for a young\naudience?\nFor verification, INFOBench authors constructed a list of criteria for each\ninstruction, each framed as a yes/no question. For example, the output to the\ninstruction ",
        "chapter": 4
      },
      {
        "start": 17000,
        "text": "ant meal of the day”. The\ninternet is flooded with misinformation: false marketing claims, statistics\nmade up to advance political agendas, and sensational, biased social media\nposts. In addition, it’s easy to fall for the absence of evidence fallacy. One\nmight take the statement “there’s no link between X and Y” as factually\ncorrect because of a failure to find the evidence that supported the link.\nOne interesting research question is what evidence AI models find\nconvincing, as the answer sheds light on how AI models process conflicting\ninformation and determine what the facts are. For exampl",
        "chapter": 4
      }
    ],
    "input_tokens": 1721,
    "output_tokens": 383,
    "input_cost": 0.00025815,
    "output_cost": 0.0002298,
    "total_cost": 0.00048795,
    "reference_answer": "The chapter introduces various techniques developed to detect factual inconsistency, while noting that comprehensive coverage of these methods is beyond the scope of the chapter."
  },
  {
    "question": "What are the implications of a model's data lineage in AI training?",
    "answer": "The implications of a model's data lineage in AI training are significant for several reasons:\n\n1. **Transparency and Trust**: Understanding a model's data lineage is critical for transparency. Many models lack transparency regarding the data they are trained on, which can lead to trust issues among users and stakeholders. For instance, companies may be hesitant to utilize AI technologies if they can't ascertain the nature of the training data.\n\n2. **Copyright Concerns**: Data lineage raises important copyright issues. If a model is trained on copyrighted data, using it in products could infringe on intellectual property (IP) rights. The evolving nature of IP laws regarding AI further complicates this, as companies are often unsure if their innovations can be patented when derived from AI-generated outputs.\n\n3. **Influence on Model Development Choices**: Concerns over data lineage may lead companies to favor certain types of models. For instance, companies may opt for fully open models where training data is publicly available, believing this enhances safety and trustworthiness. However, the practical challenges of thoroughly inspecting large datasets can hinder this approach.\n\n4. **Data Contamination Risks**: Data lineage also highlights the risk of data contamination, where the same source is used for both training and evaluation data, potentially skewing results. This can compromise the reliability of model performance assessments.\n\n5. **Legal and Ethical Implications**: The lineage of training data can shape ethical considerations in model deployment. Knowing the origins of data can help ensure compliance with legal standards and community expectations regarding data use.\n\nIn summary, a model's data lineage is crucial for ensuring transparency, adhering to copyright laws, guiding development choices, mitigating contamination risks, and navigating legal and ethical considerations in AI training.",
    "context": [
      {
        "start": 56500,
        "text": "rizes 8% of its training set.\nThese memorized samples can be accidentally leaked to users or\nintentionally exploited by bad actors, as demonstrated in Chapter 5.\nData lineage and copyright\nData lineage and copyright concerns can steer a company in many\ndirections: toward open source models, toward proprietary models, or away\nfrom both.\nFor most models, there’s little transparency about what data a model is\ntrained on. In Gemini’s technical report, Google went into detail about the\nmodels’ performance but said nothing about the models’ training data other\nthan that “all data enrichment workers ",
        "chapter": 4
      },
      {
        "start": 82500,
        "text": "aluate their new models.\nData contamination can happen indirectly, such as when both evaluation\nand training data come from the same source. For example, you might\ninclude math textbooks in the training data to improve the model’s math\ncapabilities, and someone else might use questions from the same math\ntextbooks to create a benchmark to evaluate the model’s capabilities.\nData contamination can also happen intentionally for good reasons. Let’s\nsay you want to create the best possible model for your users. Initially, you\nexclude benchmark data from the model’s training data and choose the best",
        "chapter": 4
      },
      {
        "start": 57500,
        "text": "the human contribution to an innovation\nis significant enough to qualify for a patent.” It’s also unclear whether, if a\nmodel was trained on copyrighted data, and you use this model to create\nyour product, you can defend your product’s IP. Many companies whose\nexistence depends upon their IPs, such as gaming and movie studios, are\nhesitant to use AI to aid in the creation of their products, at least until IP\nlaws around AI are clarified (James Vincent, The Verge, November 15,\n2022).\nConcerns over data lineage have driven some companies toward fully open\nmodels, whose training data has been mad",
        "chapter": 4
      },
      {
        "start": 67000,
        "text": "Have to send your Don’t have to send your\ndata to model data externally\nproviders, which Fewer checks and\nmeans your team can balances for data\naccidentally leak lineage/training data\nconfidential info copyright\nPerformance\nBest-performing The best open source\nmodel will likely be models will likely be a bit\nclosed source behind commercial\nmodels\nFunctionality\nMore likely to No/limited support for\nsupport scaling, function calling and\nfunction calling, structured outputs\nstructured outputs Can access logprobs and\nLess likely to expose intermediate outputs,\nlogprobs which are helpful for\nclassi",
        "chapter": 4
      },
      {
        "start": 57000,
        "text": "mance but said nothing about the models’ training data other\nthan that “all data enrichment workers are paid at least a local living wage”.\nOpenAI’s CTO wasn’t able to provide a satisfactory answer when asked\nwhat data was used to train their models.\nOn top of that, the IP laws around AI are actively evolving. While the US\nPatent and Trademark Office (USPTO) made clear in 2024 that “AI-assisted\ninventions are not categorically unpatentable”, an AI application’s\npatentability depends on “whether the human contribution to an innovation\nis significant enough to qualify for a patent.” It’s also un",
        "chapter": 4
      },
      {
        "start": 58000,
        "text": "r data lineage have driven some companies toward fully open\nmodels, whose training data has been made publicly available. The\nargument is that this allows the community to inspect the data and make\nsure that it’s safe to use. While it sounds great in theory, in practice, it’s\nchallenging for any company to thoroughly inspect a dataset of the size\ntypically used to train foundation models.\nGiven the same concern, many companies opt for commercial models\ninstead. Open source models tend to have limited legal resources compared\nto commercial models. If you use an open source model that infringes ",
        "chapter": 4
      },
      {
        "start": 111500,
        "text": "houldn’t say things that Jackie Chan doesn’t\nknow. For example, if Jackie Chan doesn’t speak Vietnamese, you should check that the roleplaying\nmodel doesn’t speak Vietnamese. The “negative knowledge” check is very important for gaming. You\ndon’t want an NPC to accidentally give players spoilers.\n9\nHowever, the electricity cost might be different, depending on the usage.\n0\nAnother argument for making training data public is that since models are likely trained on data\nscraped from the internet, which was generated by the public, the public should have the right to\naccess the models’ training da",
        "chapter": 4
      },
      {
        "start": 11000,
        "text": "ting knowledge (“does the model know that Paris is the capital of\nFrance?”) and reasoning (“can the model infer from a table of business\nexpenses which department is spending the most?”). They aren’t ideal for\nevaluating generation capabilities such as summarization, translation, and\nessay writing. Let’s discuss how generation capabilities can be evaluated in\nthe next section.\nGeneration Capability\nAI was used to generate open-ended outputs long before generative AI\nbecame a thing. For decades, the brightest minds in NLP (natural language\nprocessing) have been working on how to evaluate the qu",
        "chapter": 4
      },
      {
        "start": 29500,
        "text": "system that automatically\ngenerates stories for a kid using only the words that they can understand.\nThe model they use needs the ability to follow the instruction to work with\na limited pool of words.\nInstruction-following capability isn’t straightforward to define or measure,\nas it can be easily conflated with domain-specific capability or generation\ncapability. Imagine you ask a model to write a lục bát poem, which is a\nVietnamese verse form. If the model fails to do so, it can either be because\nthe model doesn’t know how to write lục bát, or because it doesn’t\nunderstand what it’s supposed",
        "chapter": 4
      },
      {
        "start": 48500,
        "text": "ever, some people argue that since a model’s performance is largely a\nfunction of what data it was trained on, a model should be considered open\nonly if its training data is also made publicly available.\nOpen data allows more flexible model usage, such as retraining the model\nfrom scratch with modifications in the model architecture, training process,\nor the training data itself. Open data also makes it easier to understand the\nmodel. Some use cases also required access to the training data for auditing\npurposes, for example, to make sure that the model wasn’t trained on\n10\ncompromised or ille",
        "chapter": 4
      }
    ],
    "input_tokens": 1699,
    "output_tokens": 344,
    "input_cost": 0.00025485,
    "output_cost": 0.0002064,
    "total_cost": 0.00046125,
    "reference_answer": "Understanding a model's data lineage is critical as it affects compliance with licensing terms, particularly regarding whether a model can be trained on outputs from another model without violating rules."
  },
  {
    "question": "best practices for reporting model performance",
    "answer": "When reporting model performance, there are several best practices to consider:\n\n1. **Disclose Data Contamination**: It's essential to report what percentage of the benchmark data was included in your training data. This transparency helps contextualize the reported performance metrics. Ideally, performance should be assessed on both the overall benchmark and the cleaned samples.\n\n2. **Use Relevant Benchmarks**: Select benchmarks that closely align with the specific needs of your application. This involves creating a private leaderboard based on the most pertinent evaluation criteria for your model's function.\n\n3. **Weigh Benchmarks Accordingly**: Not all performance benchmarks are equally important. Evaluate and prioritize benchmarks based on their relevance to your application and how critical each is to your success.\n\n4. **Rigorously Evaluate Using Own Metrics**: Once a small subset of models is selected based on public benchmarks, conduct more rigorous testing with your own benchmarks and metrics to get a better understanding of how well the model will perform in your specific context.\n\n5. **Consider Performance in Relation to Cost**: As you evaluate models, analyze their performance along with the costs incurred, such as computational resources or token fees associated with model API usage.\n\nBy adhering to these practices, you can provide a more accurate and useful report on model performance that reflects the model's effectiveness in the intended application.",
    "context": [
      {
        "start": 41000,
        "text": "bserved by users by careful\nprompting, such as instructing the model to be concise, setting a stopping\ncondition for generation (discussed in Chapter 2), or other optimization\ntechniques (discussed in Chapter 9).\nTIP\nWhen evaluating models based on latency, it’s important to differentiate between the must-have and\nthe nice-to-have. If you ask users if they want lower latency, nobody will ever say no. But high\nlatency is often an annoyance, not a deal breaker.\nIf you use model APIs, they typically charge by tokens. The more input and\noutput tokens you use, the more expensive it is. Many applica",
        "chapter": 4
      },
      {
        "start": 44500,
        "text": "els along the cost–performance axes and choosing the\nmodel that gives the best performance for your bucks\nHowever, the actual selection process is a lot more nuanced. Let’s explore\nwhat it looks like.\nModel Selection Workflow\nWhen looking at models, it’s important to differentiate between hard\nattributes (what is impossible or impractical for you to change) and soft\nattributes (what you can and are willing to change).\nHard attributes are often the results of decisions made by model providers\n(licenses, training data, model size) or your own policies (privacy, control).\nFor some use cases, the ",
        "chapter": 4
      },
      {
        "start": 80500,
        "text": " need to think about how important each benchmark\nis to you and weigh their scores accordingly.\nAs you evaluate models using public benchmarks, keep in mind that the\ngoal of this process is to select a small subset of models to do more rigorous\nexperiments using your own benchmarks and metrics. This is not only\nbecause public benchmarks are unlikely to represent your application’s\nneeds perfectly, but also because they are likely contaminated. How public\nbenchmarks get contaminated and how to handle data contamination will be\nthe topic of the next section.\nData contamination with public benchm",
        "chapter": 4
      },
      {
        "start": 85500,
        "text": " training their models. Ideally,\nwhen reporting your model performance on a benchmark, it’s helpful to\ndisclose what percentage of this benchmark data is in your training data,\nand what the model’s performance is on both the overall benchmark and the\nclean samples of the benchmark. Sadly, because detecting and removing\ncontamination takes effort, many people find it easier to just skip it.\nOpenAI, when analyzing GPT-3’s contamination with common\nbenchmarks, found 13 benchmarks with at least 40% in the training data\n(Brown et al., 2020). The relative difference in performance between\nevaluating",
        "chapter": 4
      },
      {
        "start": 108000,
        "text": "ng fluency, coherence, and\nfaithfulness.\nTo help answer the question of whether to host a model or to use a model\nAPI, this chapter outlined the pros and cons of each approach along seven\naxes, including data privacy, data lineage, performance, functionality,\ncontrol, and cost. This decision, like all the build versus buy decisions, is\nunique to every team, depending not only on what the team needs but also\non what the team wants.\nThis chapter also explored the thousands of available public benchmarks.\nPublic benchmarks can help you weed out bad models, but they won’t help\nyou find the best mo",
        "chapter": 4
      },
      {
        "start": 67000,
        "text": "Have to send your Don’t have to send your\ndata to model data externally\nproviders, which Fewer checks and\nmeans your team can balances for data\naccidentally leak lineage/training data\nconfidential info copyright\nPerformance\nBest-performing The best open source\nmodel will likely be models will likely be a bit\nclosed source behind commercial\nmodels\nFunctionality\nMore likely to No/limited support for\nsupport scaling, function calling and\nfunction calling, structured outputs\nstructured outputs Can access logprobs and\nLess likely to expose intermediate outputs,\nlogprobs which are helpful for\nclassi",
        "chapter": 4
      },
      {
        "start": 1000,
        "text": "select what benchmarks to use? How\nabout public leaderboards that aggregate multiple benchmarks?\nThe model landscape is teeming with proprietary models and open source\nmodels. A question many teams will need to visit over and over again is\nwhether to host their own models or to use a model API. This question has\nbecome more nuanced with the introduction of model API services built on\ntop of open source models.\nThe last part discusses developing an evaluation pipeline that can guide the\ndevelopment of your application over time. This part brings together the\ntechniques we’ve learned throughout ",
        "chapter": 4
      },
      {
        "start": 78000,
        "text": "rboards with public benchmarks\nWhen evaluating models for a specific application, you’re basically creating\na private leaderboard that ranks models based on your evaluation criteria.\nThe first step is to gather a list of benchmarks that evaluate the capabilities\nimportant to your application. If you want to build a coding agent, look at\ncode-related benchmarks. If you build a writing assistant, look into creative\nwriting benchmarks. As new benchmarks are constantly introduced and old\nbenchmarks become saturated, you should look for the latest benchmarks.\nMake sure to evaluate how reliable a be",
        "chapter": 4
      },
      {
        "start": 67500,
        "text": "ccess logprobs and\nLess likely to expose intermediate outputs,\nlogprobs which are helpful for\nclassification tasks,\nUsing model APIs Self-hosting models\nevaluation, and\ninterpretability\nCost\nAPI cost Talent, time, engineering\neffort to optimize, host,\nmaintain (can be\nmitigated by using model\nhosting services)\nFinetuning\nCan only finetune Can finetune, quantize,\nmodels that model and optimize models (if\nproviders let you their licenses allow), but\nit can be hard to do so\nControl,\nRate limits Easier to inspect changes\naccess, and\nRisk of losing access in open source models\ntransparency\nto the m",
        "chapter": 4
      },
      {
        "start": 43500,
        "text": "your criteria, let’s move on to the next step and use them\nto select the best model for your application.\nModel Selection\nAt the end of the day, you don’t really care about which model is the best.\nYou care about which model is the best for your applications. Once you’ve\ndefined the criteria for your application, you should evaluate models against\nthese criteria.\nDuring the application development process, as you progress through\ndifferent adaptation techniques, you’ll have to do model selection over and\nover again. For example, prompt engineering might start with the strongest\nmodel overall t",
        "chapter": 4
      }
    ],
    "input_tokens": 1601,
    "output_tokens": 265,
    "input_cost": 0.00024015,
    "output_cost": 0.000159,
    "total_cost": 0.00039915,
    "reference_answer": "The chapter emphasizes the importance of disclosing the percentage of benchmark data used in training when reporting model performance, along with results on overall and clean samples of the benchmark."
  },
  {
    "question": "advanced techniques for evaluating LLMs",
    "answer": "To evaluate Large Language Models (LLMs) effectively, several advanced techniques can be applied:\n\n1. **Evaluation Harnesses**: Utilize tools like EleutherAI's lm-evaluation-harness or OpenAI's evals, which support hundreds of benchmarks. These tools allow for extensive evaluations across various capabilities, such as mathematical reasoning or task completion.\n\n2. **Benchmark Selection and Aggregation**: Choose benchmarks thoughtfully to ensure comprehensive model evaluation. Different leaderboards may use different benchmarks, so it's crucial to analyze which benchmarks are most relevant for the tasks your model will perform.\n\n3. **Custom Evaluation Pipelines**: Develop your own evaluation pipeline tailored to your specific needs. This can include both open-ended and closed-ended questions to assess the model's performance accurately, ensuring that the evaluation reflects the model's capabilities in the context of your application.\n\n4. **Iterative Evaluation Process**: Adopt an iterative workflow that includes filtering out models based on hard attributes and public performance data. Experiment continuously with your models while balancing objectives like quality, latency, and cost.\n\n5. **Human Validation of Criteria**: Refine evaluation rubrics by validating them with human participants to ensure clarity and effectiveness. This can help establish unambiguous guidelines that facilitate consistent evaluations.\n\n6. **Experiment Tracking**: Maintain thorough logs of varying factors in the evaluation process, such as evaluation data, rubrics, and prompt configurations. This will help in ensuring reproducibility and tracking the model's performance over time.\n\n7. **Contextualizing Metrics**: Align evaluation metrics with business objectives. Ensure that the performance of the LLM aligns with desired outcomes and business goals, allowing for better decision-making.\n\nBy integrating these advanced techniques into your evaluation frameworks, you can obtain a more nuanced understanding of your model's performance and applicability.",
    "context": [
      {
        "start": 69000,
        "text": "s\nsaturate, necessitating the introduction of new benchmarks.\nA tool that helps you evaluate a model on multiple benchmarks is an\nevaluation harness. As of this writing, EleutherAI’s lm-evaluation-harness\nsupports over 400 benchmarks. OpenAI’s evals lets you run any of the\napproximately 500 existing benchmarks and register new benchmarks to\nevaluate OpenAI models. Their benchmarks evaluate a wide range of\ncapabilities, from doing math and solving puzzles to identifying ASCII art\nthat represents words.\nBenchmark selection and aggregation\nBenchmark results help you identify promising models for ",
        "chapter": 4
      },
      {
        "start": 46500,
        "text": "3. Run experiments with your own evaluation pipeline to find the best\nmodel, again, balancing all your objectives.\n4. Continually monitor your model in production to detect failure and\ncollect feedback to improve your application.\nFigure 4-5. An overview of the evaluation workflow to evaluate models for your application.\nThese four steps are iterative—you might want to change the decision from\na previous step with newer information from the current step. For example,\nyou might initially want to host open source models. However, after public\nand private evaluation, you might realize that open s",
        "chapter": 4
      },
      {
        "start": 46000,
        "text": "vel, the evaluation workflow consists of four steps (see\nFigure 4-5):\n1. Filter out models whose hard attributes don’t work for you. Your list of\nhard attributes depends heavily on your own internal policies, whether\nyou want to use commercial APIs or host your own models.\n2. Use publicly available information, e.g., benchmark performance and\nleaderboard ranking, to narrow down the most promising models to\nexperiment with, balancing different objectives such as model quality,\nlatency, and cost.\n3. Run experiments with your own evaluation pipeline to find the best\nmodel, again, balancing all yo",
        "chapter": 4
      },
      {
        "start": 109500,
        "text": "ifferent methods and approaches can help mitigate many of these\nchallenges.\nEven though dedicated discussions on evaluation end here, evaluation will\ncome up again and again, not just throughout the book but also throughout\nyour application development process. Chapter 6 explores evaluating\nretrieval and agentic systems, while Chapters 7 and 9 focus on calculating a\nmodel’s memory usage, latency, and costs. Data quality verification is\naddressed in Chapter 8, and using user feedback to evaluate production\napplications is addressed in Chapter 10.\nWith that, let’s move onto the actual model adap",
        "chapter": 4
      },
      {
        "start": 8000,
        "text": "puts are easier to verify\nand reproduce. For example, if you want to evaluate a model’s ability to do\nmath, an open-ended approach is to ask the model to generate the solution\nto a given problem. A close-ended approach is to give the model several\noptions and let it pick the correct one. If the expected answer is option C\nand the model outputs option A, the model is wrong.\nThis is the approach that most public benchmarks follow. In April 2024,\n75% of the tasks in Eleuther’s lm-evaluation-harness are multiple-choice,\nincluding UC Berkeley’s MMLU (2020), Microsoft’s AGIEval (2023), and\nthe AI2 R",
        "chapter": 4
      },
      {
        "start": 71500,
        "text": " are generally thoughtful about\nhow they select benchmarks, their decision-making process isn’t always\nclear to users. Different leaderboards often end up with different\nbenchmarks, making it hard to compare and interpret their rankings. For\nexample, in late 2023, Hugging Face updated their Open LLM Leaderboard\nto use the average of six different benchmarks to rank models:\n1. ARC-C (Clark et al., 2018): Measuring the ability to solve complex,\ngrade school-level science questions.\n2. MMLU (Hendrycks et al., 2020): Measuring knowledge and reasoning\ncapabilities in 57 subjects, including elementa",
        "chapter": 4
      },
      {
        "start": 30500,
        "text": "and\nINFOBench, measure models’ capability to follow a wide range of\ninstructions, which are to give you ideas on how to evaluate a model’s\nability to follow your instructions: what criteria to use, what instructions to\ninclude in the evaluation set, and what evaluation methods are appropriate.\nThe Google benchmark IFEval, Instruction-Following Evaluation, focuses\non whether the model can produce outputs following an expected format.\nZhou et al. (2023) identified 25 types of instructions that can be\nautomatically verified, such as keyword inclusion, length constraints,\nnumber of bullet points, ",
        "chapter": 4
      },
      {
        "start": 47500,
        "text": "mber of public benchmarks and why you can’t trust them. This will set\nthe stage for the last section in the chapter. Because public benchmarks\ncan’t be trusted, you need to design your own evaluation pipeline with\nprompts and metrics you can trust.\nModel Build Versus Buy\nAn evergreen question for companies when leveraging any technology is\nwhether to build or buy. Since most companies won’t be building\nfoundation models from scratch, the question is whether to use commercial\nmodel APIs or host an open source model yourself. The answer to this\nquestion can significantly reduce your candidate mo",
        "chapter": 4
      },
      {
        "start": 106500,
        "text": "e iteration is necessary, you should be able to expect\na certain level of consistency from your evaluation pipeline. If the\nevaluation process changes constantly, you won’t be able to use the\nevaluation results to guide your application’s development.\nAs you iterate on your evaluation pipeline, make sure to do proper\nexperiment tracking: log all variables that could change in an evaluation\nprocess, including but not limited to the evaluation data, the rubric, and the\nprompt and sampling configurations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most imp",
        "chapter": 4
      },
      {
        "start": 93500,
        "text": " deserve a 1? Validate\nyour rubric with humans: yourself, coworkers, friends, etc. If humans find it\nhard to follow the rubric, you need to refine it to make it unambiguous. This\nprocess can require a lot of back and forth, but it’s necessary. A clear\nguideline is the backbone of a reliable evaluation pipeline. This guideline\ncan also be reused later for training data annotation, as discussed in\nChapter 8.\nTie evaluation metrics to business metrics\nWithin a business, an application must serve a business goal. The\napplication’s metrics must be considered in the context of the business\nproblem i",
        "chapter": 4
      }
    ],
    "input_tokens": 1621,
    "output_tokens": 360,
    "input_cost": 0.00024315,
    "output_cost": 0.000216,
    "total_cost": 0.00045915,
    "reference_answer": "The text suggests using diverse evaluation criteria, including out-of-scope inputs and error-prone examples, to rigorously assess the robustness and reliability of large language models."
  },
  {
    "question": "how to evaluate AI applications",
    "answer": "To evaluate AI applications effectively, follow these key steps based on the context provided in the \"AI Engineering\" book by Chip Huyen:\n\n1. **Establish Evaluation Criteria**: Begin by defining a set of evaluation criteria tailored to your specific application. Criteria can be categorized into several buckets, such as:\n   - **Domain-Specific Capability**: Measures how well the AI understands and performs tasks within a particular domain.\n   - **Generation Capability**: Assesses the quality and relevance of generated outputs.\n   - **Instruction-Following Capability**: Evaluates how well the AI adheres to given instructions.\n   - **Cost and Latency**: Considers the operational costs and the speed of AI responses.\n\n2. **Design Your Evaluation Pipeline**: Build a reliable evaluation pipeline capable of differentiating between good and bad outcomes. This pipeline can include different evaluation methods and techniques tailored to both open-ended and close-ended tasks.\n\n3. **Evaluate All Components in the System**: Since real-world AI applications are complex, evaluate both the overall end-to-end output as well as the intermediate outputs of individual components within the system. For example, in an application that processes resumes, evaluate the output of each step—text extraction and employer identification.\n\n4. **Iterate with Evaluation Over Time**: Implement continuous evaluation processes that adapt and improve based on real-world usage. This might include periodic assessments, where human experts review a subset of outputs to track performance changes and detect unusual patterns.\n\n5. **Understand Business Metrics**: Ground your evaluation metrics in business objectives, focusing on metrics like engagement (e.g., active users or interaction duration) to gauge the success and utility of the application.\n\n6. **Establish Minimum Performance Standards**: Define thresholds that application metrics must meet to be considered viable. For example, a chatbot may require a factual consistency score above a specified percentage to be deemed useful.\n\nBy following these steps, you'll create a robust evaluation framework that can enhance the AI application's reliability, reduce risks, and identify performance improvement opportunities.",
    "context": [
      {
        "start": 4500,
        "text": "to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can think of criteria in the\nfollowing buckets: domain-specific capability, generation capability,\ninstruction-following capability, and cost and latency.\nImagine you ask a model to summarize a legal contract. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstand",
        "chapter": 4
      },
      {
        "start": 87000,
        "text": "opic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each ap",
        "chapter": 4
      },
      {
        "start": 4000,
        "text": "timent analysis, next-\naction prediction, etc. It’s much easier to evaluate classification tasks\nthan open-ended tasks.\nWhile the evaluation-driven development approach makes sense from a\nbusiness perspective, focusing only on applications whose outcomes can be\nmeasured is similar to looking for the lost key under the lamppost (at night).\nIt’s easier to do, but it doesn’t mean we’ll find the key. We might be missing\nout on many potentially game-changing applications because there is no\neasy way to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto ",
        "chapter": 4
      },
      {
        "start": 1500,
        "text": "nt of your application over time. This part brings together the\ntechniques we’ve learned throughout the book to evaluate concrete\napplications.\nEvaluation Criteria\nWhich is worse—an application that has never been deployed or an\napplication that is deployed but no one knows whether it’s working? When I\nasked this question at conferences, most people said the latter. An\napplication that is deployed but can’t be evaluated is worse. It costs to\nmaintain, but if you want to take it down, it might cost even more.\nAI applications with questionable returns on investment are, unfortunately,\nquite comm",
        "chapter": 4
      },
      {
        "start": 95000,
        "text": ": what scores must an\napplication achieve for it to be useful? For example, you might determine\nthat your chatbot’s factual consistency score must be at least 50% for it to\nbe useful. Anything below this makes it unusable even for general customer\nrequests.\nBefore developing AI evaluation metrics, it’s crucial to first understand the\nbusiness metrics you’re targeting. Many applications focus on stickiness\nmetrics, such as daily, weekly, or monthly active users (DAU, WAU,\nMAU). Others prioritize engagement metrics, like the number of\nconversations a user initiates per month or the duration of e",
        "chapter": 4
      },
      {
        "start": 87500,
        "text": "process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each application might consist of\nmany components, and a task might be completed after many turns.\nEvaluation can happen at different levels: per task, per turn, and per\nintermediate output.\nYou should evaluate the end-to-end output and each component’s\nintermediate output independently. Consider an application that extracts a\nperson’s current employer from their resume PDF, which works in two\nsteps:\n1. Extract all the text from the PDF.\n2. Extract the current employer from the extracted text.\nIf the m",
        "chapter": 4
      },
      {
        "start": 107000,
        "text": "ations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enable you to reduce risks, discover opportunities\nto improve performance, and benchmark progresses, which will all save\nyou time and headaches down the line.\nGiven an increasing number of readily available foundation models, for\nmost application developers, the challenge is no longer in developing\nmodel",
        "chapter": 4
      },
      {
        "start": 0,
        "text": "Chapter 4. Evaluate AI Systems\nA model is only useful if it works for its intended purposes. You need to\nevaluate models in the context of your application. Chapter 3 discusses\ndifferent approaches to automatic evaluation. This chapter discusses how to\nuse these approaches to evaluate models for your applications.\nThis chapter contains three parts. It starts with a discussion of the criteria\nyou might use to evaluate your applications and how these criteria are\ndefined and calculated. For example, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-sp",
        "chapter": 4
      },
      {
        "start": 38000,
        "text": "e role, you\nmight be able to create heuristics to evaluate the model’s outputs. For\nexample, if the role is someone who doesn’t talk a lot, a heuristic would be\nthe average of the model’s outputs. Other than that, the easiest automatic\nevaluation approach is AI as a judge. You should evaluate the roleplaying\nAI on both style and knowledge. For example, if a model is supposed to\ntalk like Jackie Chan, its outputs should capture Jackie Chan’s style and are\n8\ngenerated based on Jackie Chan’s knowledge.\nAI judges for different roles will need different prompts. To give you a\nsense of what an AI ju",
        "chapter": 4
      },
      {
        "start": 98000,
        "text": "evelopment. Each day, you can use human experts to evaluate a subset of\nyour application’s outputs that day to detect any changes in the application’s\nperformance or unusual patterns in usage. For example, LinkedIn developed\na process to manually evaluate up to 500 daily conservations with their AI\nsystems.\nConsider evaluation methods to be used not just during experimentation but\nalso during production. During experimentation, you might have reference\ndata to compare your application’s outputs to, whereas, in production,\nreference data might not be immediately available. However, in productio",
        "chapter": 4
      }
    ],
    "input_tokens": 1593,
    "output_tokens": 410,
    "input_cost": 0.00023895,
    "output_cost": 0.000246,
    "total_cost": 0.00048495,
    "reference_answer": "The chapter highlights the importance of establishing specific evaluation criteria for AI applications, focusing on domain-specific capability, generation capability, instruction-following capability, and cost and latency to guide effective evaluations."
  },
  {
    "question": "mean win rate in model evaluation",
    "answer": "The mean win rate in model evaluation, as defined by the authors of HELM, is the fraction of times a model scores better than another model, averaged across different scenarios. This approach shuns the traditional averaging of scores, which treats all benchmarks equally regardless of their complexity or relevance. Instead, the mean win rate provides a more nuanced perspective by focusing on the performance comparison between models across various benchmarks.",
    "context": [
      {
        "start": 77000,
        "text": "ores equally, i.e., treating an 80% score on\nTruthfulQA the same as an 80% score on GSM-8K, even if an 80% score\non TruthfulQA might be much harder to achieve than an 80% score on\nGSM-8K. This also means giving all benchmarks the same weight, even if,\nfor some tasks, truthfulness might weigh a lot more than being able to solve\ngrade school math problems.\nHELM authors, on the other hand, decided to shun averaging in favor of\nmean win rate, which they defined as “the fraction of times a model obtains\na better score than another model, averaged across scenarios”.\nWhile public leaderboards are use",
        "chapter": 4
      },
      {
        "start": 80000,
        "text": "spent approximately $80,000–$100,000\n26\nto evaluate 30 models on their full HELM suite. The more models you\nwant to evaluate and the more benchmarks you want to use, the more\nexpensive it gets.\nOnce you’ve selected a set of benchmarks and obtained the scores for the\nmodels you care about on these benchmarks, you then need to aggregate\nthese scores to rank models. Not all benchmark scores are in the same unit\nor scale. One benchmark might use accuracy, another F1, and another\nBLEU score. You will need to think about how important each benchmark\nis to you and weigh their scores accordingly.\nAs y",
        "chapter": 4
      },
      {
        "start": 79500,
        "text": " I doubt that\n24\nOpenAI would fly completely blind. If the second reason is true, it\nreinforces the idea that the best model overall might not be the best model\nfor your application.\nNot all models have publicly available scores on all benchmarks. If the\nmodel you care about doesn’t have a publicly available score on your\n25\nbenchmark, you will need to run the evaluation yourself. Hopefully, an\nevaluation harness can help you with that. Running benchmarks can be\nexpensive. For example, Stanford spent approximately $80,000–$100,000\n26\nto evaluate 30 models on their full HELM suite. The more mod",
        "chapter": 4
      },
      {
        "start": 46000,
        "text": "vel, the evaluation workflow consists of four steps (see\nFigure 4-5):\n1. Filter out models whose hard attributes don’t work for you. Your list of\nhard attributes depends heavily on your own internal policies, whether\nyou want to use commercial APIs or host your own models.\n2. Use publicly available information, e.g., benchmark performance and\nleaderboard ranking, to narrow down the most promising models to\nexperiment with, balancing different objectives such as model quality,\nlatency, and cost.\n3. Run experiments with your own evaluation pipeline to find the best\nmodel, again, balancing all yo",
        "chapter": 4
      },
      {
        "start": 78000,
        "text": "rboards with public benchmarks\nWhen evaluating models for a specific application, you’re basically creating\na private leaderboard that ranks models based on your evaluation criteria.\nThe first step is to gather a list of benchmarks that evaluate the capabilities\nimportant to your application. If you want to build a coding agent, look at\ncode-related benchmarks. If you build a writing assistant, look into creative\nwriting benchmarks. As new benchmarks are constantly introduced and old\nbenchmarks become saturated, you should look for the latest benchmarks.\nMake sure to evaluate how reliable a be",
        "chapter": 4
      },
      {
        "start": 103000,
        "text": "uation results.\nRepeat for a number of times. If the evaluation results vary wildly for\ndifferent bootstraps, this means that you’ll need a bigger evaluation set.\nEvaluation results are used not just to evaluate a system in isolation but also\nto compare systems. They should help you decide which model, prompt, or\nother component is better. Say a new prompt achieves a 10% higher score\nthan the old prompt—how big does the evaluation set have to be for us to\nbe certain that the new prompt is indeed better? In theory, a statistical\nsignificance test can be used to compute the sample size needed fo",
        "chapter": 4
      },
      {
        "start": 76500,
        "text": "d, compute\nARC-C HellaSwag MMLU Truth\nARC-C 1.0000 0.4812 0.8672 0.480\nHellaSwag 0.4812 1.0000 0.6105 0.480\nMMLU 0.8672 0.6105 1.0000 0.550\nTruthfulQA 0.4809 0.4228 0.5507 1.000\nWinoGrande 0.8856 0.4842 0.9011 0.455\nGSM-8K 0.7438 0.3547 0.7936 0.500\nThe results from all the selected benchmarks need to be aggregated to rank\nmodels. As of this writing, Hugging Face averages a model’s scores on all\nthese benchmarks to get the final score to rank that model. Averaging means\ntreating all benchmark scores equally, i.e., treating an 80% score on\nTruthfulQA the same as an 80% score on GSM-8K, even if ",
        "chapter": 4
      },
      {
        "start": 45500,
        "text": "s. However, the accuracy jumped to\n70% after I decomposed the task into two steps. At the same time, I’ve had\nsituations where a model remained unusable for my task even after weeks\nof tweaking, and I had to give up on that model.\nWhat you define as hard and soft attributes depends on both the model and\nyour use case. For example, latency is a soft attribute if you have access to\nthe model to optimize it to run faster. It’s a hard attribute if you use a model\nhosted by someone else.\nAt a high level, the evaluation workflow consists of four steps (see\nFigure 4-5):\n1. Filter out models whose har",
        "chapter": 4
      },
      {
        "start": 106000,
        "text": "atency does your evaluation pipeline add to your\napplication?\nEvaluation, if not done carefully, can add significant latency and cost\nto your application. Some teams decide to skip evaluation in the hope\nof reducing latency. It’s a risky bet.\nIterate\nAs your needs and user behaviors change, your evaluation criteria will also\nevolve, and you’ll need to iterate on your evaluation pipeline. You might\nneed to update the evaluation criteria, change the scoring rubric, and add or\nremove examples. While iteration is necessary, you should be able to expect\na certain level of consistency from your eval",
        "chapter": 4
      },
      {
        "start": 89000,
        "text": "failing. The model responds by asking for more information\nabout your hardware or the Python version you’re using. Only after you’ve\nprovided this information can the model help you debug.\nTurn-based evaluation evaluates the quality of each output. Task-based\nevaluation evaluates whether a system completes a task. Did the application\nhelp you fix the bug? How many turns did it take to complete the task? It\nmakes a big difference if a system is able to solve a problem in two turns or\nin twenty turns.\nGiven that what users really care about is whether a model can help them\naccomplish their tasks",
        "chapter": 4
      }
    ],
    "input_tokens": 1774,
    "output_tokens": 81,
    "input_cost": 0.0002661,
    "output_cost": 4.86e-05,
    "total_cost": 0.0003147,
    "reference_answer": "The chapter introduces the concept of mean win rate as an alternative to average scoring, focusing on a model's performance relative to others across different scenarios, which can offer a clearer assessment of capability."
  },
  {
    "question": "costs of hosting AI models",
    "answer": "The costs of hosting AI models can vary significantly between using model APIs and self-hosting. Here are some key considerations based on the provided context:\n\n1. **API Costs**: Model APIs typically charge based on usage, which can become quite expensive as the volume of input and output tokens increases. This can lead to significant costs, especially for applications with heavy usage.\n\n2. **Self-Hosting Costs**: When hosting your own models, the primary cost outside of engineering efforts is the compute cost. Once a company has invested in a cluster capable of serving a high volume of tokens (e.g., up to 1 billion tokens a day), the per-token cost can decrease as scale increases. For example, whether you serve 1 million or 1 billion tokens daily, the compute cost remains constant.\n\n3. **Initial Investment and Engineering Effort**: Hosting a model on your own requires considerable engineering talent, effort, and time to optimize and scale the model effectively. Companies will need to weigh these engineering costs against the potential operational savings from lower per-token costs when processing large volumes.\n\n4. **Comparison of Overall Costs**: According to a reported total cost for commercial APIs at around $38,000 versus approximately $80,000 to $100,000 for open-source models based on GPU hours, self-hosting can offer substantial cost savings at scale, despite the upfront engineering investments.\n\n5. **Strategic Considerations**: The decision to use model APIs or self-host models should account for various axes, including costs, data privacy, and control. Over time, a company’s needs may evolve, influencing this choice.\n\nIn summary, while model APIs may involve straightforward costs based on usage, hosting your own models can lead to lower costs at scale, though it requires significant initial investment and engineering resources.",
    "context": [
      {
        "start": 42000,
        "text": "hese memory\nconfigurations. It’s not a coincidence that many models today have 7 billion\nor 65 billion parameters.\nIf you use model APIs, your cost per token usually doesn’t change much as\nyou scale. However, if you host your own models, your cost per token can\nget much cheaper as you scale. If you’ve already invested in a cluster that\ncan serve a maximum of 1 billion tokens a day, the compute cost remains\n9\nthe same whether you serve 1 million tokens or 1 billion tokens a day.\nTherefore, at different scales, companies need to reevaluate whether it\nmakes more sense to use model APIs or to host",
        "chapter": 4
      },
      {
        "start": 41500,
        "text": "y charge by tokens. The more input and\noutput tokens you use, the more expensive it is. Many applications then try\nto reduce the input and output token count to manage cost.\nIf you host your own models, your cost, outside engineering cost, is\ncompute. To make the most out of the machines they have, many people\nchoose the largest models that can fit their machines. For example, GPUs\nusually come with 16 GB, 24 GB, 48 GB, and 80 GB of memory.\nTherefore, many popular models are those that max out these memory\nconfigurations. It’s not a coincidence that many models today have 7 billion\nor 65 billi",
        "chapter": 4
      },
      {
        "start": 62000,
        "text": "elf. Keep in mind that\nthere are multiple types of finetuning, such as partial finetuning and full\nfinetuning, as discussed in Chapter 7. A commercial model provider might\nsupport only some types of finetuning, not all.\nAPI cost versus engineering cost\nModel APIs charge per usage, which means that they can get prohibitively\nexpensive with heavy usage. At a certain scale, a company that is bleeding\n18\nits resources using APIs might consider hosting their own models.\nHowever, hosting a model yourself requires nontrivial time, talent, and\nengineering effort. You’ll need to optimize the model, sca",
        "chapter": 4
      },
      {
        "start": 54500,
        "text": "providers like AWS, Azure, and GCP all\nprovide API access to popular open source models. A plethora of startups\nare doing the same.\nNOTE\nThere are also commercial API providers that can deploy their services within your private networks.\nIn this discussion, I treat these privately deployed commercial APIs similarly to self-hosted models.\nThe answer to whether to host a model yourself or use a model API depends\non the use case. And the same use case can change over time. Here are\nseven axes to consider: data privacy, data lineage, performance,\nfunctionality, costs, control, and on-device deploy",
        "chapter": 4
      },
      {
        "start": 116500,
        "text": "able score, check how reliable the score is.\n6\nThe HELM paper reported that the total cost is $38,000 for commercial APIs and 19,500 GPU hours\nfor open models. If an hour of GPU costs between $2.15 and $3.18, the total cost comes out to\n$80,000–$100,000.\n7\nA friend quipped: “A benchmark stops being useful as soon as it becomes public.”\n8\nThis is because the square root of 10 is approximately 3.3.",
        "chapter": 4
      },
      {
        "start": 66500,
        "text": " use cases, running a model locally is desirable. It could\nbe because your use case targets an area without reliable internet access. It\ncould be for privacy reasons, such as when you want to give an AI assistant\naccess to all your data, but don’t want your data to leave your device.\nTable 4-4 summarizes the pros and cons of using model APIs and self-\nhosting models.\nTable 4-4. Pros and cons of using model APIs and self-hosting models (cons in italics).\nUsing model APIs Self-hosting models\nData\nHave to send your Don’t have to send your\ndata to model data externally\nproviders, which Fewer check",
        "chapter": 4
      },
      {
        "start": 68000,
        "text": "ier to inspect changes\naccess, and\nRisk of losing access in open source models\ntransparency\nto the model You can freeze a model\nLack of transparency to maintain its access, but\nin model changes and you’re responsible for\nversioning\nUsing model APIs Self-hosting models\nbuilding and maintaining\nmodel APIs\nEdge use cases\nCan’t run on device Can run on device, but\nwithout internet again, might be hard to\naccess do so\nThe pros and cons of each approach hopefully can help you decide whether\nto use a commercial API or to host a model yourself. This decision should\nsignificantly narrow your options. N",
        "chapter": 4
      },
      {
        "start": 47000,
        "text": "host open source models. However, after public\nand private evaluation, you might realize that open source models can’t\nachieve the level of performance you want and have to switch to\ncommercial APIs.\nChapter 10 discusses monitoring and collecting user feedback. The rest of\nthis chapter will discuss the first three steps. First, let’s discuss a question\nthat most teams will visit more than once: to use model APIs or to host\nmodels themselves. We’ll then continue to how to navigate the dizzying\nnumber of public benchmarks and why you can’t trust them. This will set\nthe stage for the last section",
        "chapter": 4
      },
      {
        "start": 108000,
        "text": "ng fluency, coherence, and\nfaithfulness.\nTo help answer the question of whether to host a model or to use a model\nAPI, this chapter outlined the pros and cons of each approach along seven\naxes, including data privacy, data lineage, performance, functionality,\ncontrol, and cost. This decision, like all the build versus buy decisions, is\nunique to every team, depending not only on what the team needs but also\non what the team wants.\nThis chapter also explored the thousands of available public benchmarks.\nPublic benchmarks can help you weed out bad models, but they won’t help\nyou find the best mo",
        "chapter": 4
      },
      {
        "start": 48000,
        "text": "n open source model yourself. The answer to this\nquestion can significantly reduce your candidate model pool.\nLet’s first go into what exactly open source means when it comes to\nmodels, then discuss the pros and cons of these two approaches.\nOpen source, open weight, and model licenses\nThe term “open source model” has become contentious. Originally, open\nsource was used to refer to any model that people can download and use.\nFor many use cases, being able to download the model is sufficient.\nHowever, some people argue that since a model’s performance is largely a\nfunction of what data it was t",
        "chapter": 4
      }
    ],
    "input_tokens": 1666,
    "output_tokens": 364,
    "input_cost": 0.0002499,
    "output_cost": 0.0002184,
    "total_cost": 0.0004683,
    "reference_answer": "The chapter outlines the costs associated with self-hosting models, highlighting the time, talent, and engineering effort required, which can be alleviated by using model hosting services."
  },
  {
    "question": "best practices for evaluating AI models",
    "answer": "To effectively evaluate AI models based on the guidelines from the \"AI Engineering\" book by Chip Huyen, consider the following best practices:\n\n1. **Define Clear Evaluation Criteria**: Start with a list of specific criteria tailored to your application, such as domain-specific capabilities, generation quality, instruction-following accuracy, cost, and latency. This will provide a structured framework for assessing model performance.\n\n2. **Filter Models Based on Hard Attributes**: Eliminate candidates that do not meet your essential requirements. These could be related to internal policies, such as preferences for commercial APIs versus self-hosted models.\n\n3. **Leverage Public Benchmarks**: Use publicly available information, benchmarks, and leaderboard rankings to narrow down your options. This helps address different objectives, including model quality and cost-effectiveness.\n\n4. **Run Experiments and Use a Custom Evaluation Pipeline**: Implement your own evaluation pipeline to experiment with the shortlisted models. This will help you find the model that best meets your application’s specific needs.\n\n5. **Iterate and Monitor Continuously**: Model evaluation should be an iterative process. Adjust your decision-making based on new information gained from running experiments. Continuously monitor the model in production to detect failures and gather feedback for further improvements.\n\n6. **Evaluate All Components of the System**: Given that real-world applications are complex, ensure that you assess every component involved in the system to understand the overall performance.\n\n7. **Balance Automatic and Human Evaluation**: Use automatic metrics where feasible, but do not hesitate to incorporate human assessments, especially for open-ended tasks, where human evaluation can serve as a critical guide for quality.\n\nFollowing these best practices will help ensure you effectively evaluate AI models against your defined criteria and improve the overall reliability and effectiveness of your AI applications.",
    "context": [
      {
        "start": 46500,
        "text": "3. Run experiments with your own evaluation pipeline to find the best\nmodel, again, balancing all your objectives.\n4. Continually monitor your model in production to detect failure and\ncollect feedback to improve your application.\nFigure 4-5. An overview of the evaluation workflow to evaluate models for your application.\nThese four steps are iterative—you might want to change the decision from\na previous step with newer information from the current step. For example,\nyou might initially want to host open source models. However, after public\nand private evaluation, you might realize that open s",
        "chapter": 4
      },
      {
        "start": 46000,
        "text": "vel, the evaluation workflow consists of four steps (see\nFigure 4-5):\n1. Filter out models whose hard attributes don’t work for you. Your list of\nhard attributes depends heavily on your own internal policies, whether\nyou want to use commercial APIs or host your own models.\n2. Use publicly available information, e.g., benchmark performance and\nleaderboard ranking, to narrow down the most promising models to\nexperiment with, balancing different objectives such as model quality,\nlatency, and cost.\n3. Run experiments with your own evaluation pipeline to find the best\nmodel, again, balancing all yo",
        "chapter": 4
      },
      {
        "start": 0,
        "text": "Chapter 4. Evaluate AI Systems\nA model is only useful if it works for its intended purposes. You need to\nevaluate models in the context of your application. Chapter 3 discusses\ndifferent approaches to automatic evaluation. This chapter discusses how to\nuse these approaches to evaluate models for your applications.\nThis chapter contains three parts. It starts with a discussion of the criteria\nyou might use to evaluate your applications and how these criteria are\ndefined and calculated. For example, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-sp",
        "chapter": 4
      },
      {
        "start": 86500,
        "text": "rks should keep part of their data private and\nprovide a tool for model developers to automatically evaluate models\nagainst the private hold-out data.\nPublic benchmarks will help you filter out bad models, but they won’t help\nyou find the best models for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability t",
        "chapter": 4
      },
      {
        "start": 4500,
        "text": "to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can think of criteria in the\nfollowing buckets: domain-specific capability, generation capability,\ninstruction-following capability, and cost and latency.\nImagine you ask a model to summarize a legal contract. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstand",
        "chapter": 4
      },
      {
        "start": 87000,
        "text": "opic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each ap",
        "chapter": 4
      },
      {
        "start": 43500,
        "text": "your criteria, let’s move on to the next step and use them\nto select the best model for your application.\nModel Selection\nAt the end of the day, you don’t really care about which model is the best.\nYou care about which model is the best for your applications. Once you’ve\ndefined the criteria for your application, you should evaluate models against\nthese criteria.\nDuring the application development process, as you progress through\ndifferent adaptation techniques, you’ll have to do model selection over and\nover again. For example, prompt engineering might start with the strongest\nmodel overall t",
        "chapter": 4
      },
      {
        "start": 107000,
        "text": "ations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enable you to reduce risks, discover opportunities\nto improve performance, and benchmark progresses, which will all save\nyou time and headaches down the line.\nGiven an increasing number of readily available foundation models, for\nmost application developers, the challenge is no longer in developing\nmodel",
        "chapter": 4
      },
      {
        "start": 97500,
        "text": "e\nused to evaluate a model’s perplexity for a generated text, which can be\nused for measurements such as fluency and factual consistency.\nUse automatic metrics as much as possible, but don’t be afraid to fall back\non human evaluation, even in production. Having human experts manually\nevaluate a model’s quality is a long-standing practice in AI. Given the\nchallenges of evaluating open-ended responses, many teams are looking at\nhuman evaluation as the North Star metric to guide their application\ndevelopment. Each day, you can use human experts to evaluate a subset of\nyour application’s outputs t",
        "chapter": 4
      },
      {
        "start": 78000,
        "text": "rboards with public benchmarks\nWhen evaluating models for a specific application, you’re basically creating\na private leaderboard that ranks models based on your evaluation criteria.\nThe first step is to gather a list of benchmarks that evaluate the capabilities\nimportant to your application. If you want to build a coding agent, look at\ncode-related benchmarks. If you build a writing assistant, look into creative\nwriting benchmarks. As new benchmarks are constantly introduced and old\nbenchmarks become saturated, you should look for the latest benchmarks.\nMake sure to evaluate how reliable a be",
        "chapter": 4
      }
    ],
    "input_tokens": 1542,
    "output_tokens": 358,
    "input_cost": 0.0002313,
    "output_cost": 0.0002148,
    "total_cost": 0.0004461,
    "reference_answer": "The chapter outlines an iterative evaluation workflow that helps teams determine whether to use model APIs or self-hosted models based on performance needs."
  },
  {
    "question": "what are essential functionalities for AI models?",
    "answer": "Essential functionalities for AI models, as discussed in the context of the \"AI Engineering\" book by Chip Huyen, typically include:\n\n1. **Domain-Specific Capability**: The ability of a model to perform tasks relevant to its specific domain, evaluated through metrics suitable for that domain.\n\n2. **Generation Capability**: The model's proficiency in generating outputs, such as texts, summaries, or code, assessed based on fluency, coherence, and factual consistency.\n\n3. **Instruction-Following Capability**: How well the model can understand and act upon specific instructions or queries, critical for applications requiring precise output.\n\n4. **Evaluation Metrics**: Reliable evaluation pipelines to measure the model's performance against predefined criteria for different applications, which can include functional correctness, efficiency, and cost-effectiveness.\n\n5. **Cost and Latency**: Balancing the overall cost and processing speed of the model, ensuring that high-quality models are also practical in real-world applications.\n\nThese functionalities are vital for ensuring that AI models are effective, efficient, and applicable to their intended use cases.",
    "context": [
      {
        "start": 4500,
        "text": "to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can think of criteria in the\nfollowing buckets: domain-specific capability, generation capability,\ninstruction-following capability, and cost and latency.\nImagine you ask a model to summarize a legal contract. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstand",
        "chapter": 4
      },
      {
        "start": 6500,
        "text": "tion, code debugging, grade school math,\nscience knowledge, common sense, reasoning, legal knowledge, tool use,\ngame playing, etc. The list goes on.\nDomain-specific capabilities are commonly evaluated using exact\nevaluation. Coding-related capabilities are typically evaluated using\nfunctional correctness, as discussed in Chapter 3. While functional\ncorrectness is important, it might not be the only aspect that you care about.\nYou might also care about efficiency and cost. For example, would you\nwant a car that runs but consumes an excessive amount of fuel? Similarly, if\nan SQL query generated ",
        "chapter": 4
      },
      {
        "start": 0,
        "text": "Chapter 4. Evaluate AI Systems\nA model is only useful if it works for its intended purposes. You need to\nevaluate models in the context of your application. Chapter 3 discusses\ndifferent approaches to automatic evaluation. This chapter discusses how to\nuse these approaches to evaluate models for your applications.\nThis chapter contains three parts. It starts with a discussion of the criteria\nyou might use to evaluate your applications and how these criteria are\ndefined and calculated. For example, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-sp",
        "chapter": 4
      },
      {
        "start": 11000,
        "text": "ting knowledge (“does the model know that Paris is the capital of\nFrance?”) and reasoning (“can the model infer from a table of business\nexpenses which department is spending the most?”). They aren’t ideal for\nevaluating generation capabilities such as summarization, translation, and\nessay writing. Let’s discuss how generation capabilities can be evaluated in\nthe next section.\nGeneration Capability\nAI was used to generate open-ended outputs long before generative AI\nbecame a thing. For decades, the brightest minds in NLP (natural language\nprocessing) have been working on how to evaluate the qu",
        "chapter": 4
      },
      {
        "start": 3500,
        "text": "ecommender systems are common because their successes can be\n1\nevaluated by an increase in engagement or purchase-through rates.\nThe success of a fraud detection system can be measured by how much\nmoney is saved from prevented frauds.\nCoding is a common generative AI use case because, unlike other\ngeneration tasks, generated code can be evaluated using functional\ncorrectness.\nEven though foundation models are open-ended, many of their use cases\nare close-ended, such as intent classification, sentiment analysis, next-\naction prediction, etc. It’s much easier to evaluate classification tasks\ntha",
        "chapter": 4
      },
      {
        "start": 107000,
        "text": "ations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enable you to reduce risks, discover opportunities\nto improve performance, and benchmark progresses, which will all save\nyou time and headaches down the line.\nGiven an increasing number of readily available foundation models, for\nmost application developers, the challenge is no longer in developing\nmodel",
        "chapter": 4
      },
      {
        "start": 13000,
        "text": "eneration capabilities have improved, AI-generated texts have\nbecome nearly indistinguishable from human-generated texts. Fluency and\n2\ncoherence become less important. However, these metrics can still be\nuseful for weaker models or for applications involving creative writing and\nlow-resource languages. Fluency and coherence can be evaluated using AI\nas a judge—asking an AI model how fluent and coherent a text is—or using\nperplexity, as discussed in Chapter 3.\nGenerative models, with their new capabilities and new use cases, have new\nissues that require new metrics to track. The most pressing ",
        "chapter": 4
      },
      {
        "start": 4000,
        "text": "timent analysis, next-\naction prediction, etc. It’s much easier to evaluate classification tasks\nthan open-ended tasks.\nWhile the evaluation-driven development approach makes sense from a\nbusiness perspective, focusing only on applications whose outcomes can be\nmeasured is similar to looking for the lost key under the lamppost (at night).\nIt’s easier to do, but it doesn’t mean we’ll find the key. We might be missing\nout on many potentially game-changing applications because there is no\neasy way to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto ",
        "chapter": 4
      },
      {
        "start": 38000,
        "text": "e role, you\nmight be able to create heuristics to evaluate the model’s outputs. For\nexample, if the role is someone who doesn’t talk a lot, a heuristic would be\nthe average of the model’s outputs. Other than that, the easiest automatic\nevaluation approach is AI as a judge. You should evaluate the roleplaying\nAI on both style and knowledge. For example, if a model is supposed to\ntalk like Jackie Chan, its outputs should capture Jackie Chan’s style and are\n8\ngenerated based on Jackie Chan’s knowledge.\nAI judges for different roles will need different prompts. To give you a\nsense of what an AI ju",
        "chapter": 4
      },
      {
        "start": 39500,
        "text": "to the role; the richer,\nthe better. (If the question contains\nreference answers, then the role−specific\nknowledge and memories are based on the\nreference answer. )\nCost and Latency\nA model that generates high-quality outputs but is too slow and expensive\nto run will not be useful. When evaluating models, it’s important to balance\nmodel quality, latency, and cost. Many companies opt for lower-quality\nmodels if they provide better cost and latency. Cost and latency\noptimization are discussed in detail in Chapter 9, so this section will be\nquick.\nOptimizing for multiple objectives is an active f",
        "chapter": 4
      }
    ],
    "input_tokens": 1595,
    "output_tokens": 217,
    "input_cost": 0.00023925,
    "output_cost": 0.0001302,
    "total_cost": 0.00036945,
    "reference_answer": "The chapter outlines key functionalities like scalability, function calling, structured outputs, and output guardrails as crucial for AI models to function effectively in real-world applications."
  },
  {
    "question": "how to rank LLMs based on role performance",
    "answer": "To rank large language models (LLMs) based on their role performance, you can follow a structured evaluation process that involves several key steps:\n\n1. **Define Evaluation Criteria**: Establish clear criteria for ranking. According to the context provided, two important criteria are:\n   - **Distinctiveness of Speaking Style**: Evaluate which model demonstrates a more pronounced and relevant speaking style in line with the role description.\n   - **Richness of Knowledge**: Assess which model outputs incorporate more knowledge and memories pertinent to the role.\n\n2. **Prompt Design**: Create specific prompts for each model that instruct them to perform the designated role. Incorporate role characteristics and expectations to guide the models effectively.\n\n3. **Use AI Judges**: Employ AI as judges to evaluate the responses based on the defined criteria. The AI judges can use similarity scores and text quality assessments to determine how well each model reflects the designated persona.\n\n4. **Aggregate Scores**: After obtaining the evaluations from AI judges, aggregate the scores. Since different criteria might yield varying score types, you’ll need a method for weighing these scores appropriately, considering the importance of each criterion in your specific context.\n\n5. **Benchmark Selection**: You may also create a private leaderboard by selecting relevant benchmarks that align with the roles being assessed. Ensure the benchmarks effectively measure the capabilities important for the specific application you are focusing on.\n\n6. **Iteration and Refinement**: Since no single evaluation method is flawless, combine various evaluation techniques to mitigate biases and limitations. Adjust the criteria and benchmarks based on preliminary results to improve the ranking process over iterations.\n\nBy implementing these steps, you will be able to effectively rank LLMs based on their ability to fulfill specific roles.",
    "context": [
      {
        "start": 71500,
        "text": " are generally thoughtful about\nhow they select benchmarks, their decision-making process isn’t always\nclear to users. Different leaderboards often end up with different\nbenchmarks, making it hard to compare and interpret their rankings. For\nexample, in late 2023, Hugging Face updated their Open LLM Leaderboard\nto use the average of six different benchmarks to rank models:\n1. ARC-C (Clark et al., 2018): Measuring the ability to solve complex,\ngrade school-level science questions.\n2. MMLU (Hendrycks et al., 2020): Measuring knowledge and reasoning\ncapabilities in 57 subjects, including elementa",
        "chapter": 4
      },
      {
        "start": 39000,
        "text": "e then output\nusing Python dictionaries and lists.\nUser Prompt:\nThe models below are to play the role of\n‘‘{role_name}’’. The role description of\n‘‘{role_name}’’ is\n‘‘{role_description_and_catchphrases}’’. I\nneed to rank the following models based on the\ntwo criteria below:\n1. Which one has more pronounced role speaking\nstyle, and speaks more in line with the role\ndescription. The more distinctive the speaking\nstyle, the better.\n2. Which one’s output contains more knowledge\nand memories related to the role; the richer,\nthe better. (If the question contains\nreference answers, then the role−spec",
        "chapter": 4
      },
      {
        "start": 38500,
        "text": "dge.\nAI judges for different roles will need different prompts. To give you a\nsense of what an AI judge’s prompt looks like, here is the beginning of the\nprompt used by the RoleLLM AI judge to rank models based on their ability\nto play a certain role. For the full prompt, please check out Wang et al.\n(2023).\nSystem Instruction:\nYou are a role−playing performance comparison\nassistant. You should rank the models based on\nthe role characteristics and text quality of\ntheir responses. The rankings are then output\nusing Python dictionaries and lists.\nUser Prompt:\nThe models below are to play the rol",
        "chapter": 4
      },
      {
        "start": 43000,
        "text": "tput token 1M tokens 1M to\nScale TPM (tokens X > 1M TPM > 1M\nper minute)\nLatency Time to first Internal user < 200ms < 100\ntoken (P90) prompt dataset\nLatency Time per total Internal user < 1m < 30s\nquery (P90) prompt dataset\nOverall model Elo score Chatbot > 1200 > 125\nquality Arena’s\nranking\nCode pass@1 HumanEval > 90% > 95%\ngeneration\ncapability\nHard\nCriteria Metric Benchmark Ideal\nrequirement\nFactual Internal GPT Internal > 0.8 > 0.9\nconsistency metric hallucination\ndataset\nNow that you have your criteria, let’s move on to the next step and use them\nto select the best model for your applica",
        "chapter": 4
      },
      {
        "start": 37500,
        "text": "ang et al., 2023) and\nCharacterEval (Tu et al., 2024). CharacterEval used human annotators and\ntrained a reward model to evaluate each roleplaying aspect on a five-point\nscale. RoleLLM evaluates a model’s ability to emulate a persona using both\ncarefully crafted similarity scores (how similar the generated outputs are to\nthe expected outputs) and AI judges.\nIf AI in your application is supposed to assume a certain role, make sure to\nevaluate whether your model stays in character. Depending on the role, you\nmight be able to create heuristics to evaluate the model’s outputs. For\nexample, if the ",
        "chapter": 4
      },
      {
        "start": 78000,
        "text": "rboards with public benchmarks\nWhen evaluating models for a specific application, you’re basically creating\na private leaderboard that ranks models based on your evaluation criteria.\nThe first step is to gather a list of benchmarks that evaluate the capabilities\nimportant to your application. If you want to build a coding agent, look at\ncode-related benchmarks. If you build a writing assistant, look into creative\nwriting benchmarks. As new benchmarks are constantly introduced and old\nbenchmarks become saturated, you should look for the latest benchmarks.\nMake sure to evaluate how reliable a be",
        "chapter": 4
      },
      {
        "start": 109000,
        "text": "reating a private leaderboard\nto rank models based on your needs.\nThis chapter ends with how to use all the evaluation techniques and criteria\ndiscussed in the last chapter and how to create an evaluation pipeline for\nyour application. No perfect evaluation method exists. It’s impossible to\ncapture the ability of a high-dimensional system using one- or few-\ndimensional scores. Evaluating modern AI systems has many limitations\nand biases. However, this doesn’t mean we shouldn’t do it. Combining\ndifferent methods and approaches can help mitigate many of these\nchallenges.\nEven though dedicated di",
        "chapter": 4
      },
      {
        "start": 80000,
        "text": "spent approximately $80,000–$100,000\n26\nto evaluate 30 models on their full HELM suite. The more models you\nwant to evaluate and the more benchmarks you want to use, the more\nexpensive it gets.\nOnce you’ve selected a set of benchmarks and obtained the scores for the\nmodels you care about on these benchmarks, you then need to aggregate\nthese scores to rank models. Not all benchmark scores are in the same unit\nor scale. One benchmark might use accuracy, another F1, and another\nBLEU score. You will need to think about how important each benchmark\nis to you and weigh their scores accordingly.\nAs y",
        "chapter": 4
      },
      {
        "start": 74500,
        "text": "hallenge of selecting benchmarks to rank\nmodels. If leaderboard developers can’t explain their benchmark selection\nprocesses, it might be because it’s really hard to do so.\nAn important aspect of benchmark selection that is often overlooked is\nbenchmark correlation. It is important because if two benchmarks are\nperfectly correlated, you don’t want both of them. Strongly correlated\n21\nbenchmarks can exaggerate biases.\nNOTE\nWhile I was writing this book, many benchmarks became saturated or close to being saturated. In\nJune 2024, less than a year after their leaderboard’s last revamp, Hugging Fac",
        "chapter": 4
      },
      {
        "start": 38000,
        "text": "e role, you\nmight be able to create heuristics to evaluate the model’s outputs. For\nexample, if the role is someone who doesn’t talk a lot, a heuristic would be\nthe average of the model’s outputs. Other than that, the easiest automatic\nevaluation approach is AI as a judge. You should evaluate the roleplaying\nAI on both style and knowledge. For example, if a model is supposed to\ntalk like Jackie Chan, its outputs should capture Jackie Chan’s style and are\n8\ngenerated based on Jackie Chan’s knowledge.\nAI judges for different roles will need different prompts. To give you a\nsense of what an AI ju",
        "chapter": 4
      }
    ],
    "input_tokens": 1765,
    "output_tokens": 344,
    "input_cost": 0.00026475,
    "output_cost": 0.0002064,
    "total_cost": 0.00047115,
    "reference_answer": "The chapter provides a methodology for assessing LLMs by creating and comparing their outputs against defined role characteristics and quality criteria, using Python for organization."
  },
  {
    "question": "best practices for collecting user feedback on AI performance",
    "answer": "To effectively collect user feedback on AI performance, here are some best practices derived from principles in the \"AI Engineering\" book by Chip Huyen:\n\n1. **Establish Evaluation Criteria**: Define specific criteria that measure performance, such as relevance, factual consistency, and user engagement metrics. This helps users understand what to provide feedback on.\n\n2. **Use Diverse Feedback Types**: Incorporate multiple feedback types for a comprehensive assessment. For example, utilize metrics like daily active users, conversation initiation rates, or qualitative user inputs to gather a well-rounded view of performance.\n\n3. **Engage Human Evaluators**: Regularly use human experts to evaluate a subset of outputs. For instance, LinkedIn’s practice of manually assessing 500 daily conversations can help detect performance changes or unusual usage patterns.\n\n4. **Monitor During Production**: Implement monitoring of user interactions not only during experimentation but also in production. This allows for ongoing assessment even when reference data may not be available.\n\n5. **User-Centric Feedback Collection**: Encourage users to provide input on what they consider unacceptable outputs. Clearly define the boundaries of acceptable behavior for the AI to help steer user feedback in productive directions.\n\n6. **Slice Data for Insights**: Analyze feedback by segmenting data to gain deeper insights into specific areas of performance. This can help identify particular strengths or weaknesses within the AI system.\n\n7. **Iterative Feedback and Improvement Loop**: Use feedback to continually refine both the application and its evaluation metrics. This promotes a cycle of constant improvement based on real user experiences.\n\nBy following these practices, you can create a robust framework for collecting and utilizing user feedback to enhance AI performance effectively.",
    "context": [
      {
        "start": 107000,
        "text": "ations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enable you to reduce risks, discover opportunities\nto improve performance, and benchmark progresses, which will all save\nyou time and headaches down the line.\nGiven an increasing number of readily available foundation models, for\nmost application developers, the challenge is no longer in developing\nmodel",
        "chapter": 4
      },
      {
        "start": 92000,
        "text": "good response should explain the gap between\nthis job’s requirements and the candidate’s background, and what the\ncandidate can do to close this gap.\nBefore building your application, think about what makes a good response.\nLangChain’s State of AI 2023 found that, on average, their users used 2.3\ndifferent types of feedback (criteria) to evaluate an application. For\nexample, for a customer support application, a good response might be\ndefined using three criteria:\n1. Relevance: the response is relevant to the user’s query.\n2. Factual consistency: the response is factually consistent with the c",
        "chapter": 4
      },
      {
        "start": 86500,
        "text": "rks should keep part of their data private and\nprovide a tool for model developers to automatically evaluate models\nagainst the private hold-out data.\nPublic benchmarks will help you filter out bad models, but they won’t help\nyou find the best models for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability t",
        "chapter": 4
      },
      {
        "start": 95000,
        "text": ": what scores must an\napplication achieve for it to be useful? For example, you might determine\nthat your chatbot’s factual consistency score must be at least 50% for it to\nbe useful. Anything below this makes it unusable even for general customer\nrequests.\nBefore developing AI evaluation metrics, it’s crucial to first understand the\nbusiness metrics you’re targeting. Many applications focus on stickiness\nmetrics, such as daily, weekly, or monthly active users (DAU, WAU,\nMAU). Others prioritize engagement metrics, like the number of\nconversations a user initiates per month or the duration of e",
        "chapter": 4
      },
      {
        "start": 87000,
        "text": "opic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each ap",
        "chapter": 4
      },
      {
        "start": 98000,
        "text": "evelopment. Each day, you can use human experts to evaluate a subset of\nyour application’s outputs that day to detect any changes in the application’s\nperformance or unusual patterns in usage. For example, LinkedIn developed\na process to manually evaluate up to 500 daily conservations with their AI\nsystems.\nConsider evaluation methods to be used not just during experimentation but\nalso during production. During experimentation, you might have reference\ndata to compare your application’s outputs to, whereas, in production,\nreference data might not be immediately available. However, in productio",
        "chapter": 4
      },
      {
        "start": 47000,
        "text": "host open source models. However, after public\nand private evaluation, you might realize that open source models can’t\nachieve the level of performance you want and have to switch to\ncommercial APIs.\nChapter 10 discusses monitoring and collecting user feedback. The rest of\nthis chapter will discuss the first three steps. First, let’s discuss a question\nthat most teams will visit more than once: to use model APIs or to host\nmodels themselves. We’ll then continue to how to navigate the dizzying\nnumber of public benchmarks and why you can’t trust them. This will set\nthe stage for the last section",
        "chapter": 4
      },
      {
        "start": 500,
        "text": "e, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-specific\ncapabilities like math, science, reasoning, and summarization measured?\nThe second part focuses on model selection. Given an increasing number of\nfoundation models to choose from, it can feel overwhelming to choose the\nright model for your application. Thousands of benchmarks have been\nintroduced to evaluate these models along different criteria. Can these\nbenchmarks be trusted? How do you select what benchmarks to use? How\nabout public leaderboards that aggregate multiple benchmarks?\nThe",
        "chapter": 4
      },
      {
        "start": 91000,
        "text": "bad responses look like, you\nwon’t be able to catch them.\nWhen creating the evaluation guideline, it’s important to define not only\nwhat the application should do, but also what it shouldn’t do. For example,\nif you build a customer support chatbot, should this chatbot answer\nquestions unrelated to your product, such as about an upcoming election? If\nnot, you need to define what inputs are out of the scope of your application,\nhow to detect them, and how your application should respond to them.\nDefine evaluation criteria\nOften, the hardest part of evaluation isn’t determining whether an output ",
        "chapter": 4
      },
      {
        "start": 99000,
        "text": "’s components and each\ncriterion, for both turn-based and task-based evaluation. Use actual\nproduction data if possible. If your application has natural labels that you\ncan use, that’s great. If not, you can use either humans or AI to label your\ndata. Chapter 8 discusses AI-generated data. The success of this phase also\ndepends on the clarity of the scoring rubric. The annotation guideline\ncreated for evaluation can be reused to create instruction data for finetuning\nlater, if you choose to finetune.\nSlice your data to gain a finer-grained understanding of your system.\nSlicing means separating",
        "chapter": 4
      }
    ],
    "input_tokens": 1599,
    "output_tokens": 335,
    "input_cost": 0.00023985,
    "output_cost": 0.000201,
    "total_cost": 0.00044085,
    "reference_answer": "The text suggests considering user feedback as an essential evaluation method, correlating it with other metrics, and utilizing it actively during production for continuous improvement."
  },
  {
    "question": "NLG evaluation metrics examples",
    "answer": "In the context of NLG (Natural Language Generation), several evaluation metrics have been highlighted for assessing the quality of generated texts. Examples of these metrics include:\n\n1. **Fluency**: This metric evaluates whether the generated text is grammatically correct and sounds natural, akin to something a fluent speaker would write.\n\n2. **Coherence**: This measures how well-structured and logically organized the entire text is.\n\n3. **Faithfulness**: Specifically useful for translation tasks, this metric assesses how accurately the generated text reflects the original content.\n\n4. **Relevance**: This is used primarily in summarization tasks and checks if the summary captures the most important aspects of the original document.\n\n5. **Factual Consistency**: This metric evaluates whether the generated information aligns with factual data and whether it holds across various contexts.\n\nThese metrics can be complemented by human evaluations, especially for more nuanced assessments in open-ended generation tasks.",
    "context": [
      {
        "start": 11500,
        "text": "the brightest minds in NLP (natural language\nprocessing) have been working on how to evaluate the quality of open-\nended outputs. The subfield that studies open-ended text generation is\ncalled NLG (natural language generation). NLG tasks in the early 2010s\nincluded translation, summarization, and paraphrasing.\nMetrics used to evaluate the quality of generated texts back then included\nfluency and coherence. Fluency measures whether the text is grammatically\ncorrect and natural-sounding (does this sound like something written by a\nfluent speaker?). Coherence measures how well-structured the whol",
        "chapter": 4
      },
      {
        "start": 97500,
        "text": "e\nused to evaluate a model’s perplexity for a generated text, which can be\nused for measurements such as fluency and factual consistency.\nUse automatic metrics as much as possible, but don’t be afraid to fall back\non human evaluation, even in production. Having human experts manually\nevaluate a model’s quality is a long-standing practice in AI. Given the\nchallenges of evaluating open-ended responses, many teams are looking at\nhuman evaluation as the North Star metric to guide their application\ndevelopment. Each day, you can use human experts to evaluate a subset of\nyour application’s outputs t",
        "chapter": 4
      },
      {
        "start": 12000,
        "text": " sound like something written by a\nfluent speaker?). Coherence measures how well-structured the whole text is\n(does it follow a logical structure?). Each task might also have its own\nmetrics. For example, a metric a translation task might use is faithfulness:\nhow faithful is the generated translation to the original sentence? A metric\nthat a summarization task might use is relevance: does the summary focus\non the most important aspects of the source document? (Li et al., 2022).\nSome early NLG metrics, including faithfulness and relevance, have been\nrepurposed, with significant modifications, t",
        "chapter": 4
      },
      {
        "start": 96500,
        "text": "ponse and the whole context. An unambiguous scoring\nrubric and examples will be critical for specialized scorers and AI judges to\nsucceed.\nIt’s possible to mix and match evaluation methods for the same criteria. For\nexample, you might have a cheap classifier that gives low-quality signals on\n100% of your data, and an expensive AI judge to give high-quality signals\non 1% of the data. This gives you a certain level of confidence in your\napplication while keeping costs manageable.\nWhen logprobs are available, use them. Logprobs can be used to measure\nhow confident a model is about a generated tok",
        "chapter": 4
      },
      {
        "start": 5000,
        "text": "ct. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstanding legal contracts. Generation capability metrics measure how\ncoherent or faithful the summary is. Instruction-following capability\ndetermines whether the summary is in the requested format, such as\nmeeting your length constraints. Cost and latency metrics tell you how\nmuch this summary will cost you and how long you will have to wait for it.\nThe last chapter started with an evaluation approach and discussed what\ncriteria a given approach can evaluate. This section takes a different angle:\ngive",
        "chapter": 4
      },
      {
        "start": 94500,
        "text": "hing that looks like this:\nFactual consistency of 80%: we can automate 30% of customer support\nrequests.\nFactual consistency of 90%: we can automate 50%.\nFactual consistency of 98%: we can automate 90%.\nUnderstanding the impact of evaluation metrics on business metrics is\nhelpful for planning. If you know how much gain you can get from\nimproving a certain metric, you might have more confidence to invest\nresources into improving that metric.\nIt’s also helpful to determine the usefulness threshold: what scores must an\napplication achieve for it to be useful? For example, you might determine\nthat",
        "chapter": 4
      },
      {
        "start": 105500,
        "text": "on\npipeline. Be consistent with the configurations of your evaluation.\nFor example, if you use an AI judge, make sure to set your judge’s\ntemperature to 0.\nHow correlated are your metrics?\nAs discussed in “Benchmark selection and aggregation”, if two\nmetrics are perfectly correlated, you don’t need both of them. On the\nother hand, if two metrics are not at all correlated, this means either\nan interesting insight into your model or that your metrics just aren’t\n29\ntrustworthy.\nHow much cost and latency does your evaluation pipeline add to your\napplication?\nEvaluation, if not done carefully, can",
        "chapter": 4
      },
      {
        "start": 93500,
        "text": " deserve a 1? Validate\nyour rubric with humans: yourself, coworkers, friends, etc. If humans find it\nhard to follow the rubric, you need to refine it to make it unambiguous. This\nprocess can require a lot of back and forth, but it’s necessary. A clear\nguideline is the backbone of a reliable evaluation pipeline. This guideline\ncan also be reused later for training data annotation, as discussed in\nChapter 8.\nTie evaluation metrics to business metrics\nWithin a business, an application must serve a business goal. The\napplication’s metrics must be considered in the context of the business\nproblem i",
        "chapter": 4
      },
      {
        "start": 18500,
        "text": "ing for things that don’t exist. For example, if I ask the model “What did X say about\nY?” the model is more likely to hallucinate if X has never said anything about Y than if X has.\nLet’s assume for now that you already have the context to evaluate an\noutput against—this context was either provided by users or retrieved by\nyou (context retrieval is discussed in Chapter 6). The most straightforward\nevaluation approach is AI as a judge. As discussed in Chapter 3, AI judges\ncan be asked to evaluate anything, including factual consistency. Both Liu\net al. (2023) and Luo et al. (2023) showed that ",
        "chapter": 4
      },
      {
        "start": 13000,
        "text": "eneration capabilities have improved, AI-generated texts have\nbecome nearly indistinguishable from human-generated texts. Fluency and\n2\ncoherence become less important. However, these metrics can still be\nuseful for weaker models or for applications involving creative writing and\nlow-resource languages. Fluency and coherence can be evaluated using AI\nas a judge—asking an AI model how fluent and coherent a text is—or using\nperplexity, as discussed in Chapter 3.\nGenerative models, with their new capabilities and new use cases, have new\nissues that require new metrics to track. The most pressing ",
        "chapter": 4
      }
    ],
    "input_tokens": 1638,
    "output_tokens": 189,
    "input_cost": 0.0002457,
    "output_cost": 0.0001134,
    "total_cost": 0.0003591,
    "reference_answer": "It details various metrics from the NLG field, emphasizing fluency, coherence, and specific metrics tailored for tasks like translation and summarization."
  },
  {
    "question": "open source vs commercial AI models",
    "answer": "The discussion around open source versus commercial AI models involves several key factors, including control, customizability, ease of use, community support, and performance.\n\n### Open Source Models:\n1. **Control and Customizability**: Open source models offer enterprises greater control over the model's architecture and functionality. This allows for custom modifications to cater to specific business needs.\n2. **Community Support**: A strong community can facilitate troubleshooting and the sharing of solutions, which can be beneficial over time.\n3. **Transparency**: Open source models typically provide more transparency regarding their development and performance, although the specifics of training data might not always be disclosed to avoid scrutiny.\n4. **Licensing**: These models often have various licenses (e.g., MIT, Apache 2.0) which govern their use and distribution.\n5. **Performance**: Though open source models are improving, there is a perception that they may lag behind proprietary models in terms of performance due to lack of funding and user feedback for continuous improvement.\n\n### Commercial Models:\n1. **User-Friendliness**: Generally, commercial models are designed to be easier to use and integrate, making them accessible for scaling quickly.\n2. **Support and Maintenance**: Companies provide robust support services, which can include documentation, updates, and optimizations based on user feedback.\n3. **Functionality and Scalability**: Commercial models often come with additional functionalities designed to handle demanding applications effectively and efficiently.\n4. **Access and Licensing**: These models are usually accessible only via APIs, requiring licensing, which can limit flexibility compared to open source alternatives.\n5. **Performance Incentives**: Model developers may choose to keep their best-performing models proprietary to maintain a competitive edge.\n\nIn choosing between open source and commercial models, factors such as data privacy, cost, control, and the specific use cases are crucial considerations. The landscape of AI is dynamic, and businesses must weigh these aspects based on their evolving requirements.",
    "context": [
      {
        "start": 48000,
        "text": "n open source model yourself. The answer to this\nquestion can significantly reduce your candidate model pool.\nLet’s first go into what exactly open source means when it comes to\nmodels, then discuss the pros and cons of these two approaches.\nOpen source, open weight, and model licenses\nThe term “open source model” has become contentious. Originally, open\nsource was used to refer to any model that people can download and use.\nFor many use cases, being able to download the model is sufficient.\nHowever, some people argue that since a model’s performance is largely a\nfunction of what data it was t",
        "chapter": 4
      },
      {
        "start": 63500,
        "text": "rs mimic OpenAI’s\nAPI.\nYou might also prefer models with good community support. The more\ncapabilities a model has, the more quirks it has. A model with a large\ncommunity of users means that any issue you encounter may already have\n19\nbeen experienced by others, who might have shared solutions online.\nControl, access, and transparency\nA 2024 study by a16z shows two key reasons that enterprises care about\nopen source models are control and customizability, as shown in Figure 4-8.\nFigure 4-8. Why enterprises care about open source models. Image from the 2024 study by a16z.\nIf your business depen",
        "chapter": 4
      },
      {
        "start": 63000,
        "text": "hat.\nIn general, you want a model that is easy to use and manipulate. Typically,\nproprietary models are easier to get started with and scale, but open models\nmight be easier to manipulate as their components are more accessible.\nRegardless of whether you go with open or proprietary models, you want\nthis model to follow a standard API, which makes it easier to swap models.\nMany model developers try to make their models mimic the API of the most\npopular models. As of this writing, many API providers mimic OpenAI’s\nAPI.\nYou might also prefer models with good community support. The more\ncapabiliti",
        "chapter": 4
      },
      {
        "start": 114000,
        "text": "rom model developers’ perspective, why invest\nmillions, if not billions, into building models just for others to make money?It might be argued that\nMeta supports open source models only to keep their competitors (Google, Microsoft/OpenAI) in\ncheck. Both Mistral and Cohere have open source models, but they also have APIs. At some point,\ninference services on top of Mistral and Cohere models become their competitors.There’s the\nargument that open source is better for society, and maybe that’s enough as an incentive. People who\nwant what’s good for society will continue to push for open source, a",
        "chapter": 4
      },
      {
        "start": 54500,
        "text": "providers like AWS, Azure, and GCP all\nprovide API access to popular open source models. A plethora of startups\nare doing the same.\nNOTE\nThere are also commercial API providers that can deploy their services within your private networks.\nIn this discussion, I treat these privately deployed commercial APIs similarly to self-hosted models.\nThe answer to whether to host a model yourself or use a model API depends\non the use case. And the same use case can change over time. Here are\nseven axes to consider: data privacy, data lineage, performance,\nfunctionality, costs, control, and on-device deploy",
        "chapter": 4
      },
      {
        "start": 49500,
        "text": " public,\nregardless of their training data’s availability and licenses.\nAs of this writing, the vast majority of open source models are open weight\nonly. Model developers might hide training data information on purpose, as\nthis information can open model developers to public scrutiny and potential\nlawsuits.\nAnother important attribute of open source models is their licenses. Before\nfoundation models, the open source world was confusing enough, with so\nmany different licenses, such as MIT (Massachusetts Institute of\nTechnology), Apache 2.0, GNU General Public License (GPL), BSD\n(Berkely Softwar",
        "chapter": 4
      },
      {
        "start": 113500,
        "text": "nts for auditable information of models and\ntraining data may increase. Commercial models may be able to provide certifications, saving\ncompanies from the effort.\n7\nUsers want models to be open source because open means more information and more options, but\nwhat’s in it for model developers? Many companies have sprung up to capitalize on open source\nmodels by providing inference and finetuning services. It’s not a bad thing. Many people need these\nservices to leverage open source models. But, from model developers’ perspective, why invest\nmillions, if not billions, into building models just f",
        "chapter": 4
      },
      {
        "start": 60000,
        "text": "other reason that might cause open source models to lag behind is that\nopen source developers don’t receive feedback from users to improve their\nmodels, the way commercial models do. Once a model is open sourced,\nmodel developers have no idea how the model is being used, and how well\nthe model works in the wild.\nFunctionality\nMany functionalities are needed around a model to make it work for a use\ncase. Here are some examples of these functionalities:\nScalability: making sure the inference service can support your\napplication’s traffic while maintaining the desirable latency and cost.\nFunction",
        "chapter": 4
      },
      {
        "start": 59000,
        "text": " decreasing on\nthe MMLU benchmark over time. This trend has made many people believe\nthat one day, there will be an open source model that performs just as well,\nif not better, than the strongest proprietary model.\nAs much as I want open source models to catch up with proprietary models,\nI don’t think the incentives are set up for it. If you have the strongest model\navailable, would you rather open source it for other people to capitalize on\n17\nit, or would you try to capitalize on it yourself? It’s a common practice for\ncompanies to keep their strongest models behind APIs and open source thei",
        "chapter": 4
      },
      {
        "start": 53500,
        "text": "gh both OpenAI and Azure APIs. There might be slight\ndifferences in the performance of the same model provided through\ndifferent APIs, as different APIs might use different techniques to optimize\nthis model, so make sure to run thorough tests when you switch between\nmodel APIs.\nCommercial models are only accessible via APIs licensed by the model\n13\ndevelopers. Open source models can be supported by any API provider,\nallowing you to pick and choose the provider that works best for you. For\ncommercial model providers, models are their competitive advantages. For\nAPI providers that don’t have the",
        "chapter": 4
      }
    ],
    "input_tokens": 1614,
    "output_tokens": 397,
    "input_cost": 0.0002421,
    "output_cost": 0.0002382,
    "total_cost": 0.0004803,
    "reference_answer": "The text outlines the differences between open source models and commercial models, noting that many developers release weaker models openly while keeping stronger models behind paywalls."
  },
  {
    "question": "evaluating AI with multiple-choice questions",
    "answer": "Evaluating AI systems using multiple-choice questions can be effective, particularly for assessing specific capabilities in a controlled environment. This approach simplifies the evaluation process by providing clear options to choose from, making it easier to identify correct and incorrect responses.\n\nFor instance, as detailed in the context, benchmarks such as UC Berkeley's MMLU and AGIEval utilize multiple-choice questions to systematically assess AI performance. The structured nature of these questions helps avoid inconsistencies that may arise from open-ended tasks. An example from the MMLU benchmark illustrates how a multiple-choice question can evaluate a model's understanding of concepts, like government regulations on monopolies.\n\nAdditionally, when designing an evaluation pipeline, it's important to define evaluation criteria specific to the application, covering aspects such as domain-specific knowledge, generation capability, and instruction-following ability. This foundational understanding can help in formulating effective multiple-choice questions that target desired competencies.\n\nIn summary, while multiple-choice questions are a useful tool for evaluating AI, they should be part of a broader evaluation strategy that may include both automatic metrics and human assessments to provide a comprehensive evaluation of AI performance.",
    "context": [
      {
        "start": 87000,
        "text": "opic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each ap",
        "chapter": 4
      },
      {
        "start": 4500,
        "text": "to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can think of criteria in the\nfollowing buckets: domain-specific capability, generation capability,\ninstruction-following capability, and cost and latency.\nImagine you ask a model to summarize a legal contract. At a high level,\ndomain-specific capability metrics tell you how good the model is at\nunderstand",
        "chapter": 4
      },
      {
        "start": 107000,
        "text": "ations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enable you to reduce risks, discover opportunities\nto improve performance, and benchmark progresses, which will all save\nyou time and headaches down the line.\nGiven an increasing number of readily available foundation models, for\nmost application developers, the challenge is no longer in developing\nmodel",
        "chapter": 4
      },
      {
        "start": 8500,
        "text": " are multiple-choice,\nincluding UC Berkeley’s MMLU (2020), Microsoft’s AGIEval (2023), and\nthe AI2 Reasoning Challenge (ARC-C) (2018). In their paper, AGIEval’s\nauthors explained that they excluded open-ended tasks on purpose to avoid\ninconsistent assessment.\nHere’s an example of a multiple-choice question in the MMLU benchmark:\nQuestion: One of the reasons that the government discourages and\nregulates monopolies is that\n(A) Producer surplus is lost and consumer surplus is gained.\n(B) Monopoly prices ensure productive efficiency but cost society\nallocative efficiency.\n(C) Monopoly firms do not",
        "chapter": 4
      },
      {
        "start": 38000,
        "text": "e role, you\nmight be able to create heuristics to evaluate the model’s outputs. For\nexample, if the role is someone who doesn’t talk a lot, a heuristic would be\nthe average of the model’s outputs. Other than that, the easiest automatic\nevaluation approach is AI as a judge. You should evaluate the roleplaying\nAI on both style and knowledge. For example, if a model is supposed to\ntalk like Jackie Chan, its outputs should capture Jackie Chan’s style and are\n8\ngenerated based on Jackie Chan’s knowledge.\nAI judges for different roles will need different prompts. To give you a\nsense of what an AI ju",
        "chapter": 4
      },
      {
        "start": 18500,
        "text": "ing for things that don’t exist. For example, if I ask the model “What did X say about\nY?” the model is more likely to hallucinate if X has never said anything about Y than if X has.\nLet’s assume for now that you already have the context to evaluate an\noutput against—this context was either provided by users or retrieved by\nyou (context retrieval is discussed in Chapter 6). The most straightforward\nevaluation approach is AI as a judge. As discussed in Chapter 3, AI judges\ncan be asked to evaluate anything, including factual consistency. Both Liu\net al. (2023) and Luo et al. (2023) showed that ",
        "chapter": 4
      },
      {
        "start": 0,
        "text": "Chapter 4. Evaluate AI Systems\nA model is only useful if it works for its intended purposes. You need to\nevaluate models in the context of your application. Chapter 3 discusses\ndifferent approaches to automatic evaluation. This chapter discusses how to\nuse these approaches to evaluate models for your applications.\nThis chapter contains three parts. It starts with a discussion of the criteria\nyou might use to evaluate your applications and how these criteria are\ndefined and calculated. For example, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-sp",
        "chapter": 4
      },
      {
        "start": 1500,
        "text": "nt of your application over time. This part brings together the\ntechniques we’ve learned throughout the book to evaluate concrete\napplications.\nEvaluation Criteria\nWhich is worse—an application that has never been deployed or an\napplication that is deployed but no one knows whether it’s working? When I\nasked this question at conferences, most people said the latter. An\napplication that is deployed but can’t be evaluated is worse. It costs to\nmaintain, but if you want to take it down, it might cost even more.\nAI applications with questionable returns on investment are, unfortunately,\nquite comm",
        "chapter": 4
      },
      {
        "start": 91500,
        "text": "efine evaluation criteria\nOften, the hardest part of evaluation isn’t determining whether an output is\ngood, but rather what good means. In retrospect of one year of deploying\ngenerative AI applications, LinkedIn shared that the first hurdle was in\ncreating an evaluation guideline. A correct response is not always a good\nresponse. For example, for their AI-powered Job Assessment application,\nthe response “You are a terrible fit” might be correct but not helpful, thus\nmaking it a bad response. A good response should explain the gap between\nthis job’s requirements and the candidate’s background,",
        "chapter": 4
      },
      {
        "start": 97500,
        "text": "e\nused to evaluate a model’s perplexity for a generated text, which can be\nused for measurements such as fluency and factual consistency.\nUse automatic metrics as much as possible, but don’t be afraid to fall back\non human evaluation, even in production. Having human experts manually\nevaluate a model’s quality is a long-standing practice in AI. Given the\nchallenges of evaluating open-ended responses, many teams are looking at\nhuman evaluation as the North Star metric to guide their application\ndevelopment. Each day, you can use human experts to evaluate a subset of\nyour application’s outputs t",
        "chapter": 4
      }
    ],
    "input_tokens": 1639,
    "output_tokens": 222,
    "input_cost": 0.00024585,
    "output_cost": 0.0001332,
    "total_cost": 0.00037905,
    "reference_answer": "It describes how non-coding abilities can be assessed using close-ended tasks, particularly multiple-choice questions, which simplify verification and consistency in evaluation."
  },
  {
    "question": "how to set metrics for chatbot evaluation",
    "answer": "To set metrics for chatbot evaluation, follow these steps based on the guidelines provided:\n\n1. **Understand Business Goals**: Start by identifying the business metrics you're aiming to impact with your chatbot (e.g., customer support efficiency, user engagement).\n\n2. **Define Usability Thresholds**: Determine the minimum scores that the chatbot must achieve to be considered useful. For example, establish that a factual consistency score should be at least 50% for basic functionality and higher for more complex queries.\n\n3. **Map Evaluation Metrics to Business Metrics**: Create a clear correlation between your chatbot’s evaluation metrics (like factual consistency) and the expected business outcomes. For instance, different levels of factual consistency may correlate with the percentage of customer requests the chatbot can automate.\n\n4. **Specify Evaluation Criteria**: Clearly outline what the chatbot should and shouldn’t do. Define out-of-scope inputs and how the chatbot should respond to them.\n\n5. **Use a Scoring Rubric**: Develop an unambiguous scoring rubric to evaluate the chatbot's responses, ensuring it is understandable and validated by humans.\n\n6. **Incorporate Automatic and Human Evaluations**: Utilize automatic metrics wherever possible but remain open to human evaluation, especially for complex, open-ended interactions, to assess quality effectively.\n\n7. **Iterate and Review**: Continuously refine your metrics and evaluation methods, validating them with human feedback to maintain reliability and effectiveness.\n\nBy structuring your evaluation in this way, you can effectively measure the chatbot's performance against its intended business impact.",
    "context": [
      {
        "start": 95000,
        "text": ": what scores must an\napplication achieve for it to be useful? For example, you might determine\nthat your chatbot’s factual consistency score must be at least 50% for it to\nbe useful. Anything below this makes it unusable even for general customer\nrequests.\nBefore developing AI evaluation metrics, it’s crucial to first understand the\nbusiness metrics you’re targeting. Many applications focus on stickiness\nmetrics, such as daily, weekly, or monthly active users (DAU, WAU,\nMAU). Others prioritize engagement metrics, like the number of\nconversations a user initiates per month or the duration of e",
        "chapter": 4
      },
      {
        "start": 94000,
        "text": "business goal. The\napplication’s metrics must be considered in the context of the business\nproblem it’s built to solve.\nFor example, if your customer support chatbot’s factual consistency is 80%,\nwhat does it mean for the business? For example, this level of factual\nconsistency might make the chatbot unusable for questions about billing but\ngood enough for queries about product recommendations or general\ncustomer feedback. Ideally, you want to map evaluation metrics to business\nmetrics, to something that looks like this:\nFactual consistency of 80%: we can automate 30% of customer support\nreque",
        "chapter": 4
      },
      {
        "start": 43000,
        "text": "tput token 1M tokens 1M to\nScale TPM (tokens X > 1M TPM > 1M\nper minute)\nLatency Time to first Internal user < 200ms < 100\ntoken (P90) prompt dataset\nLatency Time per total Internal user < 1m < 30s\nquery (P90) prompt dataset\nOverall model Elo score Chatbot > 1200 > 125\nquality Arena’s\nranking\nCode pass@1 HumanEval > 90% > 95%\ngeneration\ncapability\nHard\nCriteria Metric Benchmark Ideal\nrequirement\nFactual Internal GPT Internal > 0.8 > 0.9\nconsistency metric hallucination\ndataset\nNow that you have your criteria, let’s move on to the next step and use them\nto select the best model for your applica",
        "chapter": 4
      },
      {
        "start": 91000,
        "text": "bad responses look like, you\nwon’t be able to catch them.\nWhen creating the evaluation guideline, it’s important to define not only\nwhat the application should do, but also what it shouldn’t do. For example,\nif you build a customer support chatbot, should this chatbot answer\nquestions unrelated to your product, such as about an upcoming election? If\nnot, you need to define what inputs are out of the scope of your application,\nhow to detect them, and how your application should respond to them.\nDefine evaluation criteria\nOften, the hardest part of evaluation isn’t determining whether an output ",
        "chapter": 4
      },
      {
        "start": 94500,
        "text": "hing that looks like this:\nFactual consistency of 80%: we can automate 30% of customer support\nrequests.\nFactual consistency of 90%: we can automate 50%.\nFactual consistency of 98%: we can automate 90%.\nUnderstanding the impact of evaluation metrics on business metrics is\nhelpful for planning. If you know how much gain you can get from\nimproving a certain metric, you might have more confidence to invest\nresources into improving that metric.\nIt’s also helpful to determine the usefulness threshold: what scores must an\napplication achieve for it to be useful? For example, you might determine\nthat",
        "chapter": 4
      },
      {
        "start": 2500,
        "text": "t he had no idea if the\nmodel’s predictions were accurate. At the beginning of the ChatGPT fever,\ncompanies rushed to deploy customer support chatbots. Many of them are\nstill unsure if these chatbots help or hurt their user experience.\nBefore investing time, money, and resources into building an application,\nit’s important to understand how this application will be evaluated. I call\nthis approach evaluation-driven development. The name is inspired by test-\ndriven development in software engineering, which refers to the method of\nwriting tests before writing code. In AI engineering, evaluation-",
        "chapter": 4
      },
      {
        "start": 96500,
        "text": "ponse and the whole context. An unambiguous scoring\nrubric and examples will be critical for specialized scorers and AI judges to\nsucceed.\nIt’s possible to mix and match evaluation methods for the same criteria. For\nexample, you might have a cheap classifier that gives low-quality signals on\n100% of your data, and an expensive AI judge to give high-quality signals\non 1% of the data. This gives you a certain level of confidence in your\napplication while keeping costs manageable.\nWhen logprobs are available, use them. Logprobs can be used to measure\nhow confident a model is about a generated tok",
        "chapter": 4
      },
      {
        "start": 97500,
        "text": "e\nused to evaluate a model’s perplexity for a generated text, which can be\nused for measurements such as fluency and factual consistency.\nUse automatic metrics as much as possible, but don’t be afraid to fall back\non human evaluation, even in production. Having human experts manually\nevaluate a model’s quality is a long-standing practice in AI. Given the\nchallenges of evaluating open-ended responses, many teams are looking at\nhuman evaluation as the North Star metric to guide their application\ndevelopment. Each day, you can use human experts to evaluate a subset of\nyour application’s outputs t",
        "chapter": 4
      },
      {
        "start": 95500,
        "text": "engagement metrics, like the number of\nconversations a user initiates per month or the duration of each visit—the\nlonger a user stays on the app, the less likely they are to leave. Choosing\nwhich metrics to prioritize can feel like balancing profits with social\nresponsibility. While an emphasis on stickiness and engagement metrics can\nlead to higher revenues, it may also cause a product to prioritize addictive\nfeatures or extreme content, which can be detrimental to users.\nStep 3. Define Evaluation Methods and Data\nNow that you’ve developed your criteria and scoring rubrics, let’s define\nwhat ",
        "chapter": 4
      },
      {
        "start": 93500,
        "text": " deserve a 1? Validate\nyour rubric with humans: yourself, coworkers, friends, etc. If humans find it\nhard to follow the rubric, you need to refine it to make it unambiguous. This\nprocess can require a lot of back and forth, but it’s necessary. A clear\nguideline is the backbone of a reliable evaluation pipeline. This guideline\ncan also be reused later for training data annotation, as discussed in\nChapter 8.\nTie evaluation metrics to business metrics\nWithin a business, an application must serve a business goal. The\napplication’s metrics must be considered in the context of the business\nproblem i",
        "chapter": 4
      }
    ],
    "input_tokens": 1687,
    "output_tokens": 310,
    "input_cost": 0.00025305,
    "output_cost": 0.000186,
    "total_cost": 0.00043905,
    "reference_answer": "The chapter suggests determining a usefulness threshold for evaluation metrics, like a factual consistency score, to assess a chatbot's effectiveness before engaging users."
  },
  {
    "question": "how to detect data contamination in AI models",
    "answer": "To detect data contamination in AI models, you can utilize heuristics such as n-gram overlapping and perplexity:\n\n1. **N-gram Overlapping**: This technique examines sequences of tokens (e.g., phrases or words). If a sequence of tokens from the evaluation sample appears in the training data, it suggests potential contamination. For instance, if a sequence of 13 tokens from an evaluation set is also found in the training dataset, this indicates contamination.\n\n2. **Perplexity**: This measure can help determine how well a probability distribution predicts a sample. A lower perplexity indicates that the model finds the evaluation data similar to the training data, which may signify contamination.\n\nRecognizing data contamination is crucial as it can lead to inflated performance scores, misleading evaluations, and underestimating the model's generalization capabilities. Addressing contamination involves isolating affected datasets from the training process to restore integrity in model evaluation.",
    "context": [
      {
        "start": 82500,
        "text": "aluate their new models.\nData contamination can happen indirectly, such as when both evaluation\nand training data come from the same source. For example, you might\ninclude math textbooks in the training data to improve the model’s math\ncapabilities, and someone else might use questions from the same math\ntextbooks to create a benchmark to evaluate the model’s capabilities.\nData contamination can also happen intentionally for good reasons. Let’s\nsay you want to create the best possible model for your users. Initially, you\nexclude benchmark data from the model’s training data and choose the best",
        "chapter": 4
      },
      {
        "start": 83500,
        "text": "e prevalence of data contamination undermines the trustworthiness of\nevaluation benchmarks. Just because a model can achieve high performance\non bar exams doesn’t mean it’s good at giving legal advice. It could just be\nthat this model has been trained on many bar exam questions.\nTo deal with data contamination, you first need to detect the contamination,\nand then decontaminate your data. You can detect contamination using\nheuristics like n-gram overlapping and perplexity:\nN-gram overlapping\nFor example, if a sequence of 13 tokens in an evaluation sample is\nalso in the training data, the model ",
        "chapter": 4
      },
      {
        "start": 81000,
        "text": "ndle data contamination will be\nthe topic of the next section.\nData contamination with public benchmarks\nData contamination is so common that there are many different names for\nit, including data leakage, training on the test set, or simply cheating. Data\ncontamination happens when a model was trained on the same data it’s\nevaluated on. If so, it’s possible that the model just memorizes the answers\nit saw during training, causing it to achieve higher evaluation scores than it\nshould. A model that is trained on the MMLU benchmark can achieve high\nMMLU scores without being useful.\nRylan Schaeffe",
        "chapter": 4
      },
      {
        "start": 83000,
        "text": "your users. Initially, you\nexclude benchmark data from the model’s training data and choose the best\nmodel based on these benchmarks. However, because high-quality\nbenchmark data can improve the model’s performance, you then continue\ntraining your best model on benchmark data before releasing it to your\nusers. So the released model is contaminated, and your users won’t be able\nto evaluate it on contaminated benchmarks, but this might still be the right\nthing to do.\nHandling data contamination\nThe prevalence of data contamination undermines the trustworthiness of\nevaluation benchmarks. Just bec",
        "chapter": 4
      },
      {
        "start": 17500,
        "text": " light on how AI models process conflicting\ninformation and determine what the facts are. For example, Wan et al.\n(2024) found that existing “models rely heavily on the relevance of a\nwebsite to the query, while largely ignoring stylistic features that humans\nfind important such as whether a text contains scientific references or is\nwritten with a neutral tone.”\nTIP\nWhen designing metrics to measure hallucinations, it’s important to analyze the model’s outputs to\nunderstand the types of queries that it is more likely to hallucinate on. Your benchmark should focus\nmore on these queries.\nFor exa",
        "chapter": 4
      },
      {
        "start": 26500,
        "text": "elop\nmoderation tools to keep their models safe, and some of them expose their\nmoderation tools for external use.\nHarmful behaviors aren’t unique to AI outputs. They’re unfortunately\nextremely common online. Many models developed to detect toxicity in\nhuman-generated texts can be used for AI-generated texts. These\nspecialized models tend to be much smaller, faster, and cheaper than\ngeneral-purpose AI judges. Examples of these models are Facebook’s hate\nspeech detection model, the Skolkovo Institute’s toxicity classifier, and\nPerspective API. There are also many toxicity and hate speech detecti",
        "chapter": 4
      },
      {
        "start": 18500,
        "text": "ing for things that don’t exist. For example, if I ask the model “What did X say about\nY?” the model is more likely to hallucinate if X has never said anything about Y than if X has.\nLet’s assume for now that you already have the context to evaluate an\noutput against—this context was either provided by users or retrieved by\nyou (context retrieval is discussed in Chapter 6). The most straightforward\nevaluation approach is AI as a judge. As discussed in Chapter 3, AI judges\ncan be asked to evaluate anything, including factual consistency. Both Liu\net al. (2023) and Luo et al. (2023) showed that ",
        "chapter": 4
      },
      {
        "start": 86500,
        "text": "rks should keep part of their data private and\nprovide a tool for model developers to automatically evaluate models\nagainst the private hold-out data.\nPublic benchmarks will help you filter out bad models, but they won’t help\nyou find the best models for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability t",
        "chapter": 4
      },
      {
        "start": 20500,
        "text": "tuality in Large Language Models”, works by leveraging search\nengine results to verify the response. It works in four steps, as\nvisualized in Figure 4-1:\n1. Use an AI model to decompose the response into individual\nstatements.\n2. Revise each statement to make it self-contained. For example, the\n“it” in the statement “It opened in the 20th century” should be\nchanged to the original subject.\n3. For each statement, propose fact-checking queries to send to a\nGoogle Search API.\n4. Use AI to determine whether the statement is consistent with the\nresearch results.\nFigure 4-1. SAFE breaks an output in",
        "chapter": 4
      },
      {
        "start": 80500,
        "text": " need to think about how important each benchmark\nis to you and weigh their scores accordingly.\nAs you evaluate models using public benchmarks, keep in mind that the\ngoal of this process is to select a small subset of models to do more rigorous\nexperiments using your own benchmarks and metrics. This is not only\nbecause public benchmarks are unlikely to represent your application’s\nneeds perfectly, but also because they are likely contaminated. How public\nbenchmarks get contaminated and how to handle data contamination will be\nthe topic of the next section.\nData contamination with public benchm",
        "chapter": 4
      }
    ],
    "input_tokens": 1648,
    "output_tokens": 187,
    "input_cost": 0.0002472,
    "output_cost": 0.0001122,
    "total_cost": 0.0003594,
    "reference_answer": "The chapter explains that data contamination can be detected using heuristics like n-gram overlapping and perplexity, which help identify if a model has seen evaluation samples during training."
  }
]