[
  {
    "question": "importance of reliable benchmarks in AI",
    "answer": "Reliable benchmarks in AI are crucial for several reasons:\n\n1. **Evaluation as a Bottleneck**: Evaluation is identified as a significant barrier to AI adoption. The absence of reliable evaluation pipelines can hinder the development and integration of AI applications. This emphasizes the need for structured and robust benchmarks to facilitate accurate assessments.\n\n2. **Specificity to Applications**: A well-defined list of evaluation criteria tailored to specific applications is essential. Reliable benchmarks allow developers to understand what performance metrics—such as accuracy or consistency—must be reached for an AI system to be considered useful.\n\n3. **Guiding Development**: Reliable benchmarks serve as a guiding North Star for application development, helping teams make informed decisions on how to improve their models. Human evaluations are often pointed to as ideal metrics because they can effectively assess model quality, particularly for complex outputs.\n\n4. **Benchmark Selection Challenges**: The process of choosing appropriate benchmarks is complex and often overlooked. Misleading or poorly explained benchmark selections can result in ineffective evaluations, making it important to maintain transparency in how benchmarks are chosen and correlated, ensuring they are comprehensive and reflect real-world use cases.\n\n5. **Adapting to Evolving Models**: As AI capabilities improve, existing benchmarks can become saturated, necessitating the creation of new benchmarks. This adaptability is essential for maintaining relevance in model evaluation, thereby ensuring continual progress in AI technology.\n\nIn summary, reliable benchmarks are foundational to effective AI engineering, promoting transparency, guiding development, and enabling accurate evaluations that drive AI adoption.",
    "context": [
      {
        "start": 107000,
        "text": "ations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enabl",
        "chapter": 4
      },
      {
        "start": 4500,
        "text": "to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can th",
        "chapter": 4
      },
      {
        "start": 95000,
        "text": ": what scores must an\napplication achieve for it to be useful? For example, you might determine\nthat your chatbot’s factual consistency score must be at least 50% for it to\nbe useful. Anything below this makes it unusable even for general customer\nrequests.\nBefore developing AI evaluation metrics, i",
        "chapter": 4
      },
      {
        "start": 68750,
        "text": "ks designed to evaluate a model’s different\ncapabilities. Google’s BIG-bench (2022) alone has 214 benchmarks. The\nnumber of benchmarks rapidly grows to match the rapidly growing number\nof AI use cases. In addition, as AI models improve, old benchmarks\nsaturate, necessitating the introduction of new ",
        "chapter": 4
      },
      {
        "start": 70750,
        "text": "orate only a small number of benchmarks. Some\nleaderboards might exclude an important but expensive benchmark. For\nexample, HELM (Holistic Evaluation of Language Models) Lite left out an\ninformation retrieval benchmark (MS MARCO, Microsoft Machine\nReading Comprehension) because it’s expensive to run",
        "chapter": 4
      },
      {
        "start": 97750,
        "text": "ion. Having human experts manually\nevaluate a model’s quality is a long-standing practice in AI. Given the\nchallenges of evaluating open-ended responses, many teams are looking at\nhuman evaluation as the North Star metric to guide their application\ndevelopment. Each day, you can use human experts to",
        "chapter": 4
      },
      {
        "start": 74500,
        "text": "hallenge of selecting benchmarks to rank\nmodels. If leaderboard developers can’t explain their benchmark selection\nprocesses, it might be because it’s really hard to do so.\nAn important aspect of benchmark selection that is often overlooked is\nbenchmark correlation. It is important because if two be",
        "chapter": 4
      },
      {
        "start": 6250,
        "text": "u.\nTo evaluate whether a model has the necessary capabilities, you can rely on\ndomain-specific benchmarks, either public or private. Thousands of public\nbenchmarks have been introduced to evaluate seemingly endless\ncapabilities, including code generation, code debugging, grade school math,\nscience k",
        "chapter": 4
      },
      {
        "start": 85250,
        "text": "mance. Besides, there will always be benchmarks\ncreated after models are trained, so there will always be contaminated\nevaluation samples.\nFor model developers, a common practice is to remove benchmarks they\ncare about from their training data before training their models. Ideally,\nwhen reporting yo",
        "chapter": 4
      },
      {
        "start": 80500,
        "text": " need to think about how important each benchmark\nis to you and weigh their scores accordingly.\nAs you evaluate models using public benchmarks, keep in mind that the\ngoal of this process is to select a small subset of models to do more rigorous\nexperiments using your own benchmarks and metrics. This",
        "chapter": 4
      }
    ],
    "input_tokens": 911,
    "output_tokens": 302,
    "input_cost": 0.00013665,
    "output_cost": 0.0001812,
    "total_cost": 0.00031785,
    "reference_answer": "It emphasizes that many benchmarks may not accurately measure the intended metrics, stressing the need for careful evaluation."
  },
  {
    "question": "what is evaluation-driven development in ai?",
    "answer": "Evaluation-driven development in AI is an approach where the evaluation criteria for an application are defined before the actual building begins. This method is inspired by test-driven development in software engineering, which emphasizes creating tests prior to coding. \n\nIn AI, it's crucial to establish how an application will be assessed, as reliable evaluation is often viewed as the biggest bottleneck to AI adoption. Starting with a clear list of evaluation criteria specific to the application helps ensure that the development process is aligned with measurable outcomes. \n\nThe approach involves designing a robust evaluation pipeline to differentiate good outcomes from bad ones. This pipeline can encompass various levels of evaluation, and the reliability and efficiency of the evaluation process are critical for successfully deploying AI applications.",
    "context": [
      {
        "start": 3000,
        "text": "ing, which refers to the method of\nwriting tests before writing code. In AI engineering, evaluation-driven\ndevelopment means defining evaluation criteria before building.\nEVALUATION-DRIVEN DEVELOPMENT\nWhile some companies chase the latest hype, sensible business decisions\nare still being made based ",
        "chapter": 4
      },
      {
        "start": 2750,
        "text": "ng time, money, and resources into building an application,\nit’s important to understand how this application will be evaluated. I call\nthis approach evaluation-driven development. The name is inspired by test-\ndriven development in software engineering, which refers to the method of\nwriting tests b",
        "chapter": 4
      },
      {
        "start": 4500,
        "text": "to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can th",
        "chapter": 4
      },
      {
        "start": 107000,
        "text": "ations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enabl",
        "chapter": 4
      },
      {
        "start": 4000,
        "text": "timent analysis, next-\naction prediction, etc. It’s much easier to evaluate classification tasks\nthan open-ended tasks.\nWhile the evaluation-driven development approach makes sense from a\nbusiness perspective, focusing only on applications whose outcomes can be\nmeasured is similar to looking for the",
        "chapter": 4
      },
      {
        "start": 87000,
        "text": "opic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confus",
        "chapter": 4
      },
      {
        "start": 87500,
        "text": "process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each application might consist of\nmany components, and a task might be completed after many turns.\nEvaluation can happen at different levels: per task, per turn, and per\nintermediate output.\nYou should evalu",
        "chapter": 4
      },
      {
        "start": 104750,
        "text": "s reliability and finding ways to make your evaluation pipeline\nmore efficient. Reliability is especially important with subjective evaluation\nmethods such as AI as a judge.\nHere are some questions you should be asking about the quality of your\nevaluation pipeline:\nIs your evaluation pipeline gettin",
        "chapter": 4
      },
      {
        "start": 98000,
        "text": "evelopment. Each day, you can use human experts to evaluate a subset of\nyour application’s outputs that day to detect any changes in the application’s\nperformance or unusual patterns in usage. For example, LinkedIn developed\na process to manually evaluate up to 500 daily conservations with their AI\n",
        "chapter": 4
      },
      {
        "start": 106750,
        "text": ".\nAs you iterate on your evaluation pipeline, make sure to do proper\nexperiment tracking: log all variables that could change in an evaluation\nprocess, including but not limited to the evaluation data, the rubric, and the\nprompt and sampling configurations used for the AI judges.\nSummary\nThis is one",
        "chapter": 4
      }
    ],
    "input_tokens": 894,
    "output_tokens": 141,
    "input_cost": 0.0001341,
    "output_cost": 8.46e-05,
    "total_cost": 0.0002187,
    "reference_answer": "Evaluation-driven development refers to the approach of establishing evaluation criteria before building an AI application, similar to how test-driven development focuses on writing tests before code."
  },
  {
    "question": "performance differences in model APIs",
    "answer": "When evaluating performance differences in model APIs, it's important to recognize that varying providers may implement different optimization techniques for the same model, leading to slight performance variations. This indicates that thorough testing is crucial when switching between APIs to ensure that the performance meets your requirements.\n\nSeveral key factors should be considered regarding performance:\n\n1. **Latency**: Users typically prefer lower latency, although high latency may be tolerable depending on the context and use case. Thus, distinguishing between essential and non-essential latency can guide decisions.\n\n2. **Scalability**: For model APIs, the cost per token generally doesn't change much as usage scales. In contrast, self-hosted models may offer decreased costs per token at scale, which could influence the choice based on expected usage patterns.\n\n3. **Service Reliability**: When opting for a third-party API, the reliability and support through service-level agreements (SLAs) also factor into perceived performance. Unreliable APIs can necessitate additional engineering efforts to build safeguards.\n\n4. **Optimization Strategies**: Different providers often mimic popular APIs, like OpenAI’s, to standardize interactions, which can also influence the performance across different models.\n\nUltimately, the decision between using a model API or hosting your own model involves weighing these performance considerations alongside factors like data privacy, data lineage, functionality, control, and cost.",
    "context": [
      {
        "start": 53500,
        "text": "gh both OpenAI and Azure APIs. There might be slight\ndifferences in the performance of the same model provided through\ndifferent APIs, as different APIs might use different techniques to optimize\nthis model, so make sure to run thorough tests when you switch between\nmodel APIs.\nCommercial models are",
        "chapter": 4
      },
      {
        "start": 54750,
        "text": "ussion, I treat these privately deployed commercial APIs similarly to self-hosted models.\nThe answer to whether to host a model yourself or use a model API depends\non the use case. And the same use case can change over time. Here are\nseven axes to consider: data privacy, data lineage, performance,\nf",
        "chapter": 4
      },
      {
        "start": 41250,
        "text": "atency, it’s important to differentiate between the must-have and\nthe nice-to-have. If you ask users if they want lower latency, nobody will ever say no. But high\nlatency is often an annoyance, not a deal breaker.\nIf you use model APIs, they typically charge by tokens. The more input and\noutput toke",
        "chapter": 4
      },
      {
        "start": 63250,
        "text": " you go with open or proprietary models, you want\nthis model to follow a standard API, which makes it easier to swap models.\nMany model developers try to make their models mimic the API of the most\npopular models. As of this writing, many API providers mimic OpenAI’s\nAPI.\nYou might also prefer model",
        "chapter": 4
      },
      {
        "start": 42000,
        "text": "hese memory\nconfigurations. It’s not a coincidence that many models today have 7 billion\nor 65 billion parameters.\nIf you use model APIs, your cost per token usually doesn’t change much as\nyou scale. However, if you host your own models, your cost per token can\nget much cheaper as you scale. If you’",
        "chapter": 4
      },
      {
        "start": 108000,
        "text": "ng fluency, coherence, and\nfaithfulness.\nTo help answer the question of whether to host a model or to use a model\nAPI, this chapter outlined the pros and cons of each approach along seven\naxes, including data privacy, data lineage, performance, functionality,\ncontrol, and cost. This decision, like a",
        "chapter": 4
      },
      {
        "start": 42500,
        "text": "ferent scales, companies need to reevaluate whether it\nmakes more sense to use model APIs or to host their own models.\nTable 4-3 shows criteria you might use to evaluate models for your\napplication. The row scale is especially important when evaluating model\nAPIs, because you need a model API servic",
        "chapter": 4
      },
      {
        "start": 54000,
        "text": "cial model providers, models are their competitive advantages. For\nAPI providers that don’t have their own models, APIs are their competitive\nadvantages. This means API providers might be more motivated to provide\nbetter APIs with better pricing.\nSince building scalable inference services for larger",
        "chapter": 4
      },
      {
        "start": 44500,
        "text": "els along the cost–performance axes and choosing the\nmodel that gives the best performance for your bucks\nHowever, the actual selection process is a lot more nuanced. Let’s explore\nwhat it looks like.\nModel Selection Workflow\nWhen looking at models, it’s important to differentiate between hard\nattri",
        "chapter": 4
      },
      {
        "start": 62750,
        "text": " the other hand, using another API means that you’ll have to depend on\ntheir SLA, service-level agreement. If these APIs aren’t reliable, which is\noften the case with early startups, you’ll have to spend your engineering\neffort on guardrails around that.\nIn general, you want a model that is easy to ",
        "chapter": 4
      }
    ],
    "input_tokens": 963,
    "output_tokens": 268,
    "input_cost": 0.00014445,
    "output_cost": 0.0001608,
    "total_cost": 0.00030525,
    "reference_answer": "The excerpt mentions that the same AI model may perform differently across various APIs due to optimization techniques used, which necessitates thorough testing when switching APIs."
  },
  {
    "question": "why do OpenAI models seem worse after updates",
    "answer": "OpenAI models may appear to perform worse after updates for several reasons:\n\n1. **Evaluation Challenges**: Evaluating model performance is inherently difficult. There can be significant variability in how models perform across different benchmarks, and users may not have a clear understanding of what these benchmarks measure. Thus, the perception that a model has degraded can stem from the complexities of evaluation rather than actual declines in performance.\n\n2. **Changes in Benchmarks**: A study noted that performances for models like GPT-3.5 and GPT-4 changed notably between different time periods (e.g., March and June 2023). These variations could influence user perceptions if they are not aware that benchmarks can fluctuate.\n\n3. **Model Optimization**: Different APIs might optimize model performance in distinct ways, leading to slight performance differences. When users switch between these APIs, they may notice what they perceive as a decline in ability.\n\n4. **Best Model for the Application**: The optimal model for one application may not be the best for another; thus, users might feel that an update did not serve their specific needs effectively.\n\nOverall, while these updates are intended to enhance model capabilities, the complexities surrounding evaluation and user expectations can lead to a perception that performance has worsened.",
    "context": [
      {
        "start": 78500,
        "text": "become saturated, you should look for the latest benchmarks.\nMake sure to evaluate how reliable a benchmark is. Because anyone can\ncreate and publish a benchmark, many benchmarks might not be measuring\nwhat you expect them to measure.\nARE OPENAI’S MODELS GETTING WORSE?\nEvery time OpenAI updates its ",
        "chapter": 4
      },
      {
        "start": 78750,
        "text": "DELS GETTING WORSE?\nEvery time OpenAI updates its models, people complain that their models\nseem to be getting worse. For example, a study by Stanford and UC\nBerkeley (Chen et al., 2023) found that for many benchmarks, both GPT-\n3.5 and GPT-4’s performances changed significantly between March 2023\na",
        "chapter": 4
      },
      {
        "start": 79000,
        "text": "rmances changed significantly between March 2023\nand June 2023, as shown in Figure 4-9.\nFigure 4-9. Changes in the performances of GPT-3.5 and GPT-4 from March 2023 to\nJune 2023 on certain benchmarks (Chen et al., 2023).\nAssuming that OpenAI doesn’t intentionally release worse models, what\nmight be ",
        "chapter": 4
      },
      {
        "start": 79250,
        "text": "intentionally release worse models, what\nmight be the reason for this perception? One potential reason is that\nevaluation is hard, and no one, not even OpenAI, knows for sure if a model\nis getting better or worse. While evaluation is definitely hard, I doubt that\n24\nOpenAI would fly completely blind",
        "chapter": 4
      },
      {
        "start": 53500,
        "text": "gh both OpenAI and Azure APIs. There might be slight\ndifferences in the performance of the same model provided through\ndifferent APIs, as different APIs might use different techniques to optimize\nthis model, so make sure to run thorough tests when you switch between\nmodel APIs.\nCommercial models are",
        "chapter": 4
      },
      {
        "start": 60000,
        "text": "other reason that might cause open source models to lag behind is that\nopen source developers don’t receive feedback from users to improve their\nmodels, the way commercial models do. Once a model is open sourced,\nmodel developers have no idea how the model is being used, and how well\nthe model works",
        "chapter": 4
      },
      {
        "start": 59500,
        "text": "It’s a common practice for\ncompanies to keep their strongest models behind APIs and open source their\nweaker models.\nFigure 4-7. The gap between open source models and proprietary models is decreasing on the\nMMLU benchmark. Image by Maxime Labonne.\nFor this reason, it’s likely that the strongest ope",
        "chapter": 4
      },
      {
        "start": 69250,
        "text": "lets you run any of the\napproximately 500 existing benchmarks and register new benchmarks to\nevaluate OpenAI models. Their benchmarks evaluate a wide range of\ncapabilities, from doing math and solving puzzles to identifying ASCII art\nthat represents words.\nBenchmark selection and aggregation\nBenchma",
        "chapter": 4
      },
      {
        "start": 79500,
        "text": " I doubt that\n24\nOpenAI would fly completely blind. If the second reason is true, it\nreinforces the idea that the best model overall might not be the best model\nfor your application.\nNot all models have publicly available scores on all benchmarks. If the\nmodel you care about doesn’t have a publicly ",
        "chapter": 4
      },
      {
        "start": 59000,
        "text": " decreasing on\nthe MMLU benchmark over time. This trend has made many people believe\nthat one day, there will be an open source model that performs just as well,\nif not better, than the strongest proprietary model.\nAs much as I want open source models to catch up with proprietary models,\nI don’t thi",
        "chapter": 4
      }
    ],
    "input_tokens": 980,
    "output_tokens": 251,
    "input_cost": 0.000147,
    "output_cost": 0.0001506,
    "total_cost": 0.0002976,
    "reference_answer": "The article mentions that users often perceive OpenAI's models as deteriorating post-update, supported by a study indicating significant performance changes in GPT-3.5 and GPT-4 between March and June 2023."
  },
  {
    "question": "Why did Samsung ban ChatGPT?",
    "answer": "Samsung banned ChatGPT in May 2023 due to an incident where employees mistakenly input proprietary information into the model, leading to a leak of the company's secrets. This leak was serious enough for Samsung to take action, especially considering that some regulations in various countries prohibit sending certain data outside their borders.",
    "context": [
      {
        "start": 55500,
        "text": "leaked information was used against Samsung. However, the\nincident was serious enough for Samsung to ban ChatGPT in May 2023.\nSome countries have laws that forbid sending certain data outside their\nborders. If a model API provider wants to serve these use cases, they will\nhave to set up servers in t",
        "chapter": 4
      },
      {
        "start": 55250,
        "text": "e of the organization.\nOne of the most notable early incidents was when Samsung employees put\nSamsung’s proprietary information into ChatGPT, accidentally leaking the\n15\n15\ncompany’s secrets. It’s unclear how Samsung discovered this leak and\nhow the leaked information was used against Samsung. Howev",
        "chapter": 4
      },
      {
        "start": 113250,
        "text": "ers but don’t trust other startups.\n5\nThe story was reported by several outlets, including TechRadar (see “Samsung Workers Made a\nMajor Error by Using ChatGPT”, by Lewis Maddison (April 2023).\n6\nAs regulations are evolving around the world, requirements for auditable information of models and\ntraini",
        "chapter": 4
      },
      {
        "start": 2500,
        "text": "t he had no idea if the\nmodel’s predictions were accurate. At the beginning of the ChatGPT fever,\ncompanies rushed to deploy customer support chatbots. Many of them are\nstill unsure if these chatbots help or hurt their user experience.\nBefore investing time, money, and resources into building an app",
        "chapter": 4
      },
      {
        "start": 116250,
        "text": " to degrade and some to\nimprove. For example, migrating from GPT-3.5-turbo-0301 to GPT-3.5-turbo-1106 led to a 10% drop\nin Voiceflow’s intent classification task but an improvement in GoDaddy’s customer support chatbot.\n5\nIf there is a publicly available score, check how reliable the score is.\n6\nThe",
        "chapter": 4
      },
      {
        "start": 112500,
        "text": "allows this,\nbut if ChatGPT doesn’t, then X violated ChatGPT’s terms of use, and therefore, X can’t be used. This\nis why knowing a model’s data lineage is so important.\n3\nFor example, as of this writing, you can access GPT-4 models only via OpenAI or Azure. Some\nmight argue that being able to provid",
        "chapter": 4
      },
      {
        "start": 111250,
        "text": "shed a study of one million conversations on Chatbot Arena, but\nthese conversations aren’t grounded in real-world applications. I’m waiting for studies from model\nproviders and API providers.\n8\nThe knowledge part is tricky, as the roleplaying model shouldn’t say things that Jackie Chan doesn’t\nknow.",
        "chapter": 4
      },
      {
        "start": 28000,
        "text": "irement\nfor foundation models, and most foundation models are trained to do so.\nInstructGPT, the predecessor of ChatGPT, was named so because it was\nfinetuned for following instructions. More powerful models are generally\nbetter at following instructions. GPT-4 is better at following most\ninstructio",
        "chapter": 4
      },
      {
        "start": 56000,
        "text": "hange. In August 2023, Zoom faced a\nbacklash after people found out the company had quietly changed its terms\nof service to let Zoom use users’ service-generated data, including product\nusage data and diagnostics data, to train its AI models.\nWhat’s the problem with people using your data to train t",
        "chapter": 4
      },
      {
        "start": 75000,
        "text": "o being saturated. In\nJune 2024, less than a year after their leaderboard’s last revamp, Hugging Face updated their\nleaderboard again with an entirely new set of benchmarks that are more challenging and focus on\nmore practical capabilities. For example, GSM-8K was replaced by MATH lvl 5, which consi",
        "chapter": 4
      }
    ],
    "input_tokens": 1011,
    "output_tokens": 60,
    "input_cost": 0.00015165,
    "output_cost": 3.6e-05,
    "total_cost": 0.00018765,
    "reference_answer": "Samsung banned ChatGPT due to a serious incident involving leaked information, the details of which remain unclear, highlighting concerns over data security and privacy."
  },
  {
    "question": "methods for decontaminating evaluation data",
    "answer": "To decontaminate evaluation data, you first need to detect the presence of contamination. This can be done using heuristic methods such as n-gram overlapping and perplexity. Once contamination is detected, appropriate methods should be applied to cleanse the data.\n\n1. **Detection**: Leverage techniques like n-gram overlapping to identify sequences of tokens that may indicate contamination within the evaluation data.\n\n2. **Decontamination**: After identifying contaminated segments, you can remove or correct these sections to ensure that your evaluation data accurately represents the intended distribution and is not influenced by unwanted artifacts or biases.\n\nThis process is crucial to maintain the integrity of your evaluation process and ensure reliable results from your AI systems.",
    "context": [
      {
        "start": 46000,
        "text": "vel, the evaluation workflow consists of four steps (see\nFigure 4-5):\n1. Filter out models whose hard attributes don’t work for you. Your list of\nhard attributes depends heavily on your own internal policies, whether\nyou want to use commercial APIs or host your own models.\n2. Use publicly available ",
        "chapter": 4
      },
      {
        "start": 98250,
        "text": "luate up to 500 daily conservations with their AI\nsystems.\nConsider evaluation methods to be used not just during experimentation but\nalso during production. During experimentation, you might have reference\ndata to compare your application’s outputs to, whereas, in production,\nreference data might n",
        "chapter": 4
      },
      {
        "start": 46500,
        "text": "3. Run experiments with your own evaluation pipeline to find the best\nmodel, again, balancing all your objectives.\n4. Continually monitor your model in production to detect failure and\ncollect feedback to improve your application.\nFigure 4-5. An overview of the evaluation workflow to evaluate models",
        "chapter": 4
      },
      {
        "start": 106750,
        "text": ".\nAs you iterate on your evaluation pipeline, make sure to do proper\nexperiment tracking: log all variables that could change in an evaluation\nprocess, including but not limited to the evaluation data, the rubric, and the\nprompt and sampling configurations used for the AI judges.\nSummary\nThis is one",
        "chapter": 4
      },
      {
        "start": 83750,
        "text": "d on many bar exam questions.\nTo deal with data contamination, you first need to detect the contamination,\nand then decontaminate your data. You can detect contamination using\nheuristics like n-gram overlapping and perplexity:\nN-gram overlapping\nFor example, if a sequence of 13 tokens in an evaluati",
        "chapter": 4
      },
      {
        "start": 87250,
        "text": "valuation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a Syst",
        "chapter": 4
      },
      {
        "start": 87000,
        "text": "opic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confus",
        "chapter": 4
      },
      {
        "start": 99250,
        "text": ". Chapter 8 discusses AI-generated data. The success of this phase also\ndepends on the clarity of the scoring rubric. The annotation guideline\ncreated for evaluation can be reused to create instruction data for finetuning\nlater, if you choose to finetune.\nSlice your data to gain a finer-grained unde",
        "chapter": 4
      },
      {
        "start": 98750,
        "text": "\nuse user feedback to improve your application. How to collect user\nfeedback is discussed in Chapter 10.\nAnnotate evaluation data\nCurate a set of annotated examples to evaluate your application. You need\nannotated data to evaluate each of your system’s components and each\ncriterion, for both turn-ba",
        "chapter": 4
      },
      {
        "start": 101000,
        "text": " British Medical Journal\n(Clinical Research Edition) 292, no. 6524 (March 1986): 879–82.\nYou should have multiple evaluation sets to represent different data slices.\nYou should have one set that represents the distribution of the actual\nproduction data to estimate how the system does overall. You ca",
        "chapter": 4
      }
    ],
    "input_tokens": 923,
    "output_tokens": 141,
    "input_cost": 0.00013845,
    "output_cost": 8.46e-05,
    "total_cost": 0.00022305,
    "reference_answer": "The text emphasizes the need to actively decontaminate data after detecting contamination, which ensures that evaluation benchmarks accurately represent model performance without bias from previously seen samples."
  },
  {
    "question": "best practices for evaluating generative AI responses",
    "answer": "Best practices for evaluating generative AI responses include the following:\n\n1. **Define Evaluation Criteria**: Establish clear guidelines for what qualifies as a \"good\" response. As highlighted by LinkedIn’s experience, creating specific evaluation metrics is crucial for assessing the quality of outputs effectively.\n\n2. **Use AI as a Judge**: Employ AI models to evaluate fluency, coherence, and factual consistency. This method effectively utilizes the capabilities of generative models to assess their outputs.\n\n3. **Evaluate Across Contexts**: In production environments, consider how the evaluation approach changes compared to experimentation. Reference data used during testing may not be available in live settings, necessitating adjustments to the evaluation strategy.\n\n4. **Per Turn and Per Task Evaluation**: Analyze responses at both the individual turn level and for the overall task performance. A turn may comprise multiple interactions, so it’s essential to evaluate the cumulative effectiveness of these responses.\n\n5. **Iterative Evaluation Pipeline**: Establish a reliable evaluation pipeline that allows for continuous assessment of the model's performance over time, recognizing that evaluation is an ongoing process rather than a one-time assessment. \n\nImplementing these practices will enhance the reliability and effectiveness of generative AI responses.",
    "context": [
      {
        "start": 13250,
        "text": "involving creative writing and\nlow-resource languages. Fluency and coherence can be evaluated using AI\nas a judge—asking an AI model how fluent and coherent a text is—or using\nperplexity, as discussed in Chapter 3.\nGenerative models, with their new capabilities and new use cases, have new\nissues tha",
        "chapter": 4
      },
      {
        "start": 88750,
        "text": "ed a turn.\nGenerative AI applications, especially chatbot-like applications, allow back-\nand-forth between the user and the application, as in a conversation, to\naccomplish a task. Imagine you want to use an AI model to debug why your\nPython code is failing. The model responds by asking for more inf",
        "chapter": 4
      },
      {
        "start": 91500,
        "text": "efine evaluation criteria\nOften, the hardest part of evaluation isn’t determining whether an output is\ngood, but rather what good means. In retrospect of one year of deploying\ngenerative AI applications, LinkedIn shared that the first hurdle was in\ncreating an evaluation guideline. A correct respons",
        "chapter": 4
      },
      {
        "start": 3500,
        "text": "ecommender systems are common because their successes can be\n1\nevaluated by an increase in engagement or purchase-through rates.\nThe success of a fraud detection system can be measured by how much\nmoney is saved from prevented frauds.\nCoding is a common generative AI use case because, unlike other\ng",
        "chapter": 4
      },
      {
        "start": 3750,
        "text": "mon generative AI use case because, unlike other\ngeneration tasks, generated code can be evaluated using functional\ncorrectness.\nEven though foundation models are open-ended, many of their use cases\nare close-ended, such as intent classification, sentiment analysis, next-\naction prediction, etc. It’",
        "chapter": 4
      },
      {
        "start": 98250,
        "text": "luate up to 500 daily conservations with their AI\nsystems.\nConsider evaluation methods to be used not just during experimentation but\nalso during production. During experimentation, you might have reference\ndata to compare your application’s outputs to, whereas, in production,\nreference data might n",
        "chapter": 4
      },
      {
        "start": 18750,
        "text": "an\noutput against—this context was either provided by users or retrieved by\nyou (context retrieval is discussed in Chapter 6). The most straightforward\nevaluation approach is AI as a judge. As discussed in Chapter 3, AI judges\ncan be asked to evaluate anything, including factual consistency. Both Li",
        "chapter": 4
      },
      {
        "start": 107000,
        "text": "ations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enabl",
        "chapter": 4
      },
      {
        "start": 11250,
        "text": "as summarization, translation, and\nessay writing. Let’s discuss how generation capabilities can be evaluated in\nthe next section.\nGeneration Capability\nAI was used to generate open-ended outputs long before generative AI\nbecame a thing. For decades, the brightest minds in NLP (natural language\nproce",
        "chapter": 4
      },
      {
        "start": 88500,
        "text": "he application correctly extract the current\nemployer?\nIf applicable, evaluate your application both per turn and per task. A turn\ncan consist of multiple steps and messages. If a system takes multiple steps\nto generate an output, it’s still considered a turn.\nGenerative AI applications, especially ",
        "chapter": 4
      }
    ],
    "input_tokens": 936,
    "output_tokens": 243,
    "input_cost": 0.0001404,
    "output_cost": 0.0001458,
    "total_cost": 0.0002862,
    "reference_answer": "Best practices involve clearly stating what constitutes a good response, considering multiple criteria for evaluation, and recognizing that a correct response may not always be a good one in practical applications."
  },
  {
    "question": "limitations of public benchmarks in AI evaluation",
    "answer": "Public benchmarks in AI evaluation have several limitations:\n\n1. **Lack of Specificity**: Public benchmarks may not perfectly represent the needs of a specific application. They often evaluate general capabilities rather than tailored metrics that align with particular use cases.\n\n2. **Contamination Issues**: Public benchmarks are likely contaminated, meaning the data used in them may overlap with the training data for many models. This can lead to misleading evaluations as models may perform well on benchmarks they have indirectly \"seen\" during training.\n\n3. **Limited Value in Finding Optimal Models**: While public benchmarks can help eliminate poor models, they are not effective for identifying the best models tailored for specific applications. They can provide a narrow perspective and lead to suboptimal decisions.\n\n4. **Necessary Additional Evaluation**: After using public benchmarks to narrow down candidates, it is essential to run an evaluation pipeline with custom metrics to identify the best-performing models for the specific application.\n\nOverall, although public benchmarks can serve as a preliminary filter, they cannot replace comprehensive evaluation approaches tailored to specific organizational needs.",
    "context": [
      {
        "start": 6250,
        "text": "u.\nTo evaluate whether a model has the necessary capabilities, you can rely on\ndomain-specific benchmarks, either public or private. Thousands of public\nbenchmarks have been introduced to evaluate seemingly endless\ncapabilities, including code generation, code debugging, grade school math,\nscience k",
        "chapter": 4
      },
      {
        "start": 107000,
        "text": "ations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enabl",
        "chapter": 4
      },
      {
        "start": 80750,
        "text": "iments using your own benchmarks and metrics. This is not only\nbecause public benchmarks are unlikely to represent your application’s\nneeds perfectly, but also because they are likely contaminated. How public\nbenchmarks get contaminated and how to handle data contamination will be\nthe topic of the n",
        "chapter": 4
      },
      {
        "start": 80500,
        "text": " need to think about how important each benchmark\nis to you and weigh their scores accordingly.\nAs you evaluate models using public benchmarks, keep in mind that the\ngoal of this process is to select a small subset of models to do more rigorous\nexperiments using your own benchmarks and metrics. This",
        "chapter": 4
      },
      {
        "start": 4500,
        "text": "to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can th",
        "chapter": 4
      },
      {
        "start": 68750,
        "text": "ks designed to evaluate a model’s different\ncapabilities. Google’s BIG-bench (2022) alone has 214 benchmarks. The\nnumber of benchmarks rapidly grows to match the rapidly growing number\nof AI use cases. In addition, as AI models improve, old benchmarks\nsaturate, necessitating the introduction of new ",
        "chapter": 4
      },
      {
        "start": 79750,
        "text": " the\nmodel you care about doesn’t have a publicly available score on your\n25\nbenchmark, you will need to run the evaluation yourself. Hopefully, an\nevaluation harness can help you with that. Running benchmarks can be\nexpensive. For example, Stanford spent approximately $80,000–$100,000\n26\nto evaluat",
        "chapter": 4
      },
      {
        "start": 108500,
        "text": "hmarks.\nPublic benchmarks can help you weed out bad models, but they won’t help\nyou find the best models for your applications. Public benchmarks are also\nlikely contaminated, as their data is included in the training data of many\nmodels. There are public leaderboards that aggregate multiple benchma",
        "chapter": 4
      },
      {
        "start": 86750,
        "text": " for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success ",
        "chapter": 4
      },
      {
        "start": 78000,
        "text": "rboards with public benchmarks\nWhen evaluating models for a specific application, you’re basically creating\na private leaderboard that ranks models based on your evaluation criteria.\nThe first step is to gather a list of benchmarks that evaluate the capabilities\nimportant to your application. If you",
        "chapter": 4
      }
    ],
    "input_tokens": 918,
    "output_tokens": 211,
    "input_cost": 0.0001377,
    "output_cost": 0.0001266,
    "total_cost": 0.0002643,
    "reference_answer": "The text warns that while public benchmarks can filter out poor models, they may not identify the best fit for specific applications due to issues like contamination and unclear aggregation processes."
  },
  {
    "question": "evaluating generative AI models",
    "answer": "Evaluating generative AI models involves several critical considerations to determine effectiveness in specific applications. Here’s a summary based on the context from Chapter 4 of \"AI Engineering\" by Chip Huyen:\n\n1. **Context-Dependent Evaluation**: It's essential to evaluate models based on their intended purposes and application contexts. Each use case may demand different evaluation criteria tailored to specific tasks.\n\n2. **Evaluation Criteria Definition**: Establishing what constitutes a \"good\" output can be challenging. LinkedIn's experience highlights that creating clear evaluation guidelines is often a significant hurdle in assessing generative AI applications.\n\n3. **Automatic Evaluation Approaches**: Various approaches discussed previously (in Chapter 3) should be deployed to evaluate AI systems. These can include metrics like perplexity for fluency and coherence or specific application-based measures, such as functional correctness for coding tasks.\n\n4. **Iterative Evaluation Workflow**: The evaluation process is iterative, allowing for adjustments based on findings at each stage. This could involve reconsidering initial decisions about model selection or evaluation criteria based on new insights from the evaluation itself.\n\n5. **Different Evaluation Aspects**: Evaluators should consider multiple dimensions, such as fluency, coherence, and relevancy of responses in conversational applications. For more structured tasks like coding, functional correctness can also be a key metric.\n\nIn conclusion, a thorough understanding of the application and clear, well-defined metrics are essential for effectively evaluating generative AI models.",
    "context": [
      {
        "start": 13250,
        "text": "involving creative writing and\nlow-resource languages. Fluency and coherence can be evaluated using AI\nas a judge—asking an AI model how fluent and coherent a text is—or using\nperplexity, as discussed in Chapter 3.\nGenerative models, with their new capabilities and new use cases, have new\nissues tha",
        "chapter": 4
      },
      {
        "start": 3750,
        "text": "mon generative AI use case because, unlike other\ngeneration tasks, generated code can be evaluated using functional\ncorrectness.\nEven though foundation models are open-ended, many of their use cases\nare close-ended, such as intent classification, sentiment analysis, next-\naction prediction, etc. It’",
        "chapter": 4
      },
      {
        "start": 88750,
        "text": "ed a turn.\nGenerative AI applications, especially chatbot-like applications, allow back-\nand-forth between the user and the application, as in a conversation, to\naccomplish a task. Imagine you want to use an AI model to debug why your\nPython code is failing. The model responds by asking for more inf",
        "chapter": 4
      },
      {
        "start": 91500,
        "text": "efine evaluation criteria\nOften, the hardest part of evaluation isn’t determining whether an output is\ngood, but rather what good means. In retrospect of one year of deploying\ngenerative AI applications, LinkedIn shared that the first hurdle was in\ncreating an evaluation guideline. A correct respons",
        "chapter": 4
      },
      {
        "start": 3500,
        "text": "ecommender systems are common because their successes can be\n1\nevaluated by an increase in engagement or purchase-through rates.\nThe success of a fraud detection system can be measured by how much\nmoney is saved from prevented frauds.\nCoding is a common generative AI use case because, unlike other\ng",
        "chapter": 4
      },
      {
        "start": 0,
        "text": "Chapter 4. Evaluate AI Systems\nA model is only useful if it works for its intended purposes. You need to\nevaluate models in the context of your application. Chapter 3 discusses\ndifferent approaches to automatic evaluation. This chapter discusses how to\nuse these approaches to evaluate models for you",
        "chapter": 4
      },
      {
        "start": 11250,
        "text": "as summarization, translation, and\nessay writing. Let’s discuss how generation capabilities can be evaluated in\nthe next section.\nGeneration Capability\nAI was used to generate open-ended outputs long before generative AI\nbecame a thing. For decades, the brightest minds in NLP (natural language\nproce",
        "chapter": 4
      },
      {
        "start": 88500,
        "text": "he application correctly extract the current\nemployer?\nIf applicable, evaluate your application both per turn and per task. A turn\ncan consist of multiple steps and messages. If a system takes multiple steps\nto generate an output, it’s still considered a turn.\nGenerative AI applications, especially ",
        "chapter": 4
      },
      {
        "start": 30000,
        "text": "use\nthe model doesn’t know how to write lục bát, or because it doesn’t\nunderstand what it’s supposed to do.\nWARNING\nHow well a model performs depends on the quality of its instructions, which makes it hard to\nevaluate AI models. When a model performs poorly, it can either be because the model is bad",
        "chapter": 4
      },
      {
        "start": 46750,
        "text": "view of the evaluation workflow to evaluate models for your application.\nThese four steps are iterative—you might want to change the decision from\na previous step with newer information from the current step. For example,\nyou might initially want to host open source models. However, after public\nand",
        "chapter": 4
      }
    ],
    "input_tokens": 947,
    "output_tokens": 292,
    "input_cost": 0.00014205,
    "output_cost": 0.0001752,
    "total_cost": 0.00031725,
    "reference_answer": "The text discusses how functional correctness is a key evaluation metric for generative AI in coding tasks, contrasting it with the challenges of evaluating more open-ended applications."
  },
  {
    "question": "measuring safety and factual consistency in generative models",
    "answer": "To measure safety and factual consistency in generative models, two approaches can be utilized:\n\n1. **Factual Consistency Assessment**:\n   - **Local Factual Consistency**: This evaluates the output against a specific context. An output is considered factually consistent if it aligns with the provided context. For instance, if the context indicates that \"the sky is purple,\" then an output stating \"the sky is purple\" would be deemed factually consistent. Conversely, a statement like \"the sky is blue\" would be inconsistent if the context contradicts it.\n   - **Global Factual Consistency**: This involves assessing the output against general knowledge. For example, if a model outputs \"the sky is blue,\" this statement would be viewed as factually consistent given that it is widely accepted as a true fact.\n\n2. **Safety Evaluation**:\n   - Safety encompasses various potential harms that the model's outputs may cause, including toxicity and biases. Evaluating safety is crucial to determine whether the generated outputs could harm users or society. Different safety solutions categorize harms in various ways, as illustrated by the taxonomies from OpenAI's content moderation endpoint and Meta's Llama Guard paper.\n\nIn tandem, these methods allow for a comprehensive analysis of both the factual correctness of the outputs and their potential safety implications, ensuring a thorough understanding of the model's behavior and its societal impact. Techniques such as self-verification and knowledge-augmented verification may enhance the measurement of factual consistency further, as indicated by recent studies.",
    "context": [
      {
        "start": 15000,
        "text": "der two\nsettings: against explicitly provided facts (context) or against open\nknowledge:\nLocal factual consistency\nThe output is evaluated against a context. The output is considered\nfactually consistent if it’s supported by the given context. For\nexample, if the model outputs “the sky is blue” and ",
        "chapter": 4
      },
      {
        "start": 19000,
        "text": "e anything, including factual consistency. Both Liu\net al. (2023) and Luo et al. (2023) showed that GPT-3.5 and GPT-4 can\noutperform previous methods at measuring factual consistency. The paper\n“TruthfulQA: Measuring How Models Mimic Human Falsehoods” (Lin et\nal., 2022) shows that their finetuned mo",
        "chapter": 4
      },
      {
        "start": 19250,
        "text": "” (Lin et\nal., 2022) shows that their finetuned model GPT-judge is able to predict\nwhether a statement is considered truthful by humans with 90–96%\naccuracy. Here’s the prompt that Liu et al. (2023) used to evaluate the\nfactual consistency of a summary with respect to the original document:\nFactual ",
        "chapter": 4
      },
      {
        "start": 19750,
        "text": "\nAnswer:\nMore sophisticated AI as a judge techniques to evaluate factual consistency\nare self-verification and knowledge-augmented verification:\nSelf-verification\nSelfCheckGPT (Manakul et al., 2023) relies on an assumption that if\na model generates multiple outputs that disagree with one another,\nth",
        "chapter": 4
      },
      {
        "start": 13250,
        "text": "involving creative writing and\nlow-resource languages. Fluency and coherence can be evaluated using AI\nas a judge—asking an AI model how fluent and coherent a text is—or using\nperplexity, as discussed in Chapter 3.\nGenerative models, with their new capabilities and new use cases, have new\nissues tha",
        "chapter": 4
      },
      {
        "start": 24750,
        "text": " in GPT-4’s technical\nreport.\nSafety\nOther than factual consistency, there are many ways in which a model’s\noutputs can be harmful. Different safety solutions have different ways of\ncategorizing harms—see the taxonomy defined in OpenAI’s content\nmoderation endpoint and Meta’s Llama Guard paper (Inan",
        "chapter": 4
      },
      {
        "start": 15750,
        "text": "’s policies), and\nbusiness analysis (the extracted insights should be consistent with the\ndata).\nGlobal factual consistency\nThe output is evaluated against open knowledge. If the model\noutputs “the sky is blue” and it’s a commonly accepted fact that the\nsky is blue, this statement is considered fact",
        "chapter": 4
      },
      {
        "start": 15250,
        "text": "ample, if the model outputs “the sky is blue” and the given context\nsays that the sky is purple, this output is considered factually\ninconsistent. Conversely, given this context, if the model outputs “the\nsky is purple”, this output is factually consistent.\nLocal factual consistency is important for",
        "chapter": 4
      },
      {
        "start": 13750,
        "text": "ation\ndevelopers want to measure is factual consistency. Another issue commonly\ntracked is safety: can the generated outputs cause harm to users and\nsociety? Safety is an umbrella term for all types of toxicity and biases.\nThere are many other measurements that an application developer might\ncare ab",
        "chapter": 4
      },
      {
        "start": 14750,
        "text": "astrophic consequences,\nmany techniques have been and will be developed to detect and measure it.\nIt’s impossible to cover them all in one chapter, so I’ll go over only the\nbroad strokes.\nThe factual consistency of a model’s output can be verified under two\nsettings: against explicitly provided fact",
        "chapter": 4
      }
    ],
    "input_tokens": 1015,
    "output_tokens": 305,
    "input_cost": 0.00015225,
    "output_cost": 0.000183,
    "total_cost": 0.00033525,
    "reference_answer": "Evaluating safety includes assessing biases and toxicity in AI outputs, while factual consistency is crucial to prevent harmful misinformation in applications."
  },
  {
    "question": "How to test AI model's instruction-following ability?",
    "answer": "To test an AI model's instruction-following ability, you can utilize several evaluation methods as highlighted in the \"AI Engineering\" book. Two prominent benchmarks discussed are **IFEval** and **INFOBench**. Here’s how you can implement these evaluations:\n\n1. **Define Clear Instructions**: Create a set of instructions that the AI model needs to follow. Instructions should cover a wide range of tasks, including those that require specific output formats, such as keyword inclusion, adherence to length constraints, and structured formats like JSON or YAML.\n\n2. **Automated Verification**: Utilize automatic verification methods where possible. For example, if an instruction is to include specific keywords in an output, write a program to check for those keywords in the generated content. This can also extend to other criteria like output length or format.\n\n3. **Evaluation Criteria**: Establish clear and measurable criteria for each instruction. As per the methods suggested in the book, these criteria can be framed as yes/no questions that can be evaluated by either human judges or using automated algorithms.\n\n4. **Scoring System**: Implement a scoring system where the model’s performance on each instruction is quantified. For instance, if an instruction has three criteria and the model meets two, it scores 2 out of 3. The final score will summarize the model's overall following capability across all instructions.\n\n5. **Risk of Ambiguity**: Be aware that poor performance could stem from ambiguous instructions. Clearly articulate instructions to minimize misinterpretation, which helps in accurately assessing the model's capabilities.\n\n6. **Role-Specific Evaluations**: If the model needs to assume a specific role (e.g., communicating in a manner suitable for a young audience), ensure that the evaluation includes criteria to verify if the model adheres to that role.\n\n7. **Diversity of Instructions**: Incorporate a variety of instruction types and complexity levels to comprehensively assess the model's abilities.\n\nBy employing these strategies, you can effectively evaluate how well an AI model follows instructions, ensuring that the assessment is thorough and measurable.",
    "context": [
      {
        "start": 30750,
        "text": "and what evaluation methods are appropriate.\nThe Google benchmark IFEval, Instruction-Following Evaluation, focuses\non whether the model can produce outputs following an expected format.\nZhou et al. (2023) identified 25 types of instructions that can be\nautomatically verified, such as keyword inclus",
        "chapter": 4
      },
      {
        "start": 31500,
        "text": "fiable instructions proposed by Zhou et al. to evaluate models’\ninstruction-following capability. Table taken from the IFEval paper, which is available under the\nlicense CC BY 4.0.\nInstruction\nInstruction Description\ngroup\nKeywords Include keywords Include keywords {keyword1},\n{keyword2} in your res",
        "chapter": 4
      },
      {
        "start": 35000,
        "text": "s instruction. Each of these yes/no questions can\nbe answered by a human or AI evaluator. If the instruction has three criteria\nand the evaluator determines that a model’s output meets two of them, the\nmodel’s score for this instruction is 2/3. The final score for a model on this\nbenchmark is the nu",
        "chapter": 4
      },
      {
        "start": 30250,
        "text": " poorly, it can either be because the model is bad or the\ninstruction is bad.\nInstruction-following criteria\nDifferent benchmarks have different notions of what instruction-following\ncapability encapsulates. The two benchmarks discussed here, IFEval and\nINFOBench, measure models’ capability to follo",
        "chapter": 4
      },
      {
        "start": 34250,
        "text": " easily automated. If you instruct a\nmodel to “use language appropriate to a young audience”, how do you\nautomatically verify if the output is indeed appropriate for a young\naudience?\nFor verification, INFOBench authors constructed a list of criteria for each\ninstruction, each framed as a yes/no que",
        "chapter": 4
      },
      {
        "start": 30500,
        "text": "and\nINFOBench, measure models’ capability to follow a wide range of\ninstructions, which are to give you ideas on how to evaluate a model’s\nability to follow your instructions: what criteria to use, what instructions to\ninclude in the evaluation set, and what evaluation methods are appropriate.\nThe G",
        "chapter": 4
      },
      {
        "start": 31000,
        "text": " be\nautomatically verified, such as keyword inclusion, length constraints,\nnumber of bullet points, and JSON format. If you ask a model to write a\nsentence that uses the word “ephemeral”, you can write a program to check\nif the output contains this word; hence, this instruction is automatically\nveri",
        "chapter": 4
      },
      {
        "start": 37750,
        "text": "ully crafted similarity scores (how similar the generated outputs are to\nthe expected outputs) and AI judges.\nIf AI in your application is supposed to assume a certain role, make sure to\nevaluate whether your model stays in character. Depending on the role, you\nmight be able to create heuristics to ",
        "chapter": 4
      },
      {
        "start": 36250,
        "text": "w your\ninstructions using your own criteria. If you need a model to output YAML, include YAML\ninstructions in your benchmark. If you want a model to not say things like “As a language model”,\nevaluate the model on this instruction.\nRoleplaying\nOne of the most common types of real-world instructions ",
        "chapter": 4
      },
      {
        "start": 19750,
        "text": "\nAnswer:\nMore sophisticated AI as a judge techniques to evaluate factual consistency\nare self-verification and knowledge-augmented verification:\nSelf-verification\nSelfCheckGPT (Manakul et al., 2023) relies on an assumption that if\na model generates multiple outputs that disagree with one another,\nth",
        "chapter": 4
      }
    ],
    "input_tokens": 953,
    "output_tokens": 417,
    "input_cost": 0.00014295,
    "output_cost": 0.0002502,
    "total_cost": 0.00039315,
    "reference_answer": "The chapter emphasizes the importance of evaluating a model's instruction-following capability by analyzing its response to structured output requests and adherence to specific constraints."
  },
  {
    "question": "how to create evaluation sets for AI models",
    "answer": "To create evaluation sets for AI models, follow these structured steps based on the \"AI Engineering\" book:\n\n1. **Define Evaluation Criteria**: Start by establishing a list of evaluation criteria specific to your application. This helps to differentiate good outcomes from bad outcomes and determines what aspects of model performance are most critical to your goals.\n\n2. **Filter and Narrow Down Models**: Employ public benchmarks to identify a set of promising models. Use the hard attributes relevant to your organization's policies, such as whether you want to utilize commercial APIs or host your own models.\n\n3. **Build a Custom Evaluation Pipeline**: Design an evaluation pipeline that allows you to assess the models based on the established criteria. This involves setting up processes to run experiments and gather insights on model performance.\n\n4. **Iterative Assessment and Experiment Tracking**: As you conduct evaluations, ensure proper experiment tracking is in place. Log variables that could impact the evaluation process, including data, rubrics, and configurations used. This allows continuous refinement of your evaluation strategy.\n\n5. **Monitor and Collect Feedback**: After deployment, continually monitor the model in production to detect failures. Collect feedback for improvements, which can also inform future iterations of your evaluation sets.\n\nBy following these steps, you can establish a reliable evaluation pipeline that supports ongoing AI model development and enhancement.",
    "context": [
      {
        "start": 87000,
        "text": "opic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confus",
        "chapter": 4
      },
      {
        "start": 46000,
        "text": "vel, the evaluation workflow consists of four steps (see\nFigure 4-5):\n1. Filter out models whose hard attributes don’t work for you. Your list of\nhard attributes depends heavily on your own internal policies, whether\nyou want to use commercial APIs or host your own models.\n2. Use publicly available ",
        "chapter": 4
      },
      {
        "start": 4500,
        "text": "to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can th",
        "chapter": 4
      },
      {
        "start": 106750,
        "text": ".\nAs you iterate on your evaluation pipeline, make sure to do proper\nexperiment tracking: log all variables that could change in an evaluation\nprocess, including but not limited to the evaluation data, the rubric, and the\nprompt and sampling configurations used for the AI judges.\nSummary\nThis is one",
        "chapter": 4
      },
      {
        "start": 46500,
        "text": "3. Run experiments with your own evaluation pipeline to find the best\nmodel, again, balancing all your objectives.\n4. Continually monitor your model in production to detect failure and\ncollect feedback to improve your application.\nFigure 4-5. An overview of the evaluation workflow to evaluate models",
        "chapter": 4
      },
      {
        "start": 86750,
        "text": " for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success ",
        "chapter": 4
      },
      {
        "start": 0,
        "text": "Chapter 4. Evaluate AI Systems\nA model is only useful if it works for its intended purposes. You need to\nevaluate models in the context of your application. Chapter 3 discusses\ndifferent approaches to automatic evaluation. This chapter discusses how to\nuse these approaches to evaluate models for you",
        "chapter": 4
      },
      {
        "start": 107000,
        "text": "ations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enabl",
        "chapter": 4
      },
      {
        "start": 46750,
        "text": "view of the evaluation workflow to evaluate models for your application.\nThese four steps are iterative—you might want to change the decision from\na previous step with newer information from the current step. For example,\nyou might initially want to host open source models. However, after public\nand",
        "chapter": 4
      },
      {
        "start": 87500,
        "text": "process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each application might consist of\nmany components, and a task might be completed after many turns.\nEvaluation can happen at different levels: per task, per turn, and per\nintermediate output.\nYou should evalu",
        "chapter": 4
      }
    ],
    "input_tokens": 912,
    "output_tokens": 265,
    "input_cost": 0.0001368,
    "output_cost": 0.000159,
    "total_cost": 0.0002958,
    "reference_answer": "It stresses the necessity of having multiple evaluation sets that represent various data slices to effectively gauge a system's performance across different segments of users and inputs."
  },
  {
    "question": "what benchmarks to use for evaluating LLMs",
    "answer": "To evaluate large language models (LLMs), several benchmarks can be considered, as outlined in the \"AI Engineering\" book. Here are key benchmarks mentioned for assessing LLM capabilities:\n\n1. **ARC-C**: Measures the ability to solve complex, grade school-level science questions.\n2. **MMLU**: Assesses knowledge and reasoning across various subjects.\n3. **MuSR**: A graduate-level Q&A benchmark focusing on multistep reasoning.\n4. **BBH (BIG-bench Hard)**: Another reasoning benchmark aimed at more challenging tasks.\n5. **IFEval**: Evaluates the model's ability to follow instructions.\n\nIn addition, tools such as EleutherAI's lm-evaluation-harness and OpenAI's evals support a wide array of benchmarks, allowing for comprehensive evaluations involving over 400 and approximately 500 benchmarks, respectively. When selecting benchmarks, consider their relevance to your specific application and how they weigh against each other in the evaluation process.",
    "context": [
      {
        "start": 69000,
        "text": "s\nsaturate, necessitating the introduction of new benchmarks.\nA tool that helps you evaluate a model on multiple benchmarks is an\nevaluation harness. As of this writing, EleutherAI’s lm-evaluation-harness\nsupports over 400 benchmarks. OpenAI’s evals lets you run any of the\napproximately 500 existing",
        "chapter": 4
      },
      {
        "start": 71750,
        "text": " in late 2023, Hugging Face updated their Open LLM Leaderboard\nto use the average of six different benchmarks to rank models:\n1. ARC-C (Clark et al., 2018): Measuring the ability to solve complex,\ngrade school-level science questions.\n2. MMLU (Hendrycks et al., 2020): Measuring knowledge and reasoni",
        "chapter": 4
      },
      {
        "start": 75500,
        "text": "23): a graduate-level Q&A benchmark\nMuSR (Sprague et al., 2023): a chain-of-thought, multistep reasoning benchmark\nBBH (BIG-bench Hard) (Srivastava et al., 2023): another reasoning benchmark\nIFEval (Zhou et al., 2023): an instruction-following benchmark\nI have no doubt that these benchmarks will soo",
        "chapter": 4
      },
      {
        "start": 69250,
        "text": "lets you run any of the\napproximately 500 existing benchmarks and register new benchmarks to\nevaluate OpenAI models. Their benchmarks evaluate a wide range of\ncapabilities, from doing math and solving puzzles to identifying ASCII art\nthat represents words.\nBenchmark selection and aggregation\nBenchma",
        "chapter": 4
      },
      {
        "start": 80500,
        "text": " need to think about how important each benchmark\nis to you and weigh their scores accordingly.\nAs you evaluate models using public benchmarks, keep in mind that the\ngoal of this process is to select a small subset of models to do more rigorous\nexperiments using your own benchmarks and metrics. This",
        "chapter": 4
      },
      {
        "start": 750,
        "text": "of\nfoundation models to choose from, it can feel overwhelming to choose the\nright model for your application. Thousands of benchmarks have been\nintroduced to evaluate these models along different criteria. Can these\nbenchmarks be trusted? How do you select what benchmarks to use? How\nabout public le",
        "chapter": 4
      },
      {
        "start": 86750,
        "text": " for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success ",
        "chapter": 4
      },
      {
        "start": 79750,
        "text": " the\nmodel you care about doesn’t have a publicly available score on your\n25\nbenchmark, you will need to run the evaluation yourself. Hopefully, an\nevaluation harness can help you with that. Running benchmarks can be\nexpensive. For example, Stanford spent approximately $80,000–$100,000\n26\nto evaluat",
        "chapter": 4
      },
      {
        "start": 47500,
        "text": "mber of public benchmarks and why you can’t trust them. This will set\nthe stage for the last section in the chapter. Because public benchmarks\ncan’t be trusted, you need to design your own evaluation pipeline with\nprompts and metrics you can trust.\nModel Build Versus Buy\nAn evergreen question for co",
        "chapter": 4
      },
      {
        "start": 71500,
        "text": " are generally thoughtful about\nhow they select benchmarks, their decision-making process isn’t always\nclear to users. Different leaderboards often end up with different\nbenchmarks, making it hard to compare and interpret their rankings. For\nexample, in late 2023, Hugging Face updated their Open LLM",
        "chapter": 4
      }
    ],
    "input_tokens": 971,
    "output_tokens": 199,
    "input_cost": 0.00014565,
    "output_cost": 0.0001194,
    "total_cost": 0.00026505,
    "reference_answer": "It mentions Eleuther's lm-evaluation-harness, noting ideal examples ranging from 300 to over 1,000, which are critical for assessing model effectiveness reliably."
  },
  {
    "question": "correlation scores of AI benchmarks",
    "answer": "The correlation scores among various AI benchmarks can provide valuable insights into their relationships and effectiveness. According to the context provided, the Pearson correlation scores among the six benchmarks used on Hugging Face's leaderboard show that:\n\n1. **ARC-C, MMLU, and WinoGrande** are strongly correlated, which suggests they share similar testing grounds related to reasoning capabilities.\n2. **TruthfulQA** exhibits only a moderate correlation to the other benchmarks, indicating that improvements in reasoning and math capabilities do not necessarily guarantee improvements in truthfulness.\n\nThis correlation analysis underscores the importance of variating benchmarks in order to avoid redundancy. Strong correlations may reflect similar biases, and thus evaluation pipelines should ensure diverse metrics to capture a broader scope of model performance.",
    "context": [
      {
        "start": 76250,
        "text": "QA is only moderately correlated to other benchmarks,\nsuggesting that improving a model’s reasoning and math capabilities\ndoesn’t always improve its truthfulness.\nTable 4-5. The correlation between the six benchmarks used on Hugging Face’s leaderboard, compute\nARC-C HellaSwag MMLU Truth\nARC-C 1.0000",
        "chapter": 4
      },
      {
        "start": 75750,
        "text": "ark\nI have no doubt that these benchmarks will soon become saturated. However, discussing specific\n23\nbenchmarks, even if outdated, can still be useful as examples to evaluate and interpret benchmarks.\nTable 4-5 shows the Pearson correlation scores among the six benchmarks\nused on Hugging Face’s lea",
        "chapter": 4
      },
      {
        "start": 74750,
        "text": "ark correlation. It is important because if two benchmarks are\nperfectly correlated, you don’t want both of them. Strongly correlated\n21\nbenchmarks can exaggerate biases.\nNOTE\nWhile I was writing this book, many benchmarks became saturated or close to being saturated. In\nJune 2024, less than a year ",
        "chapter": 4
      },
      {
        "start": 76000,
        "text": "mong the six benchmarks\nused on Hugging Face’s leaderboard, computed in January 2024 by Balázs\nGalambosi. The three benchmarks WinoGrande, MMLU, and ARC-C are\nstrongly correlated, which makes sense since they all test reasoning\ncapabilities. TruthfulQA is only moderately correlated to other benchmar",
        "chapter": 4
      },
      {
        "start": 105500,
        "text": "on\npipeline. Be consistent with the configurations of your evaluation.\nFor example, if you use an AI judge, make sure to set your judge’s\ntemperature to 0.\nHow correlated are your metrics?\nAs discussed in “Benchmark selection and aggregation”, if two\nmetrics are perfectly correlated, you don’t need ",
        "chapter": 4
      },
      {
        "start": 68750,
        "text": "ks designed to evaluate a model’s different\ncapabilities. Google’s BIG-bench (2022) alone has 214 benchmarks. The\nnumber of benchmarks rapidly grows to match the rapidly growing number\nof AI use cases. In addition, as AI models improve, old benchmarks\nsaturate, necessitating the introduction of new ",
        "chapter": 4
      },
      {
        "start": 80250,
        "text": "e scores for the\nmodels you care about on these benchmarks, you then need to aggregate\nthese scores to rank models. Not all benchmark scores are in the same unit\nor scale. One benchmark might use accuracy, another F1, and another\nBLEU score. You will need to think about how important each benchmark\n",
        "chapter": 4
      },
      {
        "start": 115500,
        "text": "nd aggregation process. When launching their new\nleaderboard, Hugging Face shared a great analysis of the benchmarks correlation (2024).\n2\nIt’s both really cool and intimidating to see that in just a couple of years, benchmarks had to change\nfrom grade-level questions to graduate-level questions.\n3\n",
        "chapter": 4
      },
      {
        "start": 74500,
        "text": "hallenge of selecting benchmarks to rank\nmodels. If leaderboard developers can’t explain their benchmark selection\nprocesses, it might be because it’s really hard to do so.\nAn important aspect of benchmark selection that is often overlooked is\nbenchmark correlation. It is important because if two be",
        "chapter": 4
      },
      {
        "start": 105750,
        "text": "\nmetrics are perfectly correlated, you don’t need both of them. On the\nother hand, if two metrics are not at all correlated, this means either\nan interesting insight into your model or that your metrics just aren’t\n29\ntrustworthy.\nHow much cost and latency does your evaluation pipeline add to your\na",
        "chapter": 4
      }
    ],
    "input_tokens": 988,
    "output_tokens": 147,
    "input_cost": 0.0001482,
    "output_cost": 8.82e-05,
    "total_cost": 0.0002364,
    "reference_answer": "The chapter highlights the Pearson correlation scores among six benchmarks, indicating strong correlations among WinoGrande, MMLU, and ARC-C due to their focus on reasoning capabilities."
  },
  {
    "question": "AI response length requirements",
    "answer": "In the context of AI responses, particularly when evaluating applications such as chatbots, it's crucial to define what constitutes a \"good response.\" A good response should explain the relevance to the user's query, address factual consistency, and close any identified gaps in understanding or information. For example, while a response indicating \"You are a terrible fit\" may technically be correct, it lacks utility and fails to provide constructive feedback. \n\nEvaluation guidelines are essential for determining the usefulness of a response. Criteria for a good response might include:\n\n1. **Relevance**: The response should directly relate to the user's query.\n2. **Factual consistency**: The information provided must be accurate and verifiable.\n3. **Constructiveness**: It should offer a pathway to resolve any gaps, helping users understand what steps can be taken next.\n\nEstablishing such criteria ensures the AI can produce responses that are not only correct but also beneficial and actionable for users. This approach enhances overall user satisfaction and effectiveness of the application.",
    "context": [
      {
        "start": 92000,
        "text": "good response should explain the gap between\nthis job’s requirements and the candidate’s background, and what the\ncandidate can do to close this gap.\nBefore building your application, think about what makes a good response.\nLangChain’s State of AI 2023 found that, on average, their users used 2.3\ndi",
        "chapter": 4
      },
      {
        "start": 91750,
        "text": "reating an evaluation guideline. A correct response is not always a good\nresponse. For example, for their AI-powered Job Assessment application,\nthe response “You are a terrible fit” might be correct but not helpful, thus\nmaking it a bad response. A good response should explain the gap between\nthis ",
        "chapter": 4
      },
      {
        "start": 35000,
        "text": "s instruction. Each of these yes/no questions can\nbe answered by a human or AI evaluator. If the instruction has three criteria\nand the evaluator determines that a model’s output meets two of them, the\nmodel’s score for this instruction is 2/3. The final score for a model on this\nbenchmark is the nu",
        "chapter": 4
      },
      {
        "start": 91000,
        "text": "bad responses look like, you\nwon’t be able to catch them.\nWhen creating the evaluation guideline, it’s important to define not only\nwhat the application should do, but also what it shouldn’t do. For example,\nif you build a customer support chatbot, should this chatbot answer\nquestions unrelated to y",
        "chapter": 4
      },
      {
        "start": 4500,
        "text": "to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can th",
        "chapter": 4
      },
      {
        "start": 95000,
        "text": ": what scores must an\napplication achieve for it to be useful? For example, you might determine\nthat your chatbot’s factual consistency score must be at least 50% for it to\nbe useful. Anything below this makes it unusable even for general customer\nrequests.\nBefore developing AI evaluation metrics, i",
        "chapter": 4
      },
      {
        "start": 107000,
        "text": "ations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enabl",
        "chapter": 4
      },
      {
        "start": 92250,
        "text": "23 found that, on average, their users used 2.3\ndifferent types of feedback (criteria) to evaluate an application. For\nexample, for a customer support application, a good response might be\ndefined using three criteria:\n1. Relevance: the response is relevant to the user’s query.\n2. Factual consistenc",
        "chapter": 4
      },
      {
        "start": 20500,
        "text": "tuality in Large Language Models”, works by leveraging search\nengine results to verify the response. It works in four steps, as\nvisualized in Figure 4-1:\n1. Use an AI model to decompose the response into individual\nstatements.\n2. Revise each statement to make it self-contained. For example, the\n“it”",
        "chapter": 4
      },
      {
        "start": 22750,
        "text": "an 38 categories, including health,\nlaw, finance, and politics. This benchmark comes with a specialized AI\njudge, GPT-judge, that was finetuned to automatically evaluate whether a\nresponse is factually consistent with the reference response. Table 4-1\nshows example questions and false answers genera",
        "chapter": 4
      }
    ],
    "input_tokens": 979,
    "output_tokens": 202,
    "input_cost": 0.00014685,
    "output_cost": 0.0001212,
    "total_cost": 0.00026805,
    "reference_answer": "The chapter specifies how to set constraints on response length, such as defining minimum and maximum numbers of words, sentences, and paragraphs."
  },
  {
    "question": "benchmark selection criteria for AI leaderboards",
    "answer": "When selecting benchmarks for AI leaderboards, several criteria should be considered based on the insights from the \"AI Engineering book\" by Chip Huyen:\n\n1. **Coverage of Capabilities**: Benchmarks should cover a wide range of capabilities. This typically includes areas such as reasoning, factual consistency, and domain-specific tasks (e.g., math and science) to ensure diverse evaluation.\n\n2. **Benchmark Correlation**: It's crucial to understand the correlation between benchmarks. This ensures that the selected benchmarks provide complementary insights and that the performance of a model can be accurately assessed across different metrics.\n\n3. **Computational Feasibility**: Given the compute constraints, it’s important to consider the computational requirements of each benchmark. Expensive benchmarks may need to be excluded, even if they are significant for certain evaluations.\n\n4. **Representation of Failure Modes**: The selected benchmarks should represent the vast capabilities and different failure modes of models, which is essential for a comprehensive assessment.\n\n5. **Clarity in Selection Process**: The benchmark selection process should be transparent. Leaderboards should have a clear rationale for their chosen benchmarks to facilitate understanding and comparison among different models.\n\n6. **Adaptability**: As demonstrated by updates (like Hugging Face's Open LLM Leaderboard), benchmarks should be adaptable over time to reflect ongoing advancements in models and changing priorities in evaluation.\n\nBy adhering to these criteria, leaderboard developers can create robust and meaningful evaluations of AI models.",
    "context": [
      {
        "start": 70500,
        "text": "on their aggregated\nperformance on a subset of benchmarks. These leaderboards are immensely\nhelpful but far from being comprehensive. First, due to the compute\nconstraint—evaluating a model on a benchmark requires compute—most\nleaderboards can incorporate only a small number of benchmarks. Some\nlead",
        "chapter": 4
      },
      {
        "start": 70750,
        "text": "orate only a small number of benchmarks. Some\nleaderboards might exclude an important but expensive benchmark. For\nexample, HELM (Holistic Evaluation of Language Models) Lite left out an\ninformation retrieval benchmark (MS MARCO, Microsoft Machine\nReading Comprehension) because it’s expensive to run",
        "chapter": 4
      },
      {
        "start": 71250,
        "text": "ur benchmarks. By the end of that year, they extended it to\nsix benchmarks. A small set of benchmarks is not nearly enough to\nrepresent the vast capabilities and different failure modes of foundation\nmodels.\nAdditionally, while leaderboard developers are generally thoughtful about\nhow they select be",
        "chapter": 4
      },
      {
        "start": 74500,
        "text": "hallenge of selecting benchmarks to rank\nmodels. If leaderboard developers can’t explain their benchmark selection\nprocesses, it might be because it’s really hard to do so.\nAn important aspect of benchmark selection that is often overlooked is\nbenchmark correlation. It is important because if two be",
        "chapter": 4
      },
      {
        "start": 71500,
        "text": " are generally thoughtful about\nhow they select benchmarks, their decision-making process isn’t always\nclear to users. Different leaderboards often end up with different\nbenchmarks, making it hard to compare and interpret their rankings. For\nexample, in late 2023, Hugging Face updated their Open LLM",
        "chapter": 4
      },
      {
        "start": 73750,
        "text": "arios.\nPublic leaderboards, in general, try to balance coverage and the number of\nbenchmarks. They try to pick a small set of benchmarks that cover a wide\nrange of capabilities, typically including reasoning, factual consistency, and\ndomain-specific capabilities such as math and science.\nAt a high l",
        "chapter": 4
      },
      {
        "start": 69750,
        "text": "in your leaderboard?\nHow to aggregate these benchmark results to rank models?\nGiven so many benchmarks out there, it’s impossible to look at them all, let\nalone aggregate their results to decide which model is the best. Imagine that\nyou’re considering two models, A and B, for code generation. If mod",
        "chapter": 4
      },
      {
        "start": 78000,
        "text": "rboards with public benchmarks\nWhen evaluating models for a specific application, you’re basically creating\na private leaderboard that ranks models based on your evaluation criteria.\nThe first step is to gather a list of benchmarks that evaluate the capabilities\nimportant to your application. If you",
        "chapter": 4
      },
      {
        "start": 70250,
        "text": "ing\nbenchmark but worse in another coding benchmark?\nFor inspiration on how to create your own leaderboard from public\nbenchmarks, it’s useful to look into how public leaderboards do so.\nPublic leaderboards\nMany public leaderboards rank models based on their aggregated\nperformance on a subset of ben",
        "chapter": 4
      },
      {
        "start": 71750,
        "text": " in late 2023, Hugging Face updated their Open LLM Leaderboard\nto use the average of six different benchmarks to rank models:\n1. ARC-C (Clark et al., 2018): Measuring the ability to solve complex,\ngrade school-level science questions.\n2. MMLU (Hendrycks et al., 2020): Measuring knowledge and reasoni",
        "chapter": 4
      }
    ],
    "input_tokens": 923,
    "output_tokens": 292,
    "input_cost": 0.00013845,
    "output_cost": 0.0001752,
    "total_cost": 0.00031365,
    "reference_answer": "The chapter mentions that while leaderboard developers thoughtfully select benchmarks, their decision-making processes are not always transparent to users, leading to variability across different leaderboards."
  },
  {
    "question": "how to create evaluation guidelines for AI systems",
    "answer": "To create evaluation guidelines for AI systems based on the \"AI Engineering\" book by Chip Huyen, you can follow these structured steps:\n\n1. **Define Evaluation Criteria**: Start by outlining specific evaluation criteria relevant to your application. This will help in establishing what constitutes a successful outcome.\n\n2. **Design the Evaluation Pipeline**: Develop a reliable evaluation pipeline that allows you to differentiate good outcomes from bad ones. Ensure that the pipeline is adaptable to various evaluation methods.\n\n3. **Evaluate All Components**: Real-world AI applications often consist of multiple components. Conduct evaluations at different levels—per task, per turn, and for intermediate outputs to ensure comprehensive assessment.\n\n4. **Incorporate Human Evaluation**: Utilize human experts to evaluate model quality, especially for open-ended responses. Human evaluation can serve as a North Star metric for guiding development.\n\n5. **Experiment Tracking**: Log all variables in your evaluation process, including evaluation data, rubrics, and prompts. This allows for proper tracking of changes and facilitates future iterations.\n\n6. **Run Experiments**: Use your evaluation pipeline to conduct experiments aimed at finding the best model while balancing your defined objectives.\n\n7. **Continuous Monitoring**: Monitor your model in production to detect failures and gather feedback. This ongoing evaluation will help in refining the application over time.\n\nBy adhering to these guidelines, you can establish a robust framework for evaluating AI systems, ultimately aiding in the adoption and success of AI technologies.",
    "context": [
      {
        "start": 107000,
        "text": "ations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enabl",
        "chapter": 4
      },
      {
        "start": 4500,
        "text": "to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can th",
        "chapter": 4
      },
      {
        "start": 87000,
        "text": "opic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confus",
        "chapter": 4
      },
      {
        "start": 106750,
        "text": ".\nAs you iterate on your evaluation pipeline, make sure to do proper\nexperiment tracking: log all variables that could change in an evaluation\nprocess, including but not limited to the evaluation data, the rubric, and the\nprompt and sampling configurations used for the AI judges.\nSummary\nThis is one",
        "chapter": 4
      },
      {
        "start": 18750,
        "text": "an\noutput against—this context was either provided by users or retrieved by\nyou (context retrieval is discussed in Chapter 6). The most straightforward\nevaluation approach is AI as a judge. As discussed in Chapter 3, AI judges\ncan be asked to evaluate anything, including factual consistency. Both Li",
        "chapter": 4
      },
      {
        "start": 250,
        "text": "to\nuse these approaches to evaluate models for your applications.\nThis chapter contains three parts. It starts with a discussion of the criteria\nyou might use to evaluate your applications and how these criteria are\ndefined and calculated. For example, many people worry about AI making\nup facts—how ",
        "chapter": 4
      },
      {
        "start": 97750,
        "text": "ion. Having human experts manually\nevaluate a model’s quality is a long-standing practice in AI. Given the\nchallenges of evaluating open-ended responses, many teams are looking at\nhuman evaluation as the North Star metric to guide their application\ndevelopment. Each day, you can use human experts to",
        "chapter": 4
      },
      {
        "start": 87500,
        "text": "process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each application might consist of\nmany components, and a task might be completed after many turns.\nEvaluation can happen at different levels: per task, per turn, and per\nintermediate output.\nYou should evalu",
        "chapter": 4
      },
      {
        "start": 37750,
        "text": "ully crafted similarity scores (how similar the generated outputs are to\nthe expected outputs) and AI judges.\nIf AI in your application is supposed to assume a certain role, make sure to\nevaluate whether your model stays in character. Depending on the role, you\nmight be able to create heuristics to ",
        "chapter": 4
      },
      {
        "start": 46500,
        "text": "3. Run experiments with your own evaluation pipeline to find the best\nmodel, again, balancing all your objectives.\n4. Continually monitor your model in production to detect failure and\ncollect feedback to improve your application.\nFigure 4-5. An overview of the evaluation workflow to evaluate models",
        "chapter": 4
      }
    ],
    "input_tokens": 910,
    "output_tokens": 293,
    "input_cost": 0.0001365,
    "output_cost": 0.0001758,
    "total_cost": 0.0003123,
    "reference_answer": "The text emphasizes that establishing clear evaluation guidelines is crucial to avoid ambiguous scoring and ensures that misleading responses can be identified effectively."
  },
  {
    "question": "evaluating efficiency in AI model performance",
    "answer": "Evaluating efficiency in AI model performance involves a multi-faceted approach, as highlighted in the \"AI Engineering\" book by Chip Huyen. Here are key considerations:\n\n1. **Evaluation Criteria**: Start by defining specific evaluation criteria tailored to your application. This is essential for understanding the model's utility in your context.\n\n2. **Evaluation Pipeline**: Develop a reliable evaluation pipeline. This is crucial because consistent and accurate evaluations help differentiate effective models from ineffective ones. The effectiveness of an application often hinges on outcomes derived from this pipeline.\n\n3. **Benchmarks and Metrics**: Use public benchmarks for initial model evaluations, but remember their limitations. The goal is to refine your selection to a manageable number of models for in-depth testing with your own benchmarks.\n\n4. **Cost and Latency**: Consider the cost associated with running the model. A model that produces high-quality outputs but incurs significant operational costs or latency may not meet practical application needs.\n\n5. **Human Evaluation**: Incorporating human judgment can serve as an important evaluation metric. This is particularly useful for assessing open-ended responses and can guide the development of your application effectively.\n\nBy implementing these strategies, you can ensure that you are evaluating AI model performance efficiently and effectively, ultimately supporting better AI adoption.",
    "context": [
      {
        "start": 44500,
        "text": "els along the cost–performance axes and choosing the\nmodel that gives the best performance for your bucks\nHowever, the actual selection process is a lot more nuanced. Let’s explore\nwhat it looks like.\nModel Selection Workflow\nWhen looking at models, it’s important to differentiate between hard\nattri",
        "chapter": 4
      },
      {
        "start": 4500,
        "text": "to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can th",
        "chapter": 4
      },
      {
        "start": 80500,
        "text": " need to think about how important each benchmark\nis to you and weigh their scores accordingly.\nAs you evaluate models using public benchmarks, keep in mind that the\ngoal of this process is to select a small subset of models to do more rigorous\nexperiments using your own benchmarks and metrics. This",
        "chapter": 4
      },
      {
        "start": 107000,
        "text": "ations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enabl",
        "chapter": 4
      },
      {
        "start": 87000,
        "text": "opic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confus",
        "chapter": 4
      },
      {
        "start": 38000,
        "text": "e role, you\nmight be able to create heuristics to evaluate the model’s outputs. For\nexample, if the role is someone who doesn’t talk a lot, a heuristic would be\nthe average of the model’s outputs. Other than that, the easiest automatic\nevaluation approach is AI as a judge. You should evaluate the ro",
        "chapter": 4
      },
      {
        "start": 97750,
        "text": "ion. Having human experts manually\nevaluate a model’s quality is a long-standing practice in AI. Given the\nchallenges of evaluating open-ended responses, many teams are looking at\nhuman evaluation as the North Star metric to guide their application\ndevelopment. Each day, you can use human experts to",
        "chapter": 4
      },
      {
        "start": 39500,
        "text": "to the role; the richer,\nthe better. (If the question contains\nreference answers, then the role−specific\nknowledge and memories are based on the\nreference answer. )\nCost and Latency\nA model that generates high-quality outputs but is too slow and expensive\nto run will not be useful. When evaluating m",
        "chapter": 4
      },
      {
        "start": 79750,
        "text": " the\nmodel you care about doesn’t have a publicly available score on your\n25\nbenchmark, you will need to run the evaluation yourself. Hopefully, an\nevaluation harness can help you with that. Running benchmarks can be\nexpensive. For example, Stanford spent approximately $80,000–$100,000\n26\nto evaluat",
        "chapter": 4
      },
      {
        "start": 0,
        "text": "Chapter 4. Evaluate AI Systems\nA model is only useful if it works for its intended purposes. You need to\nevaluate models in the context of your application. Chapter 3 discusses\ndifferent approaches to automatic evaluation. This chapter discusses how to\nuse these approaches to evaluate models for you",
        "chapter": 4
      }
    ],
    "input_tokens": 949,
    "output_tokens": 254,
    "input_cost": 0.00014235,
    "output_cost": 0.0001524,
    "total_cost": 0.00029475,
    "reference_answer": "Efficiency in AI models can be evaluated through metrics like runtime and memory usage, in addition to functional correctness, to ensure a model is not just accurate but also practical for real-world applications."
  },
  {
    "question": "what are open weights in AI models?",
    "answer": "In the context of AI models, \"open weights\" refer to the model weights that are made publicly available. These weights allow users to download and utilize the model without restrictions, even if the training data used to create the model is not publicly accessible. This distinguishes \"open weights\" from \"open models,\" the latter of which also includes publicly available training data.\n\nThe majority of open-source models currently available are classified as \"open weight only,\" meaning their weights are accessible while the training data may be kept private by the developers for various reasons.",
    "context": [
      {
        "start": 49000,
        "text": "auditing\npurposes, for example, to make sure that the model wasn’t trained on\n10\ncompromised or illegally acquired data.\nTo signal whether the data is also open, the term “open weight” is used for\nmodels that don’t come with open data, whereas the term “open model” is\nused for models that come with ",
        "chapter": 4
      },
      {
        "start": 49250,
        "text": "rm “open model” is\nused for models that come with open data.\nNOTE\nSome people argue that the term open source should be reserved only for fully open models. In this\nbook, for simplicity, I use open source to refer to all models whose weights are made public,\nregardless of their training data’s avail",
        "chapter": 4
      },
      {
        "start": 49500,
        "text": " public,\nregardless of their training data’s availability and licenses.\nAs of this writing, the vast majority of open source models are open weight\nonly. Model developers might hide training data information on purpose, as\nthis information can open model developers to public scrutiny and potential\nl",
        "chapter": 4
      },
      {
        "start": 48750,
        "text": "ch as retraining the model\nfrom scratch with modifications in the model architecture, training process,\nor the training data itself. Open data also makes it easier to understand the\nmodel. Some use cases also required access to the training data for auditing\npurposes, for example, to make sure that ",
        "chapter": 4
      },
      {
        "start": 48250,
        "text": "rce, open weight, and model licenses\nThe term “open source model” has become contentious. Originally, open\nsource was used to refer to any model that people can download and use.\nFor many use cases, being able to download the model is sufficient.\nHowever, some people argue that since a model’s perfo",
        "chapter": 4
      },
      {
        "start": 57000,
        "text": "mance but said nothing about the models’ training data other\nthan that “all data enrichment workers are paid at least a local living wage”.\nOpenAI’s CTO wasn’t able to provide a satisfactory answer when asked\nwhat data was used to train their models.\nOn top of that, the IP laws around AI are activel",
        "chapter": 4
      },
      {
        "start": 48500,
        "text": "ever, some people argue that since a model’s performance is largely a\nfunction of what data it was trained on, a model should be considered open\nonly if its training data is also made publicly available.\nOpen data allows more flexible model usage, such as retraining the model\nfrom scratch with modif",
        "chapter": 4
      },
      {
        "start": 48000,
        "text": "n open source model yourself. The answer to this\nquestion can significantly reduce your candidate model pool.\nLet’s first go into what exactly open source means when it comes to\nmodels, then discuss the pros and cons of these two approaches.\nOpen source, open weight, and model licenses\nThe term “ope",
        "chapter": 4
      },
      {
        "start": 45000,
        "text": "censes, training data, model size) or your own policies (privacy, control).\nFor some use cases, the hard attributes can reduce the pool of potential\nmodels significantly.\nSoft attributes are attributes that can be improved upon, such as accuracy,\ntoxicity, or factual consistency. When estimating how",
        "chapter": 4
      },
      {
        "start": 51500,
        "text": " a teacher (typically a much larger model). Mistral\ndidn’t allow this originally but later changed its license. As of this\n12\nwriting, the Llama licenses still don’t allow it.\nSome people use the term restricted weight to refer to open source models\nwith restricted licenses. However, I find this ter",
        "chapter": 4
      }
    ],
    "input_tokens": 987,
    "output_tokens": 109,
    "input_cost": 0.00014805,
    "output_cost": 6.54e-05,
    "total_cost": 0.00021345,
    "reference_answer": "Open weights refer to model weights that are publicly available, without necessarily providing access to the training data, which can be vital for auditing and ethical considerations."
  },
  {
    "question": "how to evaluate generated text quality",
    "answer": "To evaluate the quality of generated text, consider the following metrics:\n\n1. **Fluency**: This measures whether the text is grammatically correct and sounds natural, akin to something a fluent speaker would produce.\n\n2. **Coherence**: This assesses how well-structured the text is as a whole, ensuring it follows a logical sequence.\n\n3. **Task-specific Metrics**: Different tasks may require specific metrics. For example:\n   - **Faithfulness**: In translation tasks, this metric evaluates how accurately the generated translation reflects the original text.\n   - **Relevance**: In summarization tasks, it checks if the summary focuses on the most important aspects of the source document.\n\n4. **Perplexity**: This metric can indicate fluency and factual consistency.\n\n5. **Human Evaluation**: While automatic metrics are valuable, human evaluation remains crucial, especially in production scenarios. Expert assessments can identify nuances that automatic metrics might miss.\n\nUsing a combination of these automatic and human methods will provide a comprehensive evaluation of generated text quality.",
    "context": [
      {
        "start": 13000,
        "text": "eneration capabilities have improved, AI-generated texts have\nbecome nearly indistinguishable from human-generated texts. Fluency and\n2\ncoherence become less important. However, these metrics can still be\nuseful for weaker models or for applications involving creative writing and\nlow-resource langua",
        "chapter": 4
      },
      {
        "start": 11750,
        "text": " 2010s\nincluded translation, summarization, and paraphrasing.\nMetrics used to evaluate the quality of generated texts back then included\nfluency and coherence. Fluency measures whether the text is grammatically\ncorrect and natural-sounding (does this sound like something written by a\nfluent speaker?",
        "chapter": 4
      },
      {
        "start": 12250,
        "text": "hfulness:\nhow faithful is the generated translation to the original sentence? A metric\nthat a summarization task might use is relevance: does the summary focus\non the most important aspects of the source document? (Li et al., 2022).\nSome early NLG metrics, including faithfulness and relevance, have ",
        "chapter": 4
      },
      {
        "start": 12750,
        "text": "these issues became\nless important. In the 2010s, generated texts didn’t sound natural. They\nwere typically full of grammatical errors and awkward sentences. Fluency\nand coherence, then, were important metrics to track. However, as language\nmodels’ generation capabilities have improved, AI-generated",
        "chapter": 4
      },
      {
        "start": 97500,
        "text": "e\nused to evaluate a model’s perplexity for a generated text, which can be\nused for measurements such as fluency and factual consistency.\nUse automatic metrics as much as possible, but don’t be afraid to fall back\non human evaluation, even in production. Having human experts manually\nevaluate a mode",
        "chapter": 4
      },
      {
        "start": 19250,
        "text": "” (Lin et\nal., 2022) shows that their finetuned model GPT-judge is able to predict\nwhether a statement is considered truthful by humans with 90–96%\naccuracy. Here’s the prompt that Liu et al. (2023) used to evaluate the\nfactual consistency of a summary with respect to the original document:\nFactual ",
        "chapter": 4
      },
      {
        "start": 13250,
        "text": "involving creative writing and\nlow-resource languages. Fluency and coherence can be evaluated using AI\nas a judge—asking an AI model how fluent and coherent a text is—or using\nperplexity, as discussed in Chapter 3.\nGenerative models, with their new capabilities and new use cases, have new\nissues tha",
        "chapter": 4
      },
      {
        "start": 88250,
        "text": " exactly where your system fails. The first PDF-to-text step can be\nevaluated using similarity between the extracted text and the ground truth\ntext. The second step can be evaluated using accuracy: given the correctly\nextracted text, how often does the application correctly extract the current\nemplo",
        "chapter": 4
      },
      {
        "start": 12000,
        "text": " sound like something written by a\nfluent speaker?). Coherence measures how well-structured the whole text is\n(does it follow a logical structure?). Each task might also have its own\nmetrics. For example, a metric a translation task might use is faithfulness:\nhow faithful is the generated translatio",
        "chapter": 4
      },
      {
        "start": 34250,
        "text": " easily automated. If you instruct a\nmodel to “use language appropriate to a young audience”, how do you\nautomatically verify if the output is indeed appropriate for a young\naudience?\nFor verification, INFOBench authors constructed a list of criteria for each\ninstruction, each framed as a yes/no que",
        "chapter": 4
      }
    ],
    "input_tokens": 942,
    "output_tokens": 212,
    "input_cost": 0.0001413,
    "output_cost": 0.0001272,
    "total_cost": 0.0002685,
    "reference_answer": "The chapter outlines key evaluation metrics for generated text quality including fluency, coherence, and task-specific measures like faithfulness and relevance."
  },
  {
    "question": "how to reduce latency when using AI models",
    "answer": "To reduce latency when using AI models, consider the following strategies based on the context provided:\n\n1. **Set Latency Expectations**: Clearly define what latency is acceptable for your application. Filter out models that do not meet these requirements and choose the best among those that do.\n\n2. **Optimize Prompting**: Careful prompting can significantly control latency. Techniques include:\n   - Instructing the model to generate concise outputs.\n   - Setting stopping conditions for generation, helping to minimize the number of tokens generated.\n\n3. **Understand Latency Metrics**: Familiarize yourself with the different latency metrics that are important for your use case, such as:\n   - Time to first token\n   - Time per token\n   - Time between tokens\n   - Time per query\n\n4. **Evaluate Cost vs. Quality**: Balance model quality with latency and cost. Sometimes, opting for a lower-quality model can be beneficial if it results in improved cost-effectiveness and lower latency.\n\n5. **Evaluate Your Evaluation Pipeline**: Ensure that any evaluation processes in your application do not add unnecessary latency. While skipping evaluation can be risky, it’s crucial to be aware of its impact on latency.\n\nBy implementing these strategies, you can effectively manage and reduce latency when deploying AI models.",
    "context": [
      {
        "start": 41250,
        "text": "atency, it’s important to differentiate between the must-have and\nthe nice-to-have. If you ask users if they want lower latency, nobody will ever say no. But high\nlatency is often an annoyance, not a deal breaker.\nIf you use model APIs, they typically charge by tokens. The more input and\noutput toke",
        "chapter": 4
      },
      {
        "start": 40250,
        "text": "compromise on. For example,\nif latency is something you can’t compromise on, you start with latency\nexpectations for different models, filter out all the models that don’t meet\nyour latency requirements, and then pick the best among the rest.\nThere are multiple metrics for latency for foundation mod",
        "chapter": 4
      },
      {
        "start": 40750,
        "text": "ly on the underlying model but also on each prompt\nand sampling variables. Autoregressive language models typically generate\noutputs token by token. The more tokens it has to generate, the higher the\ntotal latency. You can control the total latency observed by users by careful\nprompting, such as ins",
        "chapter": 4
      },
      {
        "start": 40500,
        "text": "re multiple metrics for latency for foundation models, including but\nnot limited to time to first token, time per token, time between tokens, time\nper query, etc. It’s important to understand what latency metrics matter to\nyou.\nLatency depends not only on the underlying model but also on each prompt",
        "chapter": 4
      },
      {
        "start": 41000,
        "text": "bserved by users by careful\nprompting, such as instructing the model to be concise, setting a stopping\ncondition for generation (discussed in Chapter 2), or other optimization\ntechniques (discussed in Chapter 9).\nTIP\nWhen evaluating models based on latency, it’s important to differentiate between th",
        "chapter": 4
      },
      {
        "start": 39500,
        "text": "to the role; the richer,\nthe better. (If the question contains\nreference answers, then the role−specific\nknowledge and memories are based on the\nreference answer. )\nCost and Latency\nA model that generates high-quality outputs but is too slow and expensive\nto run will not be useful. When evaluating m",
        "chapter": 4
      },
      {
        "start": 39750,
        "text": "nsive\nto run will not be useful. When evaluating models, it’s important to balance\nmodel quality, latency, and cost. Many companies opt for lower-quality\nmodels if they provide better cost and latency. Cost and latency\noptimization are discussed in detail in Chapter 9, so this section will be\nquick.",
        "chapter": 4
      },
      {
        "start": 60500,
        "text": "ce can support your\napplication’s traffic while maintaining the desirable latency and cost.\nFunction calling: giving the model the ability to use external tools, which\nis essential for RAG and agentic use cases, as discussed in Chapter 6.\nStructured outputs, such as asking models to generate outputs",
        "chapter": 4
      },
      {
        "start": 60000,
        "text": "other reason that might cause open source models to lag behind is that\nopen source developers don’t receive feedback from users to improve their\nmodels, the way commercial models do. Once a model is open sourced,\nmodel developers have no idea how the model is being used, and how well\nthe model works",
        "chapter": 4
      },
      {
        "start": 106000,
        "text": "atency does your evaluation pipeline add to your\napplication?\nEvaluation, if not done carefully, can add significant latency and cost\nto your application. Some teams decide to skip evaluation in the hope\nof reducing latency. It’s a risky bet.\nIterate\nAs your needs and user behaviors change, your eva",
        "chapter": 4
      }
    ],
    "input_tokens": 954,
    "output_tokens": 257,
    "input_cost": 0.0001431,
    "output_cost": 0.0001542,
    "total_cost": 0.0002973,
    "reference_answer": "You can manage latency by optimizing prompts for conciseness, setting stopping conditions for generation, and employing other optimization techniques discussed in previous chapters."
  },
  {
    "question": "why do monopolies harm consumers?",
    "answer": "Monopolies harm consumers primarily because they result in lost consumer surplus due to higher prices and lower levels of output. In a monopolistic market, the lack of competition allows the monopoly to set prices above the equilibrium level that would exist in a competitive market. As a result, consumers pay more for fewer choices, leading to decreased overall welfare in the economy. This inefficiency is one of the primary reasons why governments regulate and discourage monopolistic practices.",
    "context": [
      {
        "start": 8750,
        "text": "sessment.\nHere’s an example of a multiple-choice question in the MMLU benchmark:\nQuestion: One of the reasons that the government discourages and\nregulates monopolies is that\n(A) Producer surplus is lost and consumer surplus is gained.\n(B) Monopoly prices ensure productive efficiency but cost societ",
        "chapter": 4
      },
      {
        "start": 95750,
        "text": "with social\nresponsibility. While an emphasis on stickiness and engagement metrics can\nlead to higher revenues, it may also cause a product to prioritize addictive\nfeatures or extreme content, which can be detrimental to users.\nStep 3. Define Evaluation Methods and Data\nNow that you’ve developed you",
        "chapter": 4
      },
      {
        "start": 66000,
        "text": "unintentional side effect of a fast-growing industry. I hope that this\nwill change as the industry matures.\nA less common situation that unfortunately exists is that a model provider\ncan stop supporting your use case, your industry, or your country, or your\ncountry can ban your model provider, as It",
        "chapter": 4
      },
      {
        "start": 64250,
        "text": ". When using a service provided by someone else,\nyou’re subject to their terms and conditions, and their rate limits. You can\naccess only what’s made available to you by this provider, and thus might\nnot be able to tweak the model as needed.\nTo protect their users and themselves from potential lawsu",
        "chapter": 4
      },
      {
        "start": 114750,
        "text": " are probably not the biggest companies. The\nbiggest companies might be important enough to service providers to negotiate favorable terms.\n9\nThis is similar to the philosophy in software infrastructure to always use the most popular tools that\nhave been extensively tested by the community.\n0\nWhen I",
        "chapter": 4
      },
      {
        "start": 9000,
        "text": "rices ensure productive efficiency but cost society\nallocative efficiency.\n(C) Monopoly firms do not engage in significant research and\ndevelopment.\n(D) Consumer surplus is lost with higher prices and lower levels of\noutput.\nLabel: (D)\nA multiple-choice question (MCQ) might have one or more correct\n",
        "chapter": 4
      },
      {
        "start": 64000,
        "text": "enterprises care about open source models. Image from the 2024 study by a16z.\nIf your business depends on a model, it’s understandable that you would\nwant some control over it, and API providers might not always give you the\nlevel of control you want. When using a service provided by someone else,\ny",
        "chapter": 4
      },
      {
        "start": 63750,
        "text": "d by others, who might have shared solutions online.\nControl, access, and transparency\nA 2024 study by a16z shows two key reasons that enterprises care about\nopen source models are control and customizability, as shown in Figure 4-8.\nFigure 4-8. Why enterprises care about open source models. Image f",
        "chapter": 4
      },
      {
        "start": 17000,
        "text": "ant meal of the day”. The\ninternet is flooded with misinformation: false marketing claims, statistics\nmade up to advance political agendas, and sensational, biased social media\nposts. In addition, it’s easy to fall for the absence of evidence fallacy. One\nmight take the statement “there’s no link be",
        "chapter": 4
      },
      {
        "start": 113750,
        "text": "ons, but\nwhat’s in it for model developers? Many companies have sprung up to capitalize on open source\nmodels by providing inference and finetuning services. It’s not a bad thing. Many people need these\nservices to leverage open source models. But, from model developers’ perspective, why invest\nmill",
        "chapter": 4
      }
    ],
    "input_tokens": 951,
    "output_tokens": 90,
    "input_cost": 0.00014265,
    "output_cost": 5.4e-05,
    "total_cost": 0.00019665,
    "reference_answer": "Monopolies lead to increased prices and reduced output, which results in a loss of consumer surplus as noted in option D of the question."
  },
  {
    "question": "How does data lineage affect AI model development?",
    "answer": "Data lineage affects AI model development in several key ways:\n\n1. **Transparency and Safety**: Data lineage provides clarity on the origins and transformations of the data used to train models. Companies may prefer open models with publicly available training data to ensure that the data is safe to use and can be inspected by the community. This transparency is crucial for maintaining trust in the AI systems.\n\n2. **Legal and Copyright Concerns**: Companies are increasingly cautious about data lineage due to potential copyright issues. Concerns around how data is used can lead organizations to choose fully open models or other approaches that allow for better scrutiny of the underlying data, thus avoiding legal complications.\n\n3. **Data Contamination Risks**: Understanding data lineage helps in identifying the risks of data contamination, such as when training and evaluation data come from the same source. This awareness can influence data collection strategies and training methodologies to ensure model integrity and performance.\n\n4. **Protecting Intellectual Property**: Data lineage informs companies about their intellectual property rights concerning the data used for model training. This is especially relevant for organizations reliant on proprietary content and technologies.\n\nIn summary, a well-defined data lineage can guide organizations in making informed decisions about data usage, compliance, and model selection, ultimately impacting the development and deployment of AI models.",
    "context": [
      {
        "start": 57750,
        "text": "nies whose\nexistence depends upon their IPs, such as gaming and movie studios, are\nhesitant to use AI to aid in the creation of their products, at least until IP\nlaws around AI are clarified (James Vincent, The Verge, November 15,\n2022).\nConcerns over data lineage have driven some companies toward f",
        "chapter": 4
      },
      {
        "start": 17500,
        "text": " light on how AI models process conflicting\ninformation and determine what the facts are. For example, Wan et al.\n(2024) found that existing “models rely heavily on the relevance of a\nwebsite to the query, while largely ignoring stylistic features that humans\nfind important such as whether a text co",
        "chapter": 4
      },
      {
        "start": 58750,
        "text": "el,\nthe contracts you sign with the model providers can potentially protect you\n16\nfrom data lineage risks.\nPerformance\nVarious benchmarks have shown that the gap between open source models\nand proprietary models is closing. Figure 4-7 shows this gap decreasing on\nthe MMLU benchmark over time. This ",
        "chapter": 4
      },
      {
        "start": 82500,
        "text": "aluate their new models.\nData contamination can happen indirectly, such as when both evaluation\nand training data come from the same source. For example, you might\ninclude math textbooks in the training data to improve the model’s math\ncapabilities, and someone else might use questions from the same",
        "chapter": 4
      },
      {
        "start": 56500,
        "text": "rizes 8% of its training set.\nThese memorized samples can be accidentally leaked to users or\nintentionally exploited by bad actors, as demonstrated in Chapter 5.\nData lineage and copyright\nData lineage and copyright concerns can steer a company in many\ndirections: toward open source models, toward p",
        "chapter": 4
      },
      {
        "start": 67000,
        "text": "Have to send your Don’t have to send your\ndata to model data externally\nproviders, which Fewer checks and\nmeans your team can balances for data\naccidentally leak lineage/training data\nconfidential info copyright\nPerformance\nBest-performing The best open source\nmodel will likely be models will likely",
        "chapter": 4
      },
      {
        "start": 58000,
        "text": "r data lineage have driven some companies toward fully open\nmodels, whose training data has been made publicly available. The\nargument is that this allows the community to inspect the data and make\nsure that it’s safe to use. While it sounds great in theory, in practice, it’s\nchallenging for any com",
        "chapter": 4
      },
      {
        "start": 57000,
        "text": "mance but said nothing about the models’ training data other\nthan that “all data enrichment workers are paid at least a local living wage”.\nOpenAI’s CTO wasn’t able to provide a satisfactory answer when asked\nwhat data was used to train their models.\nOn top of that, the IP laws around AI are activel",
        "chapter": 4
      },
      {
        "start": 500,
        "text": "e, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-specific\ncapabilities like math, science, reasoning, and summarization measured?\nThe second part focuses on model selection. Given an increasing number of\nfoundation models to choose from, it can feel o",
        "chapter": 4
      },
      {
        "start": 56250,
        "text": "the problem with people using your data to train their models?\nWhile research in this area is still sparse, some studies suggest that AI\nmodels can memorize their training samples. For example, it’s been found\nthat Hugging Face’s StarCoder model memorizes 8% of its training set.\nThese memorized samp",
        "chapter": 4
      }
    ],
    "input_tokens": 954,
    "output_tokens": 257,
    "input_cost": 0.0001431,
    "output_cost": 0.0001542,
    "total_cost": 0.0002973,
    "reference_answer": "Data lineage influences decisions about adopting open source versus proprietary models, alongside navigating copyright concerns related to data usage in AI training."
  },
  {
    "question": "open source vs proprietary model performance",
    "answer": "Based on the context provided from the AI Engineering book, here’s a comparative overview of open source versus proprietary model performance:\n\n1. **Performance Gap**: Open source models are often perceived to lag behind the strongest proprietary models. While there is a belief that the performance gap might close over time, as of now, proprietary models are generally more robust and performant, especially for demanding use cases.\n\n2. **Use Cases**: Many use cases exist where open source models may be sufficient. For tasks that do not require the absolute best performance, open source models can provide viable solutions.\n\n3. **User Feedback and Iteration**: Proprietary models benefit from ongoing user feedback, which aids in continuous improvement and refinement. In contrast, once an open source model is released, developers lack insights about usage and performance, making it challenging to iterate effectively.\n\n4. **Accessibility and Manipulation**: Proprietary models may be easier to start with and scale, providing a simplified user experience. However, open source models allow for more accessibility in terms of manipulation and customization, which can be beneficial in specific scenarios.\n\n5. **Legal and Resource Constraints**: Open source models often have less legal support than proprietary models, which can affect their application in commercial environments.\n\n6. **Trend Observations**: There are indications that the performance of open source models is improving, as evidenced by decreasing gaps in specific benchmarks (like MMLU). Nonetheless, there’s skepticism as to whether they will completely match the performance of proprietary models.\n\nIn summary, while proprietary models currently offer superior performance owing to better resources and user feedback mechanisms, open source models remain valuable for various can use cases, particularly due to their flexibility and customizability.",
    "context": [
      {
        "start": 59750,
        "text": "or this reason, it’s likely that the strongest open source model will lag\nbehind the strongest proprietary models for the foreseeable future.\nHowever, for many use cases that don’t need the strongest models, open\nsource models might be sufficient.\nAnother reason that might cause open source models t",
        "chapter": 4
      },
      {
        "start": 63000,
        "text": "hat.\nIn general, you want a model that is easy to use and manipulate. Typically,\nproprietary models are easier to get started with and scale, but open models\nmight be easier to manipulate as their components are more accessible.\nRegardless of whether you go with open or proprietary models, you want\n",
        "chapter": 4
      },
      {
        "start": 59000,
        "text": " decreasing on\nthe MMLU benchmark over time. This trend has made many people believe\nthat one day, there will be an open source model that performs just as well,\nif not better, than the strongest proprietary model.\nAs much as I want open source models to catch up with proprietary models,\nI don’t thi",
        "chapter": 4
      },
      {
        "start": 60000,
        "text": "other reason that might cause open source models to lag behind is that\nopen source developers don’t receive feedback from users to improve their\nmodels, the way commercial models do. Once a model is open sourced,\nmodel developers have no idea how the model is being used, and how well\nthe model works",
        "chapter": 4
      },
      {
        "start": 48000,
        "text": "n open source model yourself. The answer to this\nquestion can significantly reduce your candidate model pool.\nLet’s first go into what exactly open source means when it comes to\nmodels, then discuss the pros and cons of these two approaches.\nOpen source, open weight, and model licenses\nThe term “ope",
        "chapter": 4
      },
      {
        "start": 63750,
        "text": "d by others, who might have shared solutions online.\nControl, access, and transparency\nA 2024 study by a16z shows two key reasons that enterprises care about\nopen source models are control and customizability, as shown in Figure 4-8.\nFigure 4-8. Why enterprises care about open source models. Image f",
        "chapter": 4
      },
      {
        "start": 58250,
        "text": " theory, in practice, it’s\nchallenging for any company to thoroughly inspect a dataset of the size\ntypically used to train foundation models.\nGiven the same concern, many companies opt for commercial models\ninstead. Open source models tend to have limited legal resources compared\nto commercial model",
        "chapter": 4
      },
      {
        "start": 59500,
        "text": "It’s a common practice for\ncompanies to keep their strongest models behind APIs and open source their\nweaker models.\nFigure 4-7. The gap between open source models and proprietary models is decreasing on the\nMMLU benchmark. Image by Maxime Labonne.\nFor this reason, it’s likely that the strongest ope",
        "chapter": 4
      },
      {
        "start": 63250,
        "text": " you go with open or proprietary models, you want\nthis model to follow a standard API, which makes it easier to swap models.\nMany model developers try to make their models mimic the API of the most\npopular models. As of this writing, many API providers mimic OpenAI’s\nAPI.\nYou might also prefer model",
        "chapter": 4
      },
      {
        "start": 47000,
        "text": "host open source models. However, after public\nand private evaluation, you might realize that open source models can’t\nachieve the level of performance you want and have to switch to\ncommercial APIs.\nChapter 10 discusses monitoring and collecting user feedback. The rest of\nthis chapter will discuss ",
        "chapter": 4
      }
    ],
    "input_tokens": 951,
    "output_tokens": 344,
    "input_cost": 0.00014265,
    "output_cost": 0.0002064,
    "total_cost": 0.00034905,
    "reference_answer": "The chapter outlines that while the performance gap between open source models and proprietary models is decreasing, strong proprietary models are likely to remain superior. Open source models may still be viable for many applications despite this lag."
  },
  {
    "question": "techniques for detecting factual inconsistency in language models",
    "answer": "In the context of detecting factual inconsistencies in language models, two main techniques are highlighted:\n\n1. **Self-verification**: This approach involves the model checking its own outputs. For example, if a model generates multiple outputs that contradict each other, it signals a potential factual inconsistency. This method relies on the assumption that conflicting outputs indicate a lack of factual accuracy, as exemplified by SelfCheckGPT.\n\n2. **Knowledge-augmented verification**: This technique, demonstrated by SAFE (Search-Augmented Factuality Evaluator), evaluates model outputs against a broader set of open knowledge. The output is considered factually consistent if it aligns with widely accepted facts. For instance, if the model claims \"the sky is blue\" and this is a commonly accepted fact, the output would be seen as consistent. Conversely, if it contradicts established facts, it would be flagged as inconsistent.\n\nAdditionally, these techniques can be categorized into two settings: local factual consistency (evaluating against specific context) and global factual consistency (evaluating against general knowledge). Local consistency checks allow for specific context-based evaluations, while global checks assess the general factual accuracy of the statements made by the model.",
    "context": [
      {
        "start": 19750,
        "text": "\nAnswer:\nMore sophisticated AI as a judge techniques to evaluate factual consistency\nare self-verification and knowledge-augmented verification:\nSelf-verification\nSelfCheckGPT (Manakul et al., 2023) relies on an assumption that if\na model generates multiple outputs that disagree with one another,\nth",
        "chapter": 4
      },
      {
        "start": 15000,
        "text": "der two\nsettings: against explicitly provided facts (context) or against open\nknowledge:\nLocal factual consistency\nThe output is evaluated against a context. The output is considered\nfactually consistent if it’s supported by the given context. For\nexample, if the model outputs “the sky is blue” and ",
        "chapter": 4
      },
      {
        "start": 72500,
        "text": "ng,\nfocusing on a model’s understanding of facts.\n5. WinoGrande (Sakaguchi et al., 2019): Measuring the ability to solve\nchallenging pronoun resolution problems that are designed to be difficult\nfor language models, requiring sophisticated commonsense reasoning.\n6. GSM-8K (Grade School Math, OpenAI,",
        "chapter": 4
      },
      {
        "start": 15250,
        "text": "ample, if the model outputs “the sky is blue” and the given context\nsays that the sky is purple, this output is considered factually\ninconsistent. Conversely, given this context, if the model outputs “the\nsky is purple”, this output is factually consistent.\nLocal factual consistency is important for",
        "chapter": 4
      },
      {
        "start": 17500,
        "text": " light on how AI models process conflicting\ninformation and determine what the facts are. For example, Wan et al.\n(2024) found that existing “models rely heavily on the relevance of a\nwebsite to the query, while largely ignoring stylistic features that humans\nfind important such as whether a text co",
        "chapter": 4
      },
      {
        "start": 34000,
        "text": " to follow content constraints (such as “discuss only climate\nchange”), linguistic guidelines (such as “use Victorian English”), and style\nrules (such as “use a respectful tone”). However, the verification of these\nexpanded instruction types can’t be easily automated. If you instruct a\nmodel to “use",
        "chapter": 4
      },
      {
        "start": 20250,
        "text": "orks but can be prohibitively expensive, as it requires\nmany AI queries to evaluate a response.\nKnowledge-augmented verification\nSAFE, Search-Augmented Factuality Evaluator, introduced by\nGoogle DeepMind (Wei et al., 2024) in the paper “Long-Form\nFactuality in Large Language Models”, works by levera",
        "chapter": 4
      },
      {
        "start": 22000,
        "text": " implies factual\ninconsistency, and neutral implies that consistency can’t be determined.\nInstead of using general-purpose AI judges, you can train scorers\nspecialized in factual consistency prediction. These scorers take in a pair of\n(premise, hypothesis) as input and output one of the predefined c",
        "chapter": 4
      },
      {
        "start": 34250,
        "text": " easily automated. If you instruct a\nmodel to “use language appropriate to a young audience”, how do you\nautomatically verify if the output is indeed appropriate for a young\naudience?\nFor verification, INFOBench authors constructed a list of criteria for each\ninstruction, each framed as a yes/no que",
        "chapter": 4
      },
      {
        "start": 15750,
        "text": "’s policies), and\nbusiness analysis (the extracted insights should be consistent with the\ndata).\nGlobal factual consistency\nThe output is evaluated against open knowledge. If the model\noutputs “the sky is blue” and it’s a commonly accepted fact that the\nsky is blue, this statement is considered fact",
        "chapter": 4
      }
    ],
    "input_tokens": 1002,
    "output_tokens": 238,
    "input_cost": 0.0001503,
    "output_cost": 0.0001428,
    "total_cost": 0.0002931,
    "reference_answer": "The chapter introduces various techniques developed to detect factual inconsistency, while noting that comprehensive coverage of these methods is beyond the scope of the chapter."
  },
  {
    "question": "What are the implications of a model's data lineage in AI training?",
    "answer": "The implications of a model's data lineage in AI training are multi-faceted:\n\n1. **Data Contamination Risks**: Data lineage can highlight risks of data contamination, where both training and evaluation data originate from the same source. This can lead to biased outcomes or unreliable model performance.\n\n2. **Intellectual Property Concerns**: Data lineage intersects with copyright issues, compelling companies to navigate toward open-source models where training data transparency is prioritized. This raises concerns about the safety and legality of using certain datasets.\n\n3. **Regulatory and Legal Implications**: The need for clear IP laws surrounding AI influences companies, particularly those reliant on proprietary data, to be cautious about using AI in product creation until such frameworks are established.\n\n4. **Model Leakage Risks**: Models can unintentionally memorize and leak specific training data, posing security threats. This emphasizes the importance of understanding the origins and handling of training data.\n\n5. **Community Trust and Scrutiny**: Open models that disclose their training data enable community inspection, which theoretically enhances safety and trust but presents practical challenges in ensuring the data’s reliability.\n\nOverall, scrutinizing data lineage is crucial for maintaining ethical standards, ensuring legal compliance, and fulfilling safety requirements in AI development.",
    "context": [
      {
        "start": 56500,
        "text": "rizes 8% of its training set.\nThese memorized samples can be accidentally leaked to users or\nintentionally exploited by bad actors, as demonstrated in Chapter 5.\nData lineage and copyright\nData lineage and copyright concerns can steer a company in many\ndirections: toward open source models, toward p",
        "chapter": 4
      },
      {
        "start": 82500,
        "text": "aluate their new models.\nData contamination can happen indirectly, such as when both evaluation\nand training data come from the same source. For example, you might\ninclude math textbooks in the training data to improve the model’s math\ncapabilities, and someone else might use questions from the same",
        "chapter": 4
      },
      {
        "start": 57000,
        "text": "mance but said nothing about the models’ training data other\nthan that “all data enrichment workers are paid at least a local living wage”.\nOpenAI’s CTO wasn’t able to provide a satisfactory answer when asked\nwhat data was used to train their models.\nOn top of that, the IP laws around AI are activel",
        "chapter": 4
      },
      {
        "start": 58000,
        "text": "r data lineage have driven some companies toward fully open\nmodels, whose training data has been made publicly available. The\nargument is that this allows the community to inspect the data and make\nsure that it’s safe to use. While it sounds great in theory, in practice, it’s\nchallenging for any com",
        "chapter": 4
      },
      {
        "start": 57750,
        "text": "nies whose\nexistence depends upon their IPs, such as gaming and movie studios, are\nhesitant to use AI to aid in the creation of their products, at least until IP\nlaws around AI are clarified (James Vincent, The Verge, November 15,\n2022).\nConcerns over data lineage have driven some companies toward f",
        "chapter": 4
      },
      {
        "start": 56250,
        "text": "the problem with people using your data to train their models?\nWhile research in this area is still sparse, some studies suggest that AI\nmodels can memorize their training samples. For example, it’s been found\nthat Hugging Face’s StarCoder model memorizes 8% of its training set.\nThese memorized samp",
        "chapter": 4
      },
      {
        "start": 58750,
        "text": "el,\nthe contracts you sign with the model providers can potentially protect you\n16\nfrom data lineage risks.\nPerformance\nVarious benchmarks have shown that the gap between open source models\nand proprietary models is closing. Figure 4-7 shows this gap decreasing on\nthe MMLU benchmark over time. This ",
        "chapter": 4
      },
      {
        "start": 67000,
        "text": "Have to send your Don’t have to send your\ndata to model data externally\nproviders, which Fewer checks and\nmeans your team can balances for data\naccidentally leak lineage/training data\nconfidential info copyright\nPerformance\nBest-performing The best open source\nmodel will likely be models will likely",
        "chapter": 4
      },
      {
        "start": 17500,
        "text": " light on how AI models process conflicting\ninformation and determine what the facts are. For example, Wan et al.\n(2024) found that existing “models rely heavily on the relevance of a\nwebsite to the query, while largely ignoring stylistic features that humans\nfind important such as whether a text co",
        "chapter": 4
      },
      {
        "start": 500,
        "text": "e, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-specific\ncapabilities like math, science, reasoning, and summarization measured?\nThe second part focuses on model selection. Given an increasing number of\nfoundation models to choose from, it can feel o",
        "chapter": 4
      }
    ],
    "input_tokens": 959,
    "output_tokens": 247,
    "input_cost": 0.00014385,
    "output_cost": 0.0001482,
    "total_cost": 0.00029205,
    "reference_answer": "Understanding a model's data lineage is critical as it affects compliance with licensing terms, particularly regarding whether a model can be trained on outputs from another model without violating rules."
  },
  {
    "question": "best practices for reporting model performance",
    "answer": "When reporting model performance, it’s essential to follow these best practices:\n\n1. **Disclose Data Overlap**: Clearly state what percentage of the benchmark data was included in the training set. This transparency helps others understand the model's generalizability.\n\n2. **Report on Overall and Clean Samples**: Include performance metrics for both the overall benchmark and specifically for the clean samples. This distinction can highlight the model's robustness and effectiveness under different conditions.\n\n3. **Contextual Evaluation**: Consider the importance of each benchmark for the specific application and weigh their scores accordingly. Not all benchmarks may hold equal significance for every use case.\n\n4. **Remove Contaminated Samples**: Before training, it is a common practice to remove benchmarks of interest from the training data. This ensures that model evaluation is not biased by prior exposure to the benchmark data.\n\n5. **Rigorous Custom Evaluations**: After narrowing down model options using public benchmarks, conduct a custom evaluation using your own metrics and benchmarks tailored to your application. \n\nThese steps enhance the credibility and utility of reported performance metrics, facilitating better decision-making for model adoption and deployment.",
    "context": [
      {
        "start": 85500,
        "text": " training their models. Ideally,\nwhen reporting your model performance on a benchmark, it’s helpful to\ndisclose what percentage of this benchmark data is in your training data,\nand what the model’s performance is on both the overall benchmark and the\nclean samples of the benchmark. Sadly, because de",
        "chapter": 4
      },
      {
        "start": 44500,
        "text": "els along the cost–performance axes and choosing the\nmodel that gives the best performance for your bucks\nHowever, the actual selection process is a lot more nuanced. Let’s explore\nwhat it looks like.\nModel Selection Workflow\nWhen looking at models, it’s important to differentiate between hard\nattri",
        "chapter": 4
      },
      {
        "start": 80500,
        "text": " need to think about how important each benchmark\nis to you and weigh their scores accordingly.\nAs you evaluate models using public benchmarks, keep in mind that the\ngoal of this process is to select a small subset of models to do more rigorous\nexperiments using your own benchmarks and metrics. This",
        "chapter": 4
      },
      {
        "start": 43500,
        "text": "your criteria, let’s move on to the next step and use them\nto select the best model for your application.\nModel Selection\nAt the end of the day, you don’t really care about which model is the best.\nYou care about which model is the best for your applications. Once you’ve\ndefined the criteria for you",
        "chapter": 4
      },
      {
        "start": 85250,
        "text": "mance. Besides, there will always be benchmarks\ncreated after models are trained, so there will always be contaminated\nevaluation samples.\nFor model developers, a common practice is to remove benchmarks they\ncare about from their training data before training their models. Ideally,\nwhen reporting yo",
        "chapter": 4
      },
      {
        "start": 83000,
        "text": "your users. Initially, you\nexclude benchmark data from the model’s training data and choose the best\nmodel based on these benchmarks. However, because high-quality\nbenchmark data can improve the model’s performance, you then continue\ntraining your best model on benchmark data before releasing it to ",
        "chapter": 4
      },
      {
        "start": 86750,
        "text": " for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success ",
        "chapter": 4
      },
      {
        "start": 41000,
        "text": "bserved by users by careful\nprompting, such as instructing the model to be concise, setting a stopping\ncondition for generation (discussed in Chapter 2), or other optimization\ntechniques (discussed in Chapter 9).\nTIP\nWhen evaluating models based on latency, it’s important to differentiate between th",
        "chapter": 4
      },
      {
        "start": 47250,
        "text": "r feedback. The rest of\nthis chapter will discuss the first three steps. First, let’s discuss a question\nthat most teams will visit more than once: to use model APIs or to host\nmodels themselves. We’ll then continue to how to navigate the dizzying\nnumber of public benchmarks and why you can’t trust ",
        "chapter": 4
      },
      {
        "start": 1000,
        "text": "select what benchmarks to use? How\nabout public leaderboards that aggregate multiple benchmarks?\nThe model landscape is teeming with proprietary models and open source\nmodels. A question many teams will need to visit over and over again is\nwhether to host their own models or to use a model API. This",
        "chapter": 4
      }
    ],
    "input_tokens": 942,
    "output_tokens": 229,
    "input_cost": 0.0001413,
    "output_cost": 0.0001374,
    "total_cost": 0.0002787,
    "reference_answer": "The chapter emphasizes the importance of disclosing the percentage of benchmark data used in training when reporting model performance, along with results on overall and clean samples of the benchmark."
  },
  {
    "question": "advanced techniques for evaluating LLMs",
    "answer": "Evaluating large language models (LLMs) involves several advanced techniques that leverage various benchmarks and tailored evaluation pipelines. Here’s a summary based on the context:\n\n1. **Use of Multiple Benchmarks**: Recent approaches, such as the update to Hugging Face's Open LLM Leaderboard, aggregate scores from multiple benchmarks to provide a comprehensive evaluation. Key benchmarks include:\n   - **ARC-C**: Evaluates the ability to solve complex grade school-level science questions.\n   - **MMLU**: Assesses a model's knowledge and reasoning capabilities.\n\n2. **Evaluation Harnesses**: Tools like EleutherAI’s lm-evaluation-harness and OpenAI’s evals support numerous benchmarks, allowing for robust comparative evaluations.\n\n3. **Custom Evaluation Pipelines**: After narrowing down potential models using public benchmarks, it is crucial to implement a customized evaluation pipeline. This pipeline should incorporate prompts and metrics tailored to specific application needs, given that public benchmarks can sometimes be misleading.\n\n4. **Instruction-Following Benchmarks**: The IFEval benchmark, for example, focuses on whether models output responses that adhere to expected formats, which is vital for practical applications.\n\n5. **Component Evaluation**: It's essential to evaluate all components in a system to comprehensively understand their performance and how they contribute to the overall model effectiveness.\n\n6. **Ongoing Monitoring and Feedback**: Continuous monitoring of the model's performance in production helps identify failure points and gather user feedback to inform model improvements.\n\nBy integrating these techniques, practitioners can better assess and enhance the effectiveness of LLMs in their specific contexts.",
    "context": [
      {
        "start": 71750,
        "text": " in late 2023, Hugging Face updated their Open LLM Leaderboard\nto use the average of six different benchmarks to rank models:\n1. ARC-C (Clark et al., 2018): Measuring the ability to solve complex,\ngrade school-level science questions.\n2. MMLU (Hendrycks et al., 2020): Measuring knowledge and reasoni",
        "chapter": 4
      },
      {
        "start": 69000,
        "text": "s\nsaturate, necessitating the introduction of new benchmarks.\nA tool that helps you evaluate a model on multiple benchmarks is an\nevaluation harness. As of this writing, EleutherAI’s lm-evaluation-harness\nsupports over 400 benchmarks. OpenAI’s evals lets you run any of the\napproximately 500 existing",
        "chapter": 4
      },
      {
        "start": 46500,
        "text": "3. Run experiments with your own evaluation pipeline to find the best\nmodel, again, balancing all your objectives.\n4. Continually monitor your model in production to detect failure and\ncollect feedback to improve your application.\nFigure 4-5. An overview of the evaluation workflow to evaluate models",
        "chapter": 4
      },
      {
        "start": 87250,
        "text": "valuation\nmethods and techniques, it can be confusing to pick the right combination\nfor your evaluation pipeline. This section focuses on evaluating open-ended\ntasks. Evaluating close-ended tasks is easier, and its pipeline can be inferred\nfrom this process.\nStep 1. Evaluate All Components in a Syst",
        "chapter": 4
      },
      {
        "start": 30750,
        "text": "and what evaluation methods are appropriate.\nThe Google benchmark IFEval, Instruction-Following Evaluation, focuses\non whether the model can produce outputs following an expected format.\nZhou et al. (2023) identified 25 types of instructions that can be\nautomatically verified, such as keyword inclus",
        "chapter": 4
      },
      {
        "start": 86750,
        "text": " for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success ",
        "chapter": 4
      },
      {
        "start": 47500,
        "text": "mber of public benchmarks and why you can’t trust them. This will set\nthe stage for the last section in the chapter. Because public benchmarks\ncan’t be trusted, you need to design your own evaluation pipeline with\nprompts and metrics you can trust.\nModel Build Versus Buy\nAn evergreen question for co",
        "chapter": 4
      },
      {
        "start": 8250,
        "text": "tions and let it pick the correct one. If the expected answer is option C\nand the model outputs option A, the model is wrong.\nThis is the approach that most public benchmarks follow. In April 2024,\n75% of the tasks in Eleuther’s lm-evaluation-harness are multiple-choice,\nincluding UC Berkeley’s MMLU",
        "chapter": 4
      },
      {
        "start": 30500,
        "text": "and\nINFOBench, measure models’ capability to follow a wide range of\ninstructions, which are to give you ideas on how to evaluate a model’s\nability to follow your instructions: what criteria to use, what instructions to\ninclude in the evaluation set, and what evaluation methods are appropriate.\nThe G",
        "chapter": 4
      },
      {
        "start": 75500,
        "text": "23): a graduate-level Q&A benchmark\nMuSR (Sprague et al., 2023): a chain-of-thought, multistep reasoning benchmark\nBBH (BIG-bench Hard) (Srivastava et al., 2023): another reasoning benchmark\nIFEval (Zhou et al., 2023): an instruction-following benchmark\nI have no doubt that these benchmarks will soo",
        "chapter": 4
      }
    ],
    "input_tokens": 986,
    "output_tokens": 323,
    "input_cost": 0.0001479,
    "output_cost": 0.0001938,
    "total_cost": 0.0003417,
    "reference_answer": "The text suggests using diverse evaluation criteria, including out-of-scope inputs and error-prone examples, to rigorously assess the robustness and reliability of large language models."
  },
  {
    "question": "how to evaluate AI applications",
    "answer": "To evaluate AI applications effectively, follow these steps based on the guidelines provided in Chip Huyen's \"AI Engineering\":\n\n1. **Define Evaluation Criteria**: Start by establishing a clear list of evaluation criteria tailored to your specific application. These criteria should align with the goals of the application and can include factors such as accuracy, reliability, and user satisfaction.\n\n2. **Develop an Evaluation Pipeline**: A robust evaluation pipeline is essential. This should differentiate between good and bad outcomes, using various evaluation methods and metrics. It’s important to design this pipeline thoughtfully, as it will determine the reliability of your evaluation process.\n\n3. **Evaluate Various System Components**: Since AI applications can be complex and consist of multiple components, evaluate each at different levels—per task, per interaction step (turn), and for intermediate outputs. This multifaceted evaluation will help in pinpointing issues more effectively.\n\n4. **Consider Business Metrics**: Align your evaluation metrics with business objectives. This could involve monitoring user engagement metrics (like DAU, WAU, MAU) or other relevant performance indicators that are crucial for the application’s success.\n\n5. **Incorporate Human Evaluation**: For open-ended responses, consider using human experts to evaluate outputs. This qualitative assessment can serve as a North Star metric and help in identifying any significant performance changes or unusual usage patterns over time.\n\n6. **Regular Monitoring and Adjustment**: Establish a regular schedule for evaluation, incorporating both automated methods and human assessments to maintain a check on application performance and adapt to evolving needs.\n\nBy systematically applying these strategies, you can better assess and enhance the performance of your AI applications.",
    "context": [
      {
        "start": 4500,
        "text": "to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can th",
        "chapter": 4
      },
      {
        "start": 87500,
        "text": "process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each application might consist of\nmany components, and a task might be completed after many turns.\nEvaluation can happen at different levels: per task, per turn, and per\nintermediate output.\nYou should evalu",
        "chapter": 4
      },
      {
        "start": 107000,
        "text": "ations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enabl",
        "chapter": 4
      },
      {
        "start": 87000,
        "text": "opic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confus",
        "chapter": 4
      },
      {
        "start": 95000,
        "text": ": what scores must an\napplication achieve for it to be useful? For example, you might determine\nthat your chatbot’s factual consistency score must be at least 50% for it to\nbe useful. Anything below this makes it unusable even for general customer\nrequests.\nBefore developing AI evaluation metrics, i",
        "chapter": 4
      },
      {
        "start": 250,
        "text": "to\nuse these approaches to evaluate models for your applications.\nThis chapter contains three parts. It starts with a discussion of the criteria\nyou might use to evaluate your applications and how these criteria are\ndefined and calculated. For example, many people worry about AI making\nup facts—how ",
        "chapter": 4
      },
      {
        "start": 95250,
        "text": "quests.\nBefore developing AI evaluation metrics, it’s crucial to first understand the\nbusiness metrics you’re targeting. Many applications focus on stickiness\nmetrics, such as daily, weekly, or monthly active users (DAU, WAU,\nMAU). Others prioritize engagement metrics, like the number of\nconversatio",
        "chapter": 4
      },
      {
        "start": 97750,
        "text": "ion. Having human experts manually\nevaluate a model’s quality is a long-standing practice in AI. Given the\nchallenges of evaluating open-ended responses, many teams are looking at\nhuman evaluation as the North Star metric to guide their application\ndevelopment. Each day, you can use human experts to",
        "chapter": 4
      },
      {
        "start": 0,
        "text": "Chapter 4. Evaluate AI Systems\nA model is only useful if it works for its intended purposes. You need to\nevaluate models in the context of your application. Chapter 3 discusses\ndifferent approaches to automatic evaluation. This chapter discusses how to\nuse these approaches to evaluate models for you",
        "chapter": 4
      },
      {
        "start": 98000,
        "text": "evelopment. Each day, you can use human experts to evaluate a subset of\nyour application’s outputs that day to detect any changes in the application’s\nperformance or unusual patterns in usage. For example, LinkedIn developed\na process to manually evaluate up to 500 daily conservations with their AI\n",
        "chapter": 4
      }
    ],
    "input_tokens": 921,
    "output_tokens": 328,
    "input_cost": 0.00013815,
    "output_cost": 0.0001968,
    "total_cost": 0.00033495,
    "reference_answer": "The chapter highlights the importance of establishing specific evaluation criteria for AI applications, focusing on domain-specific capability, generation capability, instruction-following capability, and cost and latency to guide effective evaluations."
  },
  {
    "question": "mean win rate in model evaluation",
    "answer": "In the context of model evaluation as discussed in the \"AI Engineering\" book by Chip Huyen, the mean win rate is defined as the fraction of times a model scores better than another model, averaged across multiple comparisons. This approach is favored over simple averaging of scores because it provides a more nuanced view of relative performance, particularly in scenarios where the benchmarks may have different scales or units. The mean win rate helps to highlight which models consistently outperform others, which can be more revealing than absolute scores that may vary significantly depending on the specific task or benchmark being used.",
    "context": [
      {
        "start": 77250,
        "text": ",\nfor some tasks, truthfulness might weigh a lot more than being able to solve\ngrade school math problems.\nHELM authors, on the other hand, decided to shun averaging in favor of\nmean win rate, which they defined as “the fraction of times a model obtains\na better score than another model, averaged ac",
        "chapter": 4
      },
      {
        "start": 79750,
        "text": " the\nmodel you care about doesn’t have a publicly available score on your\n25\nbenchmark, you will need to run the evaluation yourself. Hopefully, an\nevaluation harness can help you with that. Running benchmarks can be\nexpensive. For example, Stanford spent approximately $80,000–$100,000\n26\nto evaluat",
        "chapter": 4
      },
      {
        "start": 79250,
        "text": "intentionally release worse models, what\nmight be the reason for this perception? One potential reason is that\nevaluation is hard, and no one, not even OpenAI, knows for sure if a model\nis getting better or worse. While evaluation is definitely hard, I doubt that\n24\nOpenAI would fly completely blind",
        "chapter": 4
      },
      {
        "start": 78000,
        "text": "rboards with public benchmarks\nWhen evaluating models for a specific application, you’re basically creating\na private leaderboard that ranks models based on your evaluation criteria.\nThe first step is to gather a list of benchmarks that evaluate the capabilities\nimportant to your application. If you",
        "chapter": 4
      },
      {
        "start": 80250,
        "text": "e scores for the\nmodels you care about on these benchmarks, you then need to aggregate\nthese scores to rank models. Not all benchmark scores are in the same unit\nor scale. One benchmark might use accuracy, another F1, and another\nBLEU score. You will need to think about how important each benchmark\n",
        "chapter": 4
      },
      {
        "start": 80500,
        "text": " need to think about how important each benchmark\nis to you and weigh their scores accordingly.\nAs you evaluate models using public benchmarks, keep in mind that the\ngoal of this process is to select a small subset of models to do more rigorous\nexperiments using your own benchmarks and metrics. This",
        "chapter": 4
      },
      {
        "start": 86750,
        "text": " for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success ",
        "chapter": 4
      },
      {
        "start": 102500,
        "text": " see if they give similar evaluation results.\nBasically, you want to know that if you evaluate the model on a different\nevaluation set of 100 examples, would you get a different result? If you get\n90% on one bootstrap but 70% on another bootstrap, your evaluation\npipeline isn’t that trustworthy.\nCon",
        "chapter": 4
      },
      {
        "start": 103750,
        "text": "ibution.\nTIP\nOpenAI suggested a rough estimation of the number of evaluation samples needed to be certain that\none system is better, given a score difference, as shown in Table 4-7. A useful rule is that for every 3×\n28\ndecrease in score difference, the number of samples needed increases 10×.\nTable ",
        "chapter": 4
      },
      {
        "start": 71750,
        "text": " in late 2023, Hugging Face updated their Open LLM Leaderboard\nto use the average of six different benchmarks to rank models:\n1. ARC-C (Clark et al., 2018): Measuring the ability to solve complex,\ngrade school-level science questions.\n2. MMLU (Hendrycks et al., 2020): Measuring knowledge and reasoni",
        "chapter": 4
      }
    ],
    "input_tokens": 968,
    "output_tokens": 113,
    "input_cost": 0.0001452,
    "output_cost": 6.78e-05,
    "total_cost": 0.000213,
    "reference_answer": "The chapter introduces the concept of mean win rate as an alternative to average scoring, focusing on a model's performance relative to others across different scenarios, which can offer a clearer assessment of capability."
  },
  {
    "question": "costs of hosting AI models",
    "answer": "The costs associated with hosting AI models can vary significantly depending on several factors. \n\n1. **API Usage Costs**: If you utilize model APIs, costs are typically based on usage, often charged by the number of tokens processed (input and output). Heavy usage can rapidly become expensive, leading some companies to consider self-hosting their models.\n\n2. **Self-Hosting Costs**: When hosting your own models, the primary costs are related to compute resources. While hosting can incur significant initial expenditures for engineering talent and time, costs per token may decrease as you scale, potentially making self-hosting more economical in the long run.\n\n3. **Benchmarking Costs**: Research indicates that the total cost for using commercial APIs can be around $38,000, while deploying open models may involve approximately 19,500 GPU hours. Given that GPU costs typically range from $2.15 to $3.18 per hour, self-hosting may result in total costs between $80,000 and $100,000.\n\n4. **Trade-offs**: The decision to self-host versus using APIs also involves evaluating performance, data privacy, and maintenance responsibilities. Companies may start with open-source models or APIs and switch based on performance needs or cost considerations.\n\nIn essence, the choice between using model APIs and self-hosting involves weighing up front and ongoing costs, potential scale savings, and additional resource requirements for maintenance and infrastructure.",
    "context": [
      {
        "start": 62250,
        "text": "st\nModel APIs charge per usage, which means that they can get prohibitively\nexpensive with heavy usage. At a certain scale, a company that is bleeding\n18\nits resources using APIs might consider hosting their own models.\nHowever, hosting a model yourself requires nontrivial time, talent, and\nengineer",
        "chapter": 4
      },
      {
        "start": 42000,
        "text": "hese memory\nconfigurations. It’s not a coincidence that many models today have 7 billion\nor 65 billion parameters.\nIf you use model APIs, your cost per token usually doesn’t change much as\nyou scale. However, if you host your own models, your cost per token can\nget much cheaper as you scale. If you’",
        "chapter": 4
      },
      {
        "start": 41500,
        "text": "y charge by tokens. The more input and\noutput tokens you use, the more expensive it is. Many applications then try\nto reduce the input and output token count to manage cost.\nIf you host your own models, your cost, outside engineering cost, is\ncompute. To make the most out of the machines they have, ",
        "chapter": 4
      },
      {
        "start": 116500,
        "text": "able score, check how reliable the score is.\n6\nThe HELM paper reported that the total cost is $38,000 for commercial APIs and 19,500 GPU hours\nfor open models. If an hour of GPU costs between $2.15 and $3.18, the total cost comes out to\n$80,000–$100,000.\n7\nA friend quipped: “A benchmark stops being ",
        "chapter": 4
      },
      {
        "start": 54750,
        "text": "ussion, I treat these privately deployed commercial APIs similarly to self-hosted models.\nThe answer to whether to host a model yourself or use a model API depends\non the use case. And the same use case can change over time. Here are\nseven axes to consider: data privacy, data lineage, performance,\nf",
        "chapter": 4
      },
      {
        "start": 47000,
        "text": "host open source models. However, after public\nand private evaluation, you might realize that open source models can’t\nachieve the level of performance you want and have to switch to\ncommercial APIs.\nChapter 10 discusses monitoring and collecting user feedback. The rest of\nthis chapter will discuss ",
        "chapter": 4
      },
      {
        "start": 68250,
        "text": "elf-hosting models\nbuilding and maintaining\nmodel APIs\nEdge use cases\nCan’t run on device Can run on device, but\nwithout internet again, might be hard to\naccess do so\nThe pros and cons of each approach hopefully can help you decide whether\nto use a commercial API or to host a model yourself. This de",
        "chapter": 4
      },
      {
        "start": 1250,
        "text": " host their own models or to use a model API. This question has\nbecome more nuanced with the introduction of model API services built on\ntop of open source models.\nThe last part discusses developing an evaluation pipeline that can guide the\ndevelopment of your application over time. This part brings",
        "chapter": 4
      },
      {
        "start": 114000,
        "text": "rom model developers’ perspective, why invest\nmillions, if not billions, into building models just for others to make money?It might be argued that\nMeta supports open source models only to keep their competitors (Google, Microsoft/OpenAI) in\ncheck. Both Mistral and Cohere have open source models, bu",
        "chapter": 4
      },
      {
        "start": 57000,
        "text": "mance but said nothing about the models’ training data other\nthan that “all data enrichment workers are paid at least a local living wage”.\nOpenAI’s CTO wasn’t able to provide a satisfactory answer when asked\nwhat data was used to train their models.\nOn top of that, the IP laws around AI are activel",
        "chapter": 4
      }
    ],
    "input_tokens": 991,
    "output_tokens": 285,
    "input_cost": 0.00014865,
    "output_cost": 0.000171,
    "total_cost": 0.00031965,
    "reference_answer": "The chapter outlines the costs associated with self-hosting models, highlighting the time, talent, and engineering effort required, which can be alleviated by using model hosting services."
  },
  {
    "question": "best practices for evaluating AI models",
    "answer": "To effectively evaluate AI models, consider the following best practices based on the context provided:\n\n1. **Define Evaluation Criteria**: Start by establishing a tailored list of evaluation criteria specific to your application. These criteria should reflect the intended purposes and requirements of your AI system.\n\n2. **Use Public Benchmarks for Initial Selection**: Begin the evaluation process by applying public benchmarks to narrow down to a set of promising models. This step helps identify models that are potentially suitable for your application.\n\n3. **Develop a Custom Evaluation Pipeline**: After selecting promising models, create your own evaluation pipeline to conduct more rigorous tests. This pipeline should allow you to run experiments that balance all objectives pertinent to your application.\n\n4. **Iterative Evaluation Workflow**: Employ an iterative approach to your evaluation workflow. Be prepared to revisit and adjust decisions based on new insights gained during the evaluation process. \n\n5. **Monitor in Production**: Once models are deployed, continuously monitor their performance. This helps in detecting failures early and collecting feedback that can drive further improvements.\n\n6. **Incorporate Human Evaluation**: Utilize human experts to manually assess model quality, especially when evaluating open-ended responses. This can serve as a crucial metric to guide your application development.\n\nThese best practices ensure a comprehensive and systematic approach to evaluating AI models, promoting reliability and suitability for your specific needs.",
    "context": [
      {
        "start": 46500,
        "text": "3. Run experiments with your own evaluation pipeline to find the best\nmodel, again, balancing all your objectives.\n4. Continually monitor your model in production to detect failure and\ncollect feedback to improve your application.\nFigure 4-5. An overview of the evaluation workflow to evaluate models",
        "chapter": 4
      },
      {
        "start": 0,
        "text": "Chapter 4. Evaluate AI Systems\nA model is only useful if it works for its intended purposes. You need to\nevaluate models in the context of your application. Chapter 3 discusses\ndifferent approaches to automatic evaluation. This chapter discusses how to\nuse these approaches to evaluate models for you",
        "chapter": 4
      },
      {
        "start": 86750,
        "text": " for your application. After using public\nbenchmarks to narrow them to a set of promising models, you’ll need to run\nyour own evaluation pipeline to find the best one for your application. How\nto design a custom evaluation pipeline will be our next topic.\nDesign Your Evaluation Pipeline\nThe success ",
        "chapter": 4
      },
      {
        "start": 4500,
        "text": "to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can th",
        "chapter": 4
      },
      {
        "start": 46750,
        "text": "view of the evaluation workflow to evaluate models for your application.\nThese four steps are iterative—you might want to change the decision from\na previous step with newer information from the current step. For example,\nyou might initially want to host open source models. However, after public\nand",
        "chapter": 4
      },
      {
        "start": 250,
        "text": "to\nuse these approaches to evaluate models for your applications.\nThis chapter contains three parts. It starts with a discussion of the criteria\nyou might use to evaluate your applications and how these criteria are\ndefined and calculated. For example, many people worry about AI making\nup facts—how ",
        "chapter": 4
      },
      {
        "start": 97750,
        "text": "ion. Having human experts manually\nevaluate a model’s quality is a long-standing practice in AI. Given the\nchallenges of evaluating open-ended responses, many teams are looking at\nhuman evaluation as the North Star metric to guide their application\ndevelopment. Each day, you can use human experts to",
        "chapter": 4
      },
      {
        "start": 43750,
        "text": "ications. Once you’ve\ndefined the criteria for your application, you should evaluate models against\nthese criteria.\nDuring the application development process, as you progress through\ndifferent adaptation techniques, you’ll have to do model selection over and\nover again. For example, prompt engineer",
        "chapter": 4
      },
      {
        "start": 80500,
        "text": " need to think about how important each benchmark\nis to you and weigh their scores accordingly.\nAs you evaluate models using public benchmarks, keep in mind that the\ngoal of this process is to select a small subset of models to do more rigorous\nexperiments using your own benchmarks and metrics. This",
        "chapter": 4
      },
      {
        "start": 46000,
        "text": "vel, the evaluation workflow consists of four steps (see\nFigure 4-5):\n1. Filter out models whose hard attributes don’t work for you. Your list of\nhard attributes depends heavily on your own internal policies, whether\nyou want to use commercial APIs or host your own models.\n2. Use publicly available ",
        "chapter": 4
      }
    ],
    "input_tokens": 901,
    "output_tokens": 271,
    "input_cost": 0.00013515,
    "output_cost": 0.0001626,
    "total_cost": 0.00029775,
    "reference_answer": "The chapter outlines an iterative evaluation workflow that helps teams determine whether to use model APIs or self-hosted models based on performance needs."
  },
  {
    "question": "what are essential functionalities for AI models?",
    "answer": "Essential functionalities for AI models include:\n\n1. **Scalability**: Ensuring that the inference service can handle the application's traffic efficiently.\n\n2. **Reliable Evaluation**: Establishing a robust evaluation pipeline that defines specific criteria for model performance, enabling accurate assessment and improvement.\n\n3. **Domain-Specific Capabilities**: Incorporating functionalities that cater to specific tasks, such as coding, reasoning, and factual consistency.\n\n4. **Model Selection**: Ability to choose from various foundation models, ensuring that the selected model fits the intended application context.\n\n5. **Consistency in Role**: Evaluating if the model adheres to its defined role and stays in character throughout its operation. \n\nThese functionalities help in making the model efficient and effective for real-world applications.",
    "context": [
      {
        "start": 60250,
        "text": " model is being used, and how well\nthe model works in the wild.\nFunctionality\nMany functionalities are needed around a model to make it work for a use\ncase. Here are some examples of these functionalities:\nScalability: making sure the inference service can support your\napplication’s traffic while ma",
        "chapter": 4
      },
      {
        "start": 4500,
        "text": "to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can th",
        "chapter": 4
      },
      {
        "start": 107000,
        "text": "ations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enabl",
        "chapter": 4
      },
      {
        "start": 250,
        "text": "to\nuse these approaches to evaluate models for your applications.\nThis chapter contains three parts. It starts with a discussion of the criteria\nyou might use to evaluate your applications and how these criteria are\ndefined and calculated. For example, many people worry about AI making\nup facts—how ",
        "chapter": 4
      },
      {
        "start": 3750,
        "text": "mon generative AI use case because, unlike other\ngeneration tasks, generated code can be evaluated using functional\ncorrectness.\nEven though foundation models are open-ended, many of their use cases\nare close-ended, such as intent classification, sentiment analysis, next-\naction prediction, etc. It’",
        "chapter": 4
      },
      {
        "start": 0,
        "text": "Chapter 4. Evaluate AI Systems\nA model is only useful if it works for its intended purposes. You need to\nevaluate models in the context of your application. Chapter 3 discusses\ndifferent approaches to automatic evaluation. This chapter discusses how to\nuse these approaches to evaluate models for you",
        "chapter": 4
      },
      {
        "start": 6500,
        "text": "tion, code debugging, grade school math,\nscience knowledge, common sense, reasoning, legal knowledge, tool use,\ngame playing, etc. The list goes on.\nDomain-specific capabilities are commonly evaluated using exact\nevaluation. Coding-related capabilities are typically evaluated using\nfunctional correc",
        "chapter": 4
      },
      {
        "start": 17500,
        "text": " light on how AI models process conflicting\ninformation and determine what the facts are. For example, Wan et al.\n(2024) found that existing “models rely heavily on the relevance of a\nwebsite to the query, while largely ignoring stylistic features that humans\nfind important such as whether a text co",
        "chapter": 4
      },
      {
        "start": 500,
        "text": "e, many people worry about AI making\nup facts—how is factual consistency detected? How are domain-specific\ncapabilities like math, science, reasoning, and summarization measured?\nThe second part focuses on model selection. Given an increasing number of\nfoundation models to choose from, it can feel o",
        "chapter": 4
      },
      {
        "start": 37750,
        "text": "ully crafted similarity scores (how similar the generated outputs are to\nthe expected outputs) and AI judges.\nIf AI in your application is supposed to assume a certain role, make sure to\nevaluate whether your model stays in character. Depending on the role, you\nmight be able to create heuristics to ",
        "chapter": 4
      }
    ],
    "input_tokens": 907,
    "output_tokens": 154,
    "input_cost": 0.00013605,
    "output_cost": 9.24e-05,
    "total_cost": 0.00022845,
    "reference_answer": "The chapter outlines key functionalities like scalability, function calling, structured outputs, and output guardrails as crucial for AI models to function effectively in real-world applications."
  },
  {
    "question": "how to rank LLMs based on role performance",
    "answer": "To rank large language models (LLMs) based on their performance in specific roles, you can follow a structured approach as outlined in the AI Engineering book by Chip Huyen:\n\n1. **Define the Role and Criteria**: \n   - Clearly specify the role the model needs to play, including detailed descriptions and relevant catchphrases. This helps set expectations for the model's behavior.\n\n2. **Select Relevant Benchmarks**: \n   - Choose benchmarks that are pertinent to the role. For instance, Hugging Face updated their Open LLM Leaderboard to include diverse benchmarks, such as ARC-C for science questions and MMLU for knowledge and reasoning capabilities. Make sure the selected benchmarks align with the role's requirements.\n\n3. **Evaluate the Models**: \n   - Have the models generate outputs based on the defined role. You can use both automated and heuristic methods to assess these responses. Heuristics could include metrics like average output length for roles that require brevity.\n\n4. **Aggregate the Results**: \n   - Since benchmarks can have varying scoring systems (e.g., accuracy, F1, BLEU), it's crucial to develop a method to aggregate these scores for a unified ranking. Consider the importance of each benchmark relative to the role being evaluated.\n\n5. **Create a Ranking System**: \n   - Implement a ranking system, potentially using Python dictionaries and lists, to systematically compare and rank the models based on their evaluated performances.\n\n6. **Iterate for Improvement**: \n   - Continuously refine your evaluation criteria and methodology based on feedback and results from different models, adapting your private leaderboard as needed to reflect the most relevant performance metrics for your application.\n\nBy following these steps, you can construct a robust framework for ranking LLMs based on their role performance.",
    "context": [
      {
        "start": 71750,
        "text": " in late 2023, Hugging Face updated their Open LLM Leaderboard\nto use the average of six different benchmarks to rank models:\n1. ARC-C (Clark et al., 2018): Measuring the ability to solve complex,\ngrade school-level science questions.\n2. MMLU (Hendrycks et al., 2020): Measuring knowledge and reasoni",
        "chapter": 4
      },
      {
        "start": 38750,
        "text": ". For the full prompt, please check out Wang et al.\n(2023).\nSystem Instruction:\nYou are a role−playing performance comparison\nassistant. You should rank the models based on\nthe role characteristics and text quality of\ntheir responses. The rankings are then output\nusing Python dictionaries and lists.",
        "chapter": 4
      },
      {
        "start": 39000,
        "text": "e then output\nusing Python dictionaries and lists.\nUser Prompt:\nThe models below are to play the role of\n‘‘{role_name}’’. The role description of\n‘‘{role_name}’’ is\n‘‘{role_description_and_catchphrases}’’. I\nneed to rank the following models based on the\ntwo criteria below:\n1. Which one has more pro",
        "chapter": 4
      },
      {
        "start": 69750,
        "text": "in your leaderboard?\nHow to aggregate these benchmark results to rank models?\nGiven so many benchmarks out there, it’s impossible to look at them all, let\nalone aggregate their results to decide which model is the best. Imagine that\nyou’re considering two models, A and B, for code generation. If mod",
        "chapter": 4
      },
      {
        "start": 71500,
        "text": " are generally thoughtful about\nhow they select benchmarks, their decision-making process isn’t always\nclear to users. Different leaderboards often end up with different\nbenchmarks, making it hard to compare and interpret their rankings. For\nexample, in late 2023, Hugging Face updated their Open LLM",
        "chapter": 4
      },
      {
        "start": 43000,
        "text": "tput token 1M tokens 1M to\nScale TPM (tokens X > 1M TPM > 1M\nper minute)\nLatency Time to first Internal user < 200ms < 100\ntoken (P90) prompt dataset\nLatency Time per total Internal user < 1m < 30s\nquery (P90) prompt dataset\nOverall model Elo score Chatbot > 1200 > 125\nquality Arena’s\nranking\nCode p",
        "chapter": 4
      },
      {
        "start": 80250,
        "text": "e scores for the\nmodels you care about on these benchmarks, you then need to aggregate\nthese scores to rank models. Not all benchmark scores are in the same unit\nor scale. One benchmark might use accuracy, another F1, and another\nBLEU score. You will need to think about how important each benchmark\n",
        "chapter": 4
      },
      {
        "start": 109000,
        "text": "reating a private leaderboard\nto rank models based on your needs.\nThis chapter ends with how to use all the evaluation techniques and criteria\ndiscussed in the last chapter and how to create an evaluation pipeline for\nyour application. No perfect evaluation method exists. It’s impossible to\ncapture ",
        "chapter": 4
      },
      {
        "start": 78000,
        "text": "rboards with public benchmarks\nWhen evaluating models for a specific application, you’re basically creating\na private leaderboard that ranks models based on your evaluation criteria.\nThe first step is to gather a list of benchmarks that evaluate the capabilities\nimportant to your application. If you",
        "chapter": 4
      },
      {
        "start": 38000,
        "text": "e role, you\nmight be able to create heuristics to evaluate the model’s outputs. For\nexample, if the role is someone who doesn’t talk a lot, a heuristic would be\nthe average of the model’s outputs. Other than that, the easiest automatic\nevaluation approach is AI as a judge. You should evaluate the ro",
        "chapter": 4
      }
    ],
    "input_tokens": 1041,
    "output_tokens": 361,
    "input_cost": 0.00015615,
    "output_cost": 0.0002166,
    "total_cost": 0.00037275,
    "reference_answer": "The chapter provides a methodology for assessing LLMs by creating and comparing their outputs against defined role characteristics and quality criteria, using Python for organization."
  },
  {
    "question": "best practices for collecting user feedback on AI performance",
    "answer": "To effectively collect user feedback on AI performance, several best practices can be adopted based on the context from \"AI Engineering\" by Chip Huyen:\n\n1. **Define Evaluation Criteria**: Establish clear criteria for what constitutes a successful interaction with your AI application. For instance, users may evaluate responses based on relevance, factual consistency, and promptness.\n\n2. **Use Multiple Feedback Mechanisms**: Leverage a variety of feedback types. The context indicates that users often utilize multiple criteria to assess AI performance, so consider integrating diverse feedback collection methods such as surveys, ratings, or direct user comments.\n\n3. **Annotated Evaluation Data**: Curate a set of annotated examples that will help you assess different components of your AI application. This annotated data is crucial to evaluate the system's performance comprehensively.\n\n4. **Incorporate Human Evaluation**: Just like LinkedIn, which manually evaluates a subset of outputs daily, consider periodic human evaluations to track changes in performance and identify unusual usage patterns.\n\n5. **Iterate on Evaluation Pipeline**: Regularly refine your evaluation methods based on user feedback and system performance. This iterative process will help improve how you interpret and act upon the feedback received.\n\n6. **Experiment Tracking**: Maintain detailed logs of variables in the evaluation process, including the evaluation data and any changes in the criteria or methods used. This enables better understanding and repeatability of experiments.\n\n7. **Production Monitoring**: Continuously monitor your AI's performance in production to detect any failures and promptly collect feedback. This ongoing evaluation will ensure that the AI evolves and improves based on real user interactions.\n\nBy employing these best practices, you can create a robust framework for collecting and utilizing user feedback to enhance the performance of your AI applications.",
    "context": [
      {
        "start": 98750,
        "text": "\nuse user feedback to improve your application. How to collect user\nfeedback is discussed in Chapter 10.\nAnnotate evaluation data\nCurate a set of annotated examples to evaluate your application. You need\nannotated data to evaluate each of your system’s components and each\ncriterion, for both turn-ba",
        "chapter": 4
      },
      {
        "start": 107000,
        "text": "ations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enabl",
        "chapter": 4
      },
      {
        "start": 98250,
        "text": "luate up to 500 daily conservations with their AI\nsystems.\nConsider evaluation methods to be used not just during experimentation but\nalso during production. During experimentation, you might have reference\ndata to compare your application’s outputs to, whereas, in production,\nreference data might n",
        "chapter": 4
      },
      {
        "start": 92250,
        "text": "23 found that, on average, their users used 2.3\ndifferent types of feedback (criteria) to evaluate an application. For\nexample, for a customer support application, a good response might be\ndefined using three criteria:\n1. Relevance: the response is relevant to the user’s query.\n2. Factual consistenc",
        "chapter": 4
      },
      {
        "start": 106750,
        "text": ".\nAs you iterate on your evaluation pipeline, make sure to do proper\nexperiment tracking: log all variables that could change in an evaluation\nprocess, including but not limited to the evaluation data, the rubric, and the\nprompt and sampling configurations used for the AI judges.\nSummary\nThis is one",
        "chapter": 4
      },
      {
        "start": 95250,
        "text": "quests.\nBefore developing AI evaluation metrics, it’s crucial to first understand the\nbusiness metrics you’re targeting. Many applications focus on stickiness\nmetrics, such as daily, weekly, or monthly active users (DAU, WAU,\nMAU). Others prioritize engagement metrics, like the number of\nconversatio",
        "chapter": 4
      },
      {
        "start": 4500,
        "text": "to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can th",
        "chapter": 4
      },
      {
        "start": 98000,
        "text": "evelopment. Each day, you can use human experts to evaluate a subset of\nyour application’s outputs that day to detect any changes in the application’s\nperformance or unusual patterns in usage. For example, LinkedIn developed\na process to manually evaluate up to 500 daily conservations with their AI\n",
        "chapter": 4
      },
      {
        "start": 46500,
        "text": "3. Run experiments with your own evaluation pipeline to find the best\nmodel, again, balancing all your objectives.\n4. Continually monitor your model in production to detect failure and\ncollect feedback to improve your application.\nFigure 4-5. An overview of the evaluation workflow to evaluate models",
        "chapter": 4
      },
      {
        "start": 18750,
        "text": "an\noutput against—this context was either provided by users or retrieved by\nyou (context retrieval is discussed in Chapter 6). The most straightforward\nevaluation approach is AI as a judge. As discussed in Chapter 3, AI judges\ncan be asked to evaluate anything, including factual consistency. Both Li",
        "chapter": 4
      }
    ],
    "input_tokens": 936,
    "output_tokens": 350,
    "input_cost": 0.0001404,
    "output_cost": 0.00021,
    "total_cost": 0.0003504,
    "reference_answer": "The text suggests considering user feedback as an essential evaluation method, correlating it with other metrics, and utilizing it actively during production for continuous improvement."
  },
  {
    "question": "NLG evaluation metrics examples",
    "answer": "In the context of Natural Language Generation (NLG), several evaluation metrics are commonly used to assess the quality of generated texts. Examples of these metrics include:\n\n1. **Fluency**: This metric measures whether the text is grammatically correct and sounds natural, essentially assessing if it resembles something written by a fluent speaker.\n\n2. **Coherence**: Coherence evaluates how well the text flows logically and if the ideas are presented in a clear, understandable manner.\n\n3. **Faithfulness**: Particularly relevant in tasks like translation, this metric assesses how faithfully the generated output maintains the meaning of the original content.\n\n4. **Relevance**: In summarization tasks, relevance checks if the summary focuses on the most important aspects of the source document.\n\n5. **Factual Consistency**: This involves assessing whether the generated content aligns with known facts, ensuring that the output doesn't include inaccuracies.\n\nThese metrics are integral in developing a reliable evaluation pipeline for AI models, especially as open-ended text generation continues to evolve.",
    "context": [
      {
        "start": 11500,
        "text": "the brightest minds in NLP (natural language\nprocessing) have been working on how to evaluate the quality of open-\nended outputs. The subfield that studies open-ended text generation is\ncalled NLG (natural language generation). NLG tasks in the early 2010s\nincluded translation, summarization, and pa",
        "chapter": 4
      },
      {
        "start": 107750,
        "text": "tions, and how they are evaluated. It discussed how to evaluate both\ndomain-specific capabilities and generation capabilities, including factual\nconsistency and safety. Many criteria to evaluate foundation models\nevolved from traditional NLP, including fluency, coherence, and\nfaithfulness.\nTo help a",
        "chapter": 4
      },
      {
        "start": 11750,
        "text": " 2010s\nincluded translation, summarization, and paraphrasing.\nMetrics used to evaluate the quality of generated texts back then included\nfluency and coherence. Fluency measures whether the text is grammatically\ncorrect and natural-sounding (does this sound like something written by a\nfluent speaker?",
        "chapter": 4
      },
      {
        "start": 97500,
        "text": "e\nused to evaluate a model’s perplexity for a generated text, which can be\nused for measurements such as fluency and factual consistency.\nUse automatic metrics as much as possible, but don’t be afraid to fall back\non human evaluation, even in production. Having human experts manually\nevaluate a mode",
        "chapter": 4
      },
      {
        "start": 96500,
        "text": "ponse and the whole context. An unambiguous scoring\nrubric and examples will be critical for specialized scorers and AI judges to\nsucceed.\nIt’s possible to mix and match evaluation methods for the same criteria. For\nexample, you might have a cheap classifier that gives low-quality signals on\n100% of",
        "chapter": 4
      },
      {
        "start": 93750,
        "text": "r\nguideline is the backbone of a reliable evaluation pipeline. This guideline\ncan also be reused later for training data annotation, as discussed in\nChapter 8.\nTie evaluation metrics to business metrics\nWithin a business, an application must serve a business goal. The\napplication’s metrics must be c",
        "chapter": 4
      },
      {
        "start": 18750,
        "text": "an\noutput against—this context was either provided by users or retrieved by\nyou (context retrieval is discussed in Chapter 6). The most straightforward\nevaluation approach is AI as a judge. As discussed in Chapter 3, AI judges\ncan be asked to evaluate anything, including factual consistency. Both Li",
        "chapter": 4
      },
      {
        "start": 107000,
        "text": "ations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enabl",
        "chapter": 4
      },
      {
        "start": 72500,
        "text": "ng,\nfocusing on a model’s understanding of facts.\n5. WinoGrande (Sakaguchi et al., 2019): Measuring the ability to solve\nchallenging pronoun resolution problems that are designed to be difficult\nfor language models, requiring sophisticated commonsense reasoning.\n6. GSM-8K (Grade School Math, OpenAI,",
        "chapter": 4
      },
      {
        "start": 12250,
        "text": "hfulness:\nhow faithful is the generated translation to the original sentence? A metric\nthat a summarization task might use is relevance: does the summary focus\non the most important aspects of the source document? (Li et al., 2022).\nSome early NLG metrics, including faithfulness and relevance, have ",
        "chapter": 4
      }
    ],
    "input_tokens": 940,
    "output_tokens": 206,
    "input_cost": 0.000141,
    "output_cost": 0.0001236,
    "total_cost": 0.0002646,
    "reference_answer": "It details various metrics from the NLG field, emphasizing fluency, coherence, and specific metrics tailored for tasks like translation and summarization."
  },
  {
    "question": "open source vs commercial AI models",
    "answer": "**Open Source vs. Commercial AI Models**\n\nIn the context of AI models, open source and commercial models offer distinct advantages and disadvantages.\n\n### Open Source Models\n\n**Pros:**\n1. **Control and Customizability:** Enterprises appreciate open source models for the ability to modify and customize them according to their specific needs, which is often highlighted as a key motivation for their adoption (a16z study).\n2. **Accessibility:** Open source models can be supported by various API providers, allowing flexibility in choosing the best service without being tied to a single vendor.\n3. **Societal Benefits:** There is an argument that open source models contribute positively to society by promoting transparency and collaboration.\n\n**Cons:**\n1. **Performance Limitations:** Open source models may lag behind commercial models, particularly in terms of performance for complex tasks. They may not meet the requirements for certain advanced use cases, forcing users to switch to commercial options.\n2. **Legal and Support Challenges:** Open source models might lack extensive legal support compared to commercial counterparts, presenting challenges in compliance and data protection.\n3. **Resource Limitations:** Developers typically release weaker models as open source, which may not be competitive with the strongest proprietary models.\n\n### Commercial Models\n\n**Pros:**\n1. **Ease of Use and Scaling:** Proprietary models are often more user-friendly initially, making them easier to integrate and scale compared to open source alternatives.\n2. **Performance:** Commercial models generally offer superior performance for complex applications, which can justify their cost for businesses needing high-quality outcomes.\n3. **Support Infrastructure:** Companies providing commercial models often have more resources for legal protections and customer support, which can be a key advantage for enterprises.\n\n**Cons:**\n1. **Vendor Lock-In:** Access to commercial models typically comes only through APIs licensed by model developers, limiting flexibility and potentially leading to dependency on a single provider.\n2. **Customization Limitations:** Proprietary models may not allow as much manipulation or adjustment as open source alternatives due to restricted access to the underlying components.\n\n### Conclusion\n\nChoosing between open source and commercial AI models hinges on specific organizational needs, including the level of customization required, performance expectations, and considerations around vendor flexibility versus ease of use. For many applications that do not necessitate cutting-edge performance, open source models can be a sufficient and effective choice.",
    "context": [
      {
        "start": 48000,
        "text": "n open source model yourself. The answer to this\nquestion can significantly reduce your candidate model pool.\nLet’s first go into what exactly open source means when it comes to\nmodels, then discuss the pros and cons of these two approaches.\nOpen source, open weight, and model licenses\nThe term “ope",
        "chapter": 4
      },
      {
        "start": 59750,
        "text": "or this reason, it’s likely that the strongest open source model will lag\nbehind the strongest proprietary models for the foreseeable future.\nHowever, for many use cases that don’t need the strongest models, open\nsource models might be sufficient.\nAnother reason that might cause open source models t",
        "chapter": 4
      },
      {
        "start": 52750,
        "text": " Many model developers are also model\nservice providers. Cohere and Mistral open source some models and\nprovide APIs for some. OpenAI is typically known for their commercial\nmodels, but they’ve also open sourced models (GPT-2, CLIP). Typically,\nmodel providers open source weaker models and keep thei",
        "chapter": 4
      },
      {
        "start": 114250,
        "text": "oth Mistral and Cohere have open source models, but they also have APIs. At some point,\ninference services on top of Mistral and Cohere models become their competitors.There’s the\nargument that open source is better for society, and maybe that’s enough as an incentive. People who\nwant what’s good fo",
        "chapter": 4
      },
      {
        "start": 47000,
        "text": "host open source models. However, after public\nand private evaluation, you might realize that open source models can’t\nachieve the level of performance you want and have to switch to\ncommercial APIs.\nChapter 10 discusses monitoring and collecting user feedback. The rest of\nthis chapter will discuss ",
        "chapter": 4
      },
      {
        "start": 63750,
        "text": "d by others, who might have shared solutions online.\nControl, access, and transparency\nA 2024 study by a16z shows two key reasons that enterprises care about\nopen source models are control and customizability, as shown in Figure 4-8.\nFigure 4-8. Why enterprises care about open source models. Image f",
        "chapter": 4
      },
      {
        "start": 53750,
        "text": "u switch between\nmodel APIs.\nCommercial models are only accessible via APIs licensed by the model\n13\ndevelopers. Open source models can be supported by any API provider,\nallowing you to pick and choose the provider that works best for you. For\ncommercial model providers, models are their competitive",
        "chapter": 4
      },
      {
        "start": 63000,
        "text": "hat.\nIn general, you want a model that is easy to use and manipulate. Typically,\nproprietary models are easier to get started with and scale, but open models\nmight be easier to manipulate as their components are more accessible.\nRegardless of whether you go with open or proprietary models, you want\n",
        "chapter": 4
      },
      {
        "start": 58250,
        "text": " theory, in practice, it’s\nchallenging for any company to thoroughly inspect a dataset of the size\ntypically used to train foundation models.\nGiven the same concern, many companies opt for commercial models\ninstead. Open source models tend to have limited legal resources compared\nto commercial model",
        "chapter": 4
      },
      {
        "start": 114000,
        "text": "rom model developers’ perspective, why invest\nmillions, if not billions, into building models just for others to make money?It might be argued that\nMeta supports open source models only to keep their competitors (Google, Microsoft/OpenAI) in\ncheck. Both Mistral and Cohere have open source models, bu",
        "chapter": 4
      }
    ],
    "input_tokens": 945,
    "output_tokens": 469,
    "input_cost": 0.00014175,
    "output_cost": 0.0002814,
    "total_cost": 0.00042315,
    "reference_answer": "The text outlines the differences between open source models and commercial models, noting that many developers release weaker models openly while keeping stronger models behind paywalls."
  },
  {
    "question": "evaluating AI with multiple-choice questions",
    "answer": "The evaluation of AI systems using multiple-choice questions serves as a structured approach to assess their performance in a reliable and consistent manner. According to the \"AI Engineering\" book by Chip Huyen, notable evaluation benchmarks such as UC Berkeley’s MMLU (2020), Microsoft’s AGIEval (2023), and the AI2 Reasoning Challenge (ARC-C) (2018) utilize multiple-choice formats intentionally to avoid the inconsistencies that can arise from open-ended tasks.\n\nOne key advantage of multiple-choice questions is that they provide a clear scoring mechanism, where the correct answers can be definitively marked, allowing for objective assessment of the AI's capabilities. This structured format helps to streamline the evaluation process and minimizes ambiguity, which is particularly crucial given the challenges involved in assessing complex AI outputs.\n\nTo implement effective evaluation pipelines, the AI application should begin with a tailored list of evaluation criteria specific to its goals, facilitating a reliable measurement of its performance. Moreover, evaluations can occur at multiple levels—per task, per turn, or for intermediate outputs—allowing for a comprehensive assessment throughout the system's operation.\n\nIn summary, multiple-choice questions play a significant role in the evaluation landscape of AI, paving the way for more consistent and actionable insights into model performance, which is essential for the ongoing adoption and refinement of AI technologies.",
    "context": [
      {
        "start": 8500,
        "text": " are multiple-choice,\nincluding UC Berkeley’s MMLU (2020), Microsoft’s AGIEval (2023), and\nthe AI2 Reasoning Challenge (ARC-C) (2018). In their paper, AGIEval’s\nauthors explained that they excluded open-ended tasks on purpose to avoid\ninconsistent assessment.\nHere’s an example of a multiple-choice q",
        "chapter": 4
      },
      {
        "start": 107000,
        "text": "ations used for the AI judges.\nSummary\nThis is one of the hardest, but I believe one of the most important, AI topics\nthat I’ve written about. Not having a reliable evaluation pipeline is one of\nthe biggest blocks to AI adoption. While evaluation takes time, a reliable\nevaluation pipeline will enabl",
        "chapter": 4
      },
      {
        "start": 4500,
        "text": "to evaluate them.\nI believe that evaluation is the biggest bottleneck to AI adoption. Being able\nto build reliable evaluation pipelines will unlock many new applications.\nAn AI application, therefore, should start with a list of evaluation criteria\nspecific to the application. In general, you can th",
        "chapter": 4
      },
      {
        "start": 18750,
        "text": "an\noutput against—this context was either provided by users or retrieved by\nyou (context retrieval is discussed in Chapter 6). The most straightforward\nevaluation approach is AI as a judge. As discussed in Chapter 3, AI judges\ncan be asked to evaluate anything, including factual consistency. Both Li",
        "chapter": 4
      },
      {
        "start": 97750,
        "text": "ion. Having human experts manually\nevaluate a model’s quality is a long-standing practice in AI. Given the\nchallenges of evaluating open-ended responses, many teams are looking at\nhuman evaluation as the North Star metric to guide their application\ndevelopment. Each day, you can use human experts to",
        "chapter": 4
      },
      {
        "start": 90250,
        "text": "entify this concept. Alice\ncan only answer yes or no. The score is based on whether Bob successfully\nguesses the concept, and how many questions it takes for Bob to guess it.\nHere’s an example of a plausible conversation in this task, taken from the\nBIG-bench’s GitHub repository:\nBob: Is the concept",
        "chapter": 4
      },
      {
        "start": 87500,
        "text": "process.\nStep 1. Evaluate All Components in a System\nReal-world AI applications are complex. Each application might consist of\nmany components, and a task might be completed after many turns.\nEvaluation can happen at different levels: per task, per turn, and per\nintermediate output.\nYou should evalu",
        "chapter": 4
      },
      {
        "start": 35000,
        "text": "s instruction. Each of these yes/no questions can\nbe answered by a human or AI evaluator. If the instruction has three criteria\nand the evaluator determines that a model’s output meets two of them, the\nmodel’s score for this instruction is 2/3. The final score for a model on this\nbenchmark is the nu",
        "chapter": 4
      },
      {
        "start": 87000,
        "text": "opic.\nDesign Your Evaluation Pipeline\nThe success of an AI application often hinges on the ability to differentiate\ngood outcomes from bad outcomes. To be able to do this, you need an\nevaluation pipeline that you can rely upon. With an explosion of evaluation\nmethods and techniques, it can be confus",
        "chapter": 4
      },
      {
        "start": 104750,
        "text": "s reliability and finding ways to make your evaluation pipeline\nmore efficient. Reliability is especially important with subjective evaluation\nmethods such as AI as a judge.\nHere are some questions you should be asking about the quality of your\nevaluation pipeline:\nIs your evaluation pipeline gettin",
        "chapter": 4
      }
    ],
    "input_tokens": 959,
    "output_tokens": 265,
    "input_cost": 0.00014385,
    "output_cost": 0.000159,
    "total_cost": 0.00030285,
    "reference_answer": "It describes how non-coding abilities can be assessed using close-ended tasks, particularly multiple-choice questions, which simplify verification and consistency in evaluation."
  },
  {
    "question": "how to set metrics for chatbot evaluation",
    "answer": "To set metrics for chatbot evaluation, follow these guidelines extracted from the context:\n\n1. **Define Evaluation Criteria**: Clearly outline what the chatbot should and shouldn't do. For instance, determine if the chatbot should answer questions outside its product domain.\n\n2. **Business Context**: Align your evaluation metrics with business metrics. Understand the business problem the chatbot is designed to solve, as this influences the importance of the metrics set.\n\n3. **Factual Consistency**: Establish thresholds for factual consistency. For example, if the chatbot's factual consistency score must be at least 50% to be considered useful, set this as a benchmark.\n\n4. **Engagement Metrics**: Consider stickiness metrics (DAU, WAU, MAU) and engagement metrics (e.g., number of conversations per user) as part of your evaluation.\n\n5. **Latency Measurement**: Track the time to first response and the time per query to ensure that interactions are timely (e.g., under 200ms for the first response).\n\n6. **Quality Benchmarks**: Define acceptable quality benchmarks using internal tests or external metrics, such as achieving a specific Elo score for quality rankings.\n\n7. **Comprehensive Coverage**: Ensure that your metrics cover various aspects of the chatbot's performance, including content relevance, user satisfaction, and operational efficiency.\n\nBy using these structured steps to develop metrics, you can ensure that your chatbot remains effective and aligned with your goals.",
    "context": [
      {
        "start": 94250,
        "text": "level of factual\nconsistency might make the chatbot unusable for questions about billing but\ngood enough for queries about product recommendations or general\ncustomer feedback. Ideally, you want to map evaluation metrics to business\nmetrics, to something that looks like this:\nFactual consistency of ",
        "chapter": 4
      },
      {
        "start": 91250,
        "text": "hould this chatbot answer\nquestions unrelated to your product, such as about an upcoming election? If\nnot, you need to define what inputs are out of the scope of your application,\nhow to detect them, and how your application should respond to them.\nDefine evaluation criteria\nOften, the hardest part ",
        "chapter": 4
      },
      {
        "start": 43000,
        "text": "tput token 1M tokens 1M to\nScale TPM (tokens X > 1M TPM > 1M\nper minute)\nLatency Time to first Internal user < 200ms < 100\ntoken (P90) prompt dataset\nLatency Time per total Internal user < 1m < 30s\nquery (P90) prompt dataset\nOverall model Elo score Chatbot > 1200 > 125\nquality Arena’s\nranking\nCode p",
        "chapter": 4
      },
      {
        "start": 95000,
        "text": ": what scores must an\napplication achieve for it to be useful? For example, you might determine\nthat your chatbot’s factual consistency score must be at least 50% for it to\nbe useful. Anything below this makes it unusable even for general customer\nrequests.\nBefore developing AI evaluation metrics, i",
        "chapter": 4
      },
      {
        "start": 91000,
        "text": "bad responses look like, you\nwon’t be able to catch them.\nWhen creating the evaluation guideline, it’s important to define not only\nwhat the application should do, but also what it shouldn’t do. For example,\nif you build a customer support chatbot, should this chatbot answer\nquestions unrelated to y",
        "chapter": 4
      },
      {
        "start": 94000,
        "text": "business goal. The\napplication’s metrics must be considered in the context of the business\nproblem it’s built to solve.\nFor example, if your customer support chatbot’s factual consistency is 80%,\nwhat does it mean for the business? For example, this level of factual\nconsistency might make the chatbo",
        "chapter": 4
      },
      {
        "start": 95250,
        "text": "quests.\nBefore developing AI evaluation metrics, it’s crucial to first understand the\nbusiness metrics you’re targeting. Many applications focus on stickiness\nmetrics, such as daily, weekly, or monthly active users (DAU, WAU,\nMAU). Others prioritize engagement metrics, like the number of\nconversatio",
        "chapter": 4
      },
      {
        "start": 116250,
        "text": " to degrade and some to\nimprove. For example, migrating from GPT-3.5-turbo-0301 to GPT-3.5-turbo-1106 led to a 10% drop\nin Voiceflow’s intent classification task but an improvement in GoDaddy’s customer support chatbot.\n5\nIf there is a publicly available score, check how reliable the score is.\n6\nThe",
        "chapter": 4
      },
      {
        "start": 2500,
        "text": "t he had no idea if the\nmodel’s predictions were accurate. At the beginning of the ChatGPT fever,\ncompanies rushed to deploy customer support chatbots. Many of them are\nstill unsure if these chatbots help or hurt their user experience.\nBefore investing time, money, and resources into building an app",
        "chapter": 4
      },
      {
        "start": 43250,
        "text": "hatbot > 1200 > 125\nquality Arena’s\nranking\nCode pass@1 HumanEval > 90% > 95%\ngeneration\ncapability\nHard\nCriteria Metric Benchmark Ideal\nrequirement\nFactual Internal GPT Internal > 0.8 > 0.9\nconsistency metric hallucination\ndataset\nNow that you have your criteria, let’s move on to the next step and ",
        "chapter": 4
      }
    ],
    "input_tokens": 1028,
    "output_tokens": 291,
    "input_cost": 0.0001542,
    "output_cost": 0.0001746,
    "total_cost": 0.0003288,
    "reference_answer": "The chapter suggests determining a usefulness threshold for evaluation metrics, like a factual consistency score, to assess a chatbot's effectiveness before engaging users."
  },
  {
    "question": "how to detect data contamination in AI models",
    "answer": "To detect data contamination in AI models, you can use several heuristics as outlined in the context:\n\n1. **N-gram Overlapping**: This method involves checking for sequences of tokens that appear both in the training and evaluation datasets. A high degree of overlap may indicate contamination since the model might have memorized the answers from the training data.\n\n2. **Perplexity**: This metric measures how well a probability distribution predicts a sample. If the perplexity is unusually low on the evaluation data, it may suggest that the model has been trained on that data, thus leading to contamination.\n\nBy applying these techniques, you can identify instances where a model could be overfitting to the training data or has inadvertently learned from data it was evaluated on.",
    "context": [
      {
        "start": 81250,
        "text": " Data\ncontamination happens when a model was trained on the same data it’s\nevaluated on. If so, it’s possible that the model just memorizes the answers\nit saw during training, causing it to achieve higher evaluation scores than it\nshould. A model that is trained on the MMLU benchmark can achieve hig",
        "chapter": 4
      },
      {
        "start": 83750,
        "text": "d on many bar exam questions.\nTo deal with data contamination, you first need to detect the contamination,\nand then decontaminate your data. You can detect contamination using\nheuristics like n-gram overlapping and perplexity:\nN-gram overlapping\nFor example, if a sequence of 13 tokens in an evaluati",
        "chapter": 4
      },
      {
        "start": 82750,
        "text": "and someone else might use questions from the same math\ntextbooks to create a benchmark to evaluate the model’s capabilities.\nData contamination can also happen intentionally for good reasons. Let’s\nsay you want to create the best possible model for your users. Initially, you\nexclude benchmark data ",
        "chapter": 4
      },
      {
        "start": 81000,
        "text": "ndle data contamination will be\nthe topic of the next section.\nData contamination with public benchmarks\nData contamination is so common that there are many different names for\nit, including data leakage, training on the test set, or simply cheating. Data\ncontamination happens when a model was train",
        "chapter": 4
      },
      {
        "start": 26500,
        "text": "elop\nmoderation tools to keep their models safe, and some of them expose their\nmoderation tools for external use.\nHarmful behaviors aren’t unique to AI outputs. They’re unfortunately\nextremely common online. Many models developed to detect toxicity in\nhuman-generated texts can be used for AI-generat",
        "chapter": 4
      },
      {
        "start": 82500,
        "text": "aluate their new models.\nData contamination can happen indirectly, such as when both evaluation\nand training data come from the same source. For example, you might\ninclude math textbooks in the training data to improve the model’s math\ncapabilities, and someone else might use questions from the same",
        "chapter": 4
      },
      {
        "start": 17500,
        "text": " light on how AI models process conflicting\ninformation and determine what the facts are. For example, Wan et al.\n(2024) found that existing “models rely heavily on the relevance of a\nwebsite to the query, while largely ignoring stylistic features that humans\nfind important such as whether a text co",
        "chapter": 4
      },
      {
        "start": 37750,
        "text": "ully crafted similarity scores (how similar the generated outputs are to\nthe expected outputs) and AI judges.\nIf AI in your application is supposed to assume a certain role, make sure to\nevaluate whether your model stays in character. Depending on the role, you\nmight be able to create heuristics to ",
        "chapter": 4
      },
      {
        "start": 98250,
        "text": "luate up to 500 daily conservations with their AI\nsystems.\nConsider evaluation methods to be used not just during experimentation but\nalso during production. During experimentation, you might have reference\ndata to compare your application’s outputs to, whereas, in production,\nreference data might n",
        "chapter": 4
      },
      {
        "start": 19750,
        "text": "\nAnswer:\nMore sophisticated AI as a judge techniques to evaluate factual consistency\nare self-verification and knowledge-augmented verification:\nSelf-verification\nSelfCheckGPT (Manakul et al., 2023) relies on an assumption that if\na model generates multiple outputs that disagree with one another,\nth",
        "chapter": 4
      }
    ],
    "input_tokens": 928,
    "output_tokens": 153,
    "input_cost": 0.0001392,
    "output_cost": 9.18e-05,
    "total_cost": 0.000231,
    "reference_answer": "The chapter explains that data contamination can be detected using heuristics like n-gram overlapping and perplexity, which help identify if a model has seen evaluation samples during training."
  }
]